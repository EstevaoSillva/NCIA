{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Descritores Locais (SIFT, ORB) e Matching\n",
                "\n",
                "Diferente dos descritores globais (que olham a imagem toda), os **Descritores Locais** focam em pontos de interesse (keypoints). Isso permite reconhecer objetos mesmo com **oclusão parcial**, **rotação severa** ou **mudança de perspectiva**.\n",
                "\n",
                "## 1) O Pipeline Local\n",
                "1.  **Detecção:** Encontrar pontos estáveis (cantos, blobs) que aparecem em diferentes vistas da cena.\n",
                "2.  **Descrição:** Criar um vetor único para a vizinhança desse ponto.\n",
                "3.  **Matching:** Encontrar pares correspondentes entre duas imagens.\n",
                "4.  **Transformação:** Usar os matches para alinhar imagens (Homografia) ou converter para uma representação global (Bag of Words).\n",
                "\n",
                "> **Insight:** Uma vez extraídos os descritores, podemos descartar a imagem original. O conjunto de vetores é suficiente para o reconhecimento."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2) SIFT (Scale-Invariant Feature Transform)\n",
                "\n",
                "Proposto por David Lowe (2004), é o padrão-ouro de robustez.\n",
                "\n",
                "### Conceitos Chave:\n",
                "- **Espaço de Escala (Scale Space):** Usa Diferença de Gaussianas (DoG) para achar pontos estáveis em vários níveis de zoom (invariância a escala).\n",
                "- **Orientação Dominante:** Atribui uma direção principal ao ponto baseada no gradiente local (invariância a rotação).\n",
                "- **O Descritor:** Uma grade 4x4 onde cada célula tem um histograma de 8 orientações. Resulta em um vetor de **128 dimensões** (4x4x8).\n",
                "\n",
                "Vamos usar o OpenCV para detectar e descrever."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from skimage import data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from skimage import data\n",
                "\n",
                "# Preparando Imagens: Astronauta Original vs Rotacionada\n",
                "img1 = cv2.cvtColor(data.astronaut(), cv2.COLOR_RGB2GRAY)\n",
                "rows, cols = img1.shape\n",
                "M = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 0.8) # Rotação 45 graus + Scale 0.8x\n",
                "img2 = cv2.warpAffine(img1, M, (cols, rows))\n",
                "\n",
                "# 1. Criar SIFT (Se falhar, tente atualizar o opencv ou usar ORB)\n",
                "try:\n",
                "    sift = cv2.SIFT_create()\n",
                "except AttributeError:\n",
                "    print(\"SIFT não disponível. Usando ORB como fallback.\")\n",
                "    sift = cv2.ORB_create()\n",
                "\n",
                "# 2. Detectar e Computar\n",
                "kp1, des1 = sift.detectAndCompute(img1, None)\n",
                "kp2, des2 = sift.detectAndCompute(img2, None)\n",
                "\n",
                "# Visualizar Keypoints na Imagem 1\n",
                "img_kp = cv2.drawKeypoints(img1, kp1, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
                "\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.imshow(img_kp)\n",
                "plt.title(f\"Keypoints SIFT detectados: {len(kp1)}\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Matching (Lowe's Ratio Test)\n",
                "Ao comparar descritores (distância Euclidiana), muitos matches são ruído. Lowe propôs aceitar um match apenas se ele for **muito melhor** que o segundo melhor candidato.\n",
                "\n",
                "$$ \\frac{D(best)}{D(2nd\\ best)} < 0.75 $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Matcher força bruta (L2 para SIFT, Hamming para ORB binário)\n",
                "bf = cv2.BFMatcher()\n",
                "\n",
                "# KNN match (k=2) para aplicar Ratio Test\n",
                "matches = bf.knnMatch(des1, des2, k=2)\n",
                "\n",
                "good_matches = []\n",
                "for m, n in matches:\n",
                "    if m.distance < 0.75 * n.distance:\n",
                "        good_matches.append(m)\n",
                "\n",
                "# Desenhar Matches\n",
                "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=2)\n",
                "\n",
                "plt.figure(figsize=(12,6))\n",
                "plt.imshow(img_matches)\n",
                "plt.title(f\"Matches filtrados (Ratio Test): {len(good_matches)}\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3) Matching Geométrico e RANSAC\n",
                "\n",
                "Mesmo com o Ratio Test, alguns matches errados (outliers) sobrevivem (veja linhas cruzadas acima). O **RANSAC** (Random Sample Consensus) resolve isso encontrando a transformação geométrica (Homografia) que explica a maioria dos pontos.\n",
                "\n",
                "Ele basicamente diz: \"Qual rotação/translação/warp alinha a maioria desses pontos? Ignore quem não se alinha.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(good_matches) > 4:\n",
                "    # Extrair coordenadas dos pontos\n",
                "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
                "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
                "\n",
                "    # Calcular Homografia com RANSAC\n",
                "    M_homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
                "    matchesMask = mask.ravel().tolist()\n",
                "\n",
                "    print(f\"RANSAC encontrou {sum(matchesMask)} inliers de {len(good_matches)} matches.\")\n",
                "\n",
                "    # Desenhar APENAS inliers\n",
                "    draw_params = dict(matchColor=(0, 255, 0), singlePointColor=None, matchesMask=matchesMask, flags=2)\n",
                "    img_ransac = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, **draw_params)\n",
                "\n",
                "    plt.figure(figsize=(12,6))\n",
                "    plt.imshow(img_ransac)\n",
                "    plt.title(\"Matches Geométricos (RANSAC Inliers)\")\n",
                "    plt.axis('off')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Matches insuficientes para homografia.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4) SURF, FAST e ORB\n",
                "\n",
                "- **SURF (Speeded-Up Robust Features):** Usa *Box Filters* e imagens integrais para ser mais rápido que o SIFT. O descritor tem 64 dimensões. (Muitas vezes protegido por patente/removido do OpenCV).\n",
                "- **FAST:** Apenas detector de cantos ultra-rápido (compara pixel central com círculo ao redor). Não gera descritor.\n",
                "- **ORB (Oriented FAST and Rotated BRIEF):** Alternativa livre de patentes ao SIFT/SURF. Usa FAST para detectar e BRIEF binário para descrever. É muito rápido, mas menos robusto a ângulo/escala extremos.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exemplo Rápido de ORB na mesma imagem\n",
                "orb = cv2.ORB_create()\n",
                "kp_orb, des_orb = orb.detectAndCompute(img1, None)\n",
                "\n",
                "print(f\"Keypoints ORB: {len(kp_orb)}\")\n",
                "print(f\"Descritor ORB (Binário): shape {des_orb.shape}\") # Note que é uint8, não float\n",
                "\n",
                "# Visualização rápida\n",
                "img_orb = cv2.drawKeypoints(img1, kp_orb, None, color=(0,255,0))\n",
                "plt.imshow(img_orb); plt.axis('off'); plt.title(\"ORB Keypoints\"); plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5) Transformação: Bag of Visual Words (BoVW)\n",
                "\n",
                "Como usar descritores locais para classificar a imagem inteira?\n",
                "A ideia do **BoVW** é criar um \"vocabulário\" de texturas/padrões.\n",
                "\n",
                "1.  Extrair SIFT/ORB de várias imagens.\n",
                "2.  Agrupar vetores parecidos (K-Means) -> Os centros dos grupos são as \"Palavras Visuais\".\n",
                "3.  Cada imagem vira um histograma: \"Quantas vezes a palavra X apareceu aqui?\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "\n",
                "# --- Mini-Demo BoVW ---\n",
                "# 1. Coletar descritores de algumas imagens\n",
                "images = [data.chelsea(), data.coffee(), data.astronaut(), data.camera()]\n",
                "all_descriptors = []\n",
                "img_descriptors = [] # Guardar descritores por imagem\n",
                "\n",
                "sift_demo = cv2.SIFT_create()\n",
                "\n",
                "for img in images:\n",
                "    if img.ndim == 3: img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
                "    kp, des = sift_demo.detectAndCompute(img, None)\n",
                "    if des is not None:\n",
                "        all_descriptors.append(des)\n",
                "        img_descriptors.append(des)\n",
                "\n",
                "if len(all_descriptors) > 0:\n",
                "    # Empilhar tudo num \"caldeirão\" de features\n",
                "    stack_des = np.vstack(all_descriptors)\n",
                "    \n",
                "    # 2. KMeans para criar Vocabulário (Dicionário) de 10 palavras\n",
                "    kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
                "    kmeans.fit(stack_des)\n",
                "    \n",
                "    # 3. Criar Histogramas (BoVW) para cada imagem\n",
                "    bovw_histograms = []\n",
                "    for des in img_descriptors:\n",
                "        # Predizer a qual \"palavra\" cada descritor da imagem pertence\n",
                "        words = kmeans.predict(des)\n",
                "        hist, _ = np.histogram(words, bins=10, range=(0, 10))\n",
                "        \n",
                "        # Normalizar\n",
                "        hist = hist.astype(float) / (hist.sum() + 1e-6)\n",
                "        bovw_histograms.append(hist)\n",
                "\n",
                "    # Exibir Histogramas\n",
                "    labels = [\"Gato\", \"Café\", \"Astronauta\", \"Câmera\"]\n",
                "    fig, ax = plt.subplots(1, 4, figsize=(16, 3))\n",
                "    for a, h, l in zip(ax, bovw_histograms, labels):\n",
                "        a.bar(range(10), h)\n",
                "        a.set_title(l)\n",
                "        a.set_ylim(0, 0.5)\n",
                "    plt.suptitle(\"Histogramas Bag of Visual Words (Assinatura Global)\")\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Nenhum descritor encontrado para demo BoVW.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6) Exercícios\n",
                "1.  **SIFT vs ORB:** Tente rodar o código de matching usando `ORB_create()` e `matches = bf.match(...)` (ORB usa distância de Hamming, não L2, e não costuma usar ratio test da mesma forma, mas sim CrossCheck). Compare a qualidade no caso da rotação.\n",
                "2.  **RANSAC:** Altere o limiar (threshold) do `findHomography` de 5.0 para 1.0 ou 20.0. Como isso afeta o número de inliers?\n",
                "3.  **Vocabulário:** No demo BoVW, se aumentarmos `n_clusters` para 100, os histogramas ficam mais esparsos (mais zeros) ou mais cheios? Teste a intuição."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "myenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
