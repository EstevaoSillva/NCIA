{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKEL771xMw2a"
      },
      "source": [
        "![NCIA](NCIA_Images\\start.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItHZYTlmMw2b"
      },
      "source": [
        "# Building Neural Networks with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G7hR0ZUMw2c"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsxvtjeHMw2c"
      },
      "source": [
        "Este projeto requer Python 3.10 ou superior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-0ApDnOxMw2c",
        "outputId": "b49d8fa6-c0f2-4246-9ff9-109bb7c569d2"
      },
      "outputs": [],
      "source": [
        "#!pip install -q pandas matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSH6GiwPMw2d"
      },
      "source": [
        "<div align=\"center\" style=\"margin-top:20px; margin-bottom:20px;\">\n",
        "  \n",
        "![pt](Aula_Imagens/pytorch.png)\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPftgDAWMw2d"
      },
      "source": [
        "   \n",
        "   Isso demora 9 minutos =/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8v3qtx9HMw2d",
        "outputId": "d4948c98-eb88-47a7-a210-ac68bf4d5003"
      },
      "outputs": [],
      "source": [
        "#!pip install torch>=2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zDF0JNgHMw2e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v0vJMkeMw2e"
      },
      "source": [
        "Também requer Scikit-Learn ≥ 1.6.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CRpUlw8bMw2f"
      },
      "outputs": [],
      "source": [
        "from packaging.version import Version\n",
        "import sklearn\n",
        "assert Version(sklearn.__version__) >= Version(\"1.6.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHXQTkRKMw2f"
      },
      "source": [
        "E é claro que precisamos do PyTorch, especificamente do PyTorch ≥ 2.8.0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rydhV_w5Mw2f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "assert Version(torch.__version__) >= Version(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmDyQUYcMw2f"
      },
      "source": [
        "Como fizemos nos capítulos anteriores, vamos definir os tamanhos de fonte padrão para deixar as figuras mais bonitas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GUtQj1mzMw2f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wBXxsVhMw2f"
      },
      "source": [
        "## Sobre o Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd_QD6HXMw2g"
      },
      "source": [
        "PyTorch é uma biblioteca open source de deep learning (originada do Torch/Lua) mantida pelo laboratório FAIR (Meta AI). Em Python, oferece **tensores**, **autodiferenciação (autograd)**, **aceleração por GPU/TPU** e um ecossistema de **camadas, perdas e otimizadores prontos**, tornando o desenvolvimento de redes neurais ágil e produtivo. Conceitualmente, lembra o NumPy, mas com suporte nativo a **cálculo em hardware acelerado** e **gradientes automáticos**.\n",
        "\n",
        "\"PôfêSô, por que Pt? eu gostava do TF!\" Em 2016, o **TensorFlow** dominava o cenário por desempenho e escalabilidade, porém seu modelo de programação era mais **estático e complexo**. O PyTorch foi criado com uma proposta **mais “pythônica” e dinâmica**, usando **grafos computacionais dinâmicos (define-by-run)**, o que facilita a **depuração**, a **exploração interativa** e a **pesquisa**.\n",
        "Além do design limpo e documentação robusta, a comunidade open source cresceu rapidamente; em 2022, a governança migrou para a **PyTorch Foundation** (Linux Foundation), consolidando o ecossistema. O resultado prático foi a **adoção massiva na academia** e, por consequência, **migração gradual da indústria**.\n",
        "\n",
        "## O que você aprenderá neste capítulo\n",
        "\n",
        "* **Fundamentos**: como trabalhar com **tensores** e **autograd** no PyTorch.\n",
        "* **Primeiros passos de modelagem**: construir e treinar um **modelo de regressão linear** para entender o pipeline básico.\n",
        "* **Evolução para redes profundas**: ampliar para **redes multicamadas (MLP)**:\n",
        "\n",
        "  * **Regressão** com redes neurais.\n",
        "  * **Classificação** com redes neurais.\n",
        "* **Arquiteturas personalizadas**: criar modelos com **múltiplas entradas** ou **múltiplas saídas**.\n",
        "* **Ajuste de hiperparâmetros**: usar **Optuna** para **tunar** modelos automaticamente.\n",
        "* **Otimização e exportação**: técnicas para **otimizar desempenho** e **salvar/exportar** modelos para uso em produção.\n",
        "\n",
        "> **Resumo da ideia central**: PyTorch equilibra **simplicidade**, **flexibilidade** e **alto desempenho** graças aos **grafos dinâmicos** e ao **ecossistema maduro**. Este capítulo guia você do **básico (tensores/autograd)** à **produção (tuning, otimização e exportação)** passando por exemplos práticos de **regressão** e **classificação**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWvkyK01Mw2g"
      },
      "source": [
        "# 1. Fundamentos do PyTorch\n",
        "\n",
        "O **tensor** é a estrutura de dados central do PyTorch: um **array multidimensional** com **forma** e **tipo de dado**, usado para computação numérica. Ele é semelhante a um array do **NumPy**, mas tem duas vantagens fundamentais: pode **residir em GPU** (ou outros aceleradores) e **suporta auto-diferenciação**. A partir deste ponto, **todos os modelos** trabalharão **recebendo e produzindo tensores**, de modo análogo a como modelos do Scikit-Learn usam arrays NumPy. O próximo passo é **criar e manipular tensores**.\n",
        "\n",
        "## 1.1 PyTorch Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gviZtnerMw2g"
      },
      "source": [
        "Você pode criar um tensor PyTorch da mesma forma que criaria um array NumPy. Por exemplo, vamos criar um array 2 × 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JzdWT-ydMw2g",
        "outputId": "92b1f5fb-eb9f-4f37-97e7-35207ec3739b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 4., 7.],\n",
              "        [2., 3., 6.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([[1.0, 4.0, 7.0], [2.0, 3.0, 6.0]])\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqGrDie4Mw2g"
      },
      "source": [
        "Assim como um array do NumPy, um **tensor** pode conter **floats**, **inteiros**, **booleanos** ou **números complexos** — **apenas um tipo por tensor**. Se você o inicializa com valores de tipos mistos, o PyTorch escolhe o **mais geral** segundo a hierarquia: **complexo > float > inteiro > bool**. Também é possível **definir o tipo explicitamente** na criação (por exemplo, `dtype=torch.float16` para floats de 16 bits). **Tensores de strings ou objetos não são suportados**.\n",
        "\n",
        "> Você pode **inspecionar a forma (shape) e o tipo (dtype)** de um tensor diretamente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4tZM0W49Mw2g",
        "outputId": "73bf250f-7b1a-4aaf-d283-63d0625d1459"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g1s9DODUMw2h",
        "outputId": "14b062fe-c3c3-40b0-b345-cb19359c759c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf2YwhSoMw2h"
      },
      "source": [
        "A indexação funciona exatamente como para matrizes NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xq4Dn_JyMw2h",
        "outputId": "dd36a8bd-e539-4688-8675-c854f75e260d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BF3ZVDg7Mw2h",
        "outputId": "4dd0ffdc-eb4a-4977-a9f8-bb8f00368b1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4., 3.])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujDEsJX4Mw2h"
      },
      "source": [
        "Você pode aplicar **diversas operações numéricas** diretamente em tensores, com uma API **muito semelhante à do NumPy**: `torch.abs()`, `torch.cos()`, `torch.exp()`, `torch.max()`, `torch.mean()`, `torch.sqrt()`, entre outras.\n",
        "Quase todas também existem como **métodos do próprio tensor**, permitindo escrever `X.exp()` em vez de `torch.exp(X)`. O próximo trecho demonstra algumas dessas operações na prática."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LWdSZXWjMw2h",
        "outputId": "b2d33305-a647-4595-ed3c-495f7f7cc918"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[20., 50., 80.],\n",
              "        [30., 40., 70.]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "10 * (X + 1.0)  # item-wise addition and multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MM4STRE-Mw2i",
        "outputId": "2132302b-2c67-4ad3-f092-a60083e331f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   2.7183,   54.5981, 1096.6332],\n",
              "        [   7.3891,   20.0855,  403.4288]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.exp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IJ0hqSwRMw2i",
        "outputId": "e1822065-c6af-4154-d51d-6ec87c91bc9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.8333)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6JezKyEuMw2i",
        "outputId": "4368f409-6708-4c4e-9f0c-5319f1236a76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([2., 4., 7.]),\n",
              "indices=tensor([1, 0, 0]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.max(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Dtu6NNCBMw2i",
        "outputId": "7d0a4e4a-5f7e-48ee-fffb-58036e463d88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[66., 56.],\n",
              "        [56., 49.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X @ X.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oANY898Mw2i"
      },
      "source": [
        "Você também pode converter um tensor em uma matriz NumPy usando o método numpy() e criar um tensor a partir de uma matriz NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yDi_1dCyMw2i",
        "outputId": "1698bbb9-16c0-4dee-a232-e165f932d46b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 4., 7.],\n",
              "       [2., 3., 6.]], dtype=float32)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "X.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0h_bK1qSMw2i",
        "outputId": "36eeb627-aa23-420c-8b5a-f346341191f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 4., 7.],\n",
              "        [2., 3., 6.]], dtype=torch.float64)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor(np.array([[1., 4., 7.], [2., 3., 6.]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EisOfMW4Mw2j"
      },
      "source": [
        "No PyTorch, o **padrão de floats é 32 bits (float32)**, enquanto no NumPy é **64 bits (float64)**. Em deep learning, **float32 costuma ser preferível**: consome **metade da RAM**, **acelera os cálculos** e a rede **não precisa** da precisão extra de 64 bits.\n",
        "Ao converter um array NumPy com `torch.tensor()`, **indique** `dtype=torch.float32`. Como alternativa, `torch.FloatTensor()` **já converte** automaticamente para **32 bits**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0nTrxt9WMw2j",
        "outputId": "969de679-2271-4b5a-c1c4-99296194fe95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 4., 7.],\n",
              "        [2., 3., 6.]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor(np.array([[1., 4., 7.], [2., 3., 6.]]), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bXQBKwNxMw2j",
        "outputId": "94b3155f-6cf4-4a46-cc4d-f967d8644e14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 4., 7.],\n",
              "        [2., 3., 6.]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.FloatTensor(np.array([[1., 4., 7.], [2., 3., 6]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AYm0ri6nMw2j",
        "outputId": "044c0546-6084-4cb1-c5b7-bd9670dc8d2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1., 88.,  7.],\n",
              "        [ 2.,  3.,  6.]], dtype=torch.float64)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extra code: demonstrate torch.from_numpy()\n",
        "X2_np = np.array([[1., 4., 7.], [2., 3., 6]])\n",
        "X2 = torch.from_numpy(X2_np)  # X2_np and X2 share the same data in memory\n",
        "X2_np[0, 1] = 88\n",
        "X2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE6RtxxxMw22"
      },
      "source": [
        "Você também pode modificar um tensor no local usando indexação e fatiamento, como com uma matriz NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DIRMYjELMw22",
        "outputId": "7c51c4e4-e5c7-46b7-82b3-c767edeb81d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  1., -99.,   7.],\n",
              "        [  2., -99.,   6.]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[:, 1] = -99\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbPAKhNCMw22"
      },
      "source": [
        "A API do PyTorch oferece diversas operações **in-place** (terminadas com `_`), como `abs_()`, `sqrt_()` e `zero_()`, que **modificam o próprio tensor**. Elas podem **economizar memória** e **aumentar a velocidade** em alguns casos.\n",
        "Exemplo: `relu_()` aplica a **ReLU** diretamente, **substituindo valores negativos por 0** no mesmo tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N7zaXjetMw22",
        "outputId": "d26ab30a-b56d-4507-db15-9a96cff6bcde"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 7.],\n",
              "        [2., 0., 6.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.relu_()\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIMCuNEeMw23"
      },
      "source": [
        "Os tensores do PyTorch realmente se assemelham a matrizes NumPy. Na verdade, eles têm mais de 200 funções comuns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CXdJ476OMw23",
        "outputId": "dfc2dd45-b026-4d78-fce2-61149f8ba3b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'__getattr__, abs, absolute, acos, acosh, add, all, allclose, amax, amin, angle, any, arange, arccos, arccosh, arcsin, arcsinh, arctan, arctan2, arctanh, argmax, argmin, argsort, argwhere, asarray, asin, asinh, atan, atan2, atanh, atleast_1d, atleast_2d, atleast_3d, bincount, bitwise_and, bitwise_left_shift, bitwise_not, bitwise_or, bitwise_right_shift, bitwise_xor, broadcast_shapes, broadcast_to, can_cast, ceil, clip, column_stack, concat, concatenate, conj, copysign, corrcoef, cos, cosh, count_nonzero, cov, cross, cumprod, cumsum, deg2rad, diag, diagflat, diagonal, diff, divide, dot, dsplit, dstack, dtype, einsum, empty, empty_like, equal, exp, exp2, expm1, eye, finfo, fix, flip, fliplr, flipud, float_power, floor, floor_divide, fmax, fmin, fmod, frexp, from_dlpack, frombuffer, full, full_like, gcd, gradient, greater, greater_equal, heaviside, histogram, histogramdd, hsplit, hstack, hypot, i0, iinfo, imag, inner, isclose, isfinite, isin, isinf, isnan, isneginf, isposinf, isreal, kron, lcm, ldexp, less, less_equal, linspace, load, log, log10, log1p, log2, logaddexp, logaddexp2, logical_and, logical_not, logical_or, logical_xor, logspace, matmul, max, maximum, mean, median, meshgrid, min, minimum, moveaxis, multiply, nan_to_num, nanmean, nanmedian, nanquantile, nansum, negative, nextafter, nonzero, not_equal, ones, ones_like, outer, positive, pow, prod, promote_types, put, quantile, rad2deg, ravel, real, reciprocal, remainder, reshape, result_type, roll, rot90, round, row_stack, save, searchsorted, select, set_printoptions, sign, signbit, sin, sinc, sinh, sort, split, sqrt, square, squeeze, stack, std, subtract, sum, swapaxes, take, tan, tanh, tensordot, tile, trace, transpose, trapezoid, trapz, tril, tril_indices, triu, triu_indices, true_divide, trunc, typename, unique, unravel_index, vander, var, vdot, vsplit, vstack, where, zeros, zeros_like'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# extra code: list functions that appear both in NumPy and PyTorch\n",
        "functions = lambda mod: set(f for f in dir(mod) if callable(getattr(mod, f)))\n",
        "\", \".join(sorted(functions(torch) & functions(np)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfJi7wIQMw23"
      },
      "source": [
        "## 1.2 Hardware Acceleration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZ2_KiAMw23"
      },
      "source": [
        "**Copiar tensores para a GPU** é simples no PyTorch, desde que sua máquina tenha GPU compatível e as bibliotecas necessárias estejam instaladas. No **Google Colab**, basta usar um **runtime com GPU** (Menu *Runtime* → *Change runtime type* → selecionar uma GPU, como **Nvidia T4**). Esse ambiente já vem com o PyTorch compilado com suporte a GPU, drivers e bibliotecas requeridas (por exemplo, **CUDA** e **cuDNN**).\n",
        "Se preferir rodar localmente, instale **drivers** e **bibliotecas** apropriadas seguindo as instruções: [https://homl.info/install-p](https://homl.info/install-p).\n",
        "\n",
        "O PyTorch tem excelente suporte a **GPUs Nvidia** e também a outros aceleradores:\n",
        "\n",
        "* **Apple MPS**: aceleração em **Apple Silicon** (M1, M2, posteriores) e alguns **Intel Macs** compatíveis.\n",
        "* **AMD**: **Instinct** e **Radeon** via **ROCm** (Linux) ou **DirectML** (Windows).\n",
        "* **Intel**: **GPUs e CPUs** (Linux/Windows) via **oneAPI**.\n",
        "* **Google TPUs**: integração via **`torch_xla`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1_pZZJUHMw23",
        "outputId": "23a3c38c-0eab-4995-b3ba-27032cfd2c2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtId7owqMw23"
      },
      "source": [
        "Em um Colab GPU Runtime, o dispositivo será igual a \"cuda\". Agora, vamos criar um tensor nessa GPU. Para isso, uma opção é criar o tensor na CPU e copiá-lo para a GPU usando o método **to()**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ixGfCBcwMw24",
        "outputId": "40d3d532-8eab-4c1c-eda6-5c34ca49a30b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
        "M = M.to(device)\n",
        "M.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgxTG1_yMw24"
      },
      "source": [
        "Alternativamente, podemos criar o tensor diretamente na GPU usando o argumento do dispositivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BiJAQN6QMw24"
      },
      "outputs": [],
      "source": [
        "M = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVZQF1v4Mw24"
      },
      "source": [
        "Quando o tensor estiver na GPU, podemos executar operações nele normalmente, e todas elas ocorrerão na GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lJryfyy6Mw24",
        "outputId": "e5f886d9-f507-4940-e989-c33284e4cefa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[14., 32.],\n",
              "        [32., 77.]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "R = M @ M.T  # run some operations on the GPU\n",
        "R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YYjzFgwMw24"
      },
      "source": [
        "Quando um tensor é processado na **GPU**, o **resultado também permanece na GPU**. Isso permite **encadear várias operações** sem **copiar dados entre CPU e GPU**, evitando um gargalo comum de desempenho.\n",
        "\n",
        "**Quanto a GPU acelera?** Depende do **modelo da GPU** (as mais caras podem ser **dezenas de vezes** mais rápidas) e do **throughput de dados**:\n",
        "\n",
        "* **Modelos compute-heavy** (ex.: redes muito profundas): o **poder de cálculo** da GPU e a **quantidade de RAM** tendem a ser os fatores críticos.\n",
        "* **Modelos rasos / datasets grandes**: o **envio contínuo de dados** para a GPU pode virar o **gargalo** principal.\n",
        "\n",
        "O próximo trecho realiza um **teste comparando** a **multiplicação de matrizes** na **CPU vs GPU** para ilustrar essas diferenças de **tempo de execução** e **banda de dados**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "z9FkwWf3Mw24",
        "outputId": "8e201532-c054-4908-f1f4-a198098e07c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.28 ms ± 93.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "3.72 ms ± 120 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "M = torch.rand((1000, 1000))  # on the CPU\n",
        "M @ M.T  # warmup\n",
        "%timeit M @ M.T\n",
        "\n",
        "M = M.to(device)\n",
        "M @ M.T  # warmup\n",
        "%timeit M @ M.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juSKiQuuMw25"
      },
      "source": [
        "No teste, a **GPU (Nvidia T4 no Colab)** proporcionou um **ganho de ~26×** na multiplicação de matrizes. Com GPUs mais potentes, o **speedup** tende a ser ainda maior. Porém, **em matrizes pequenas** (ex.: `100 × 100`), o ganho cai para algo como **~2×**.\n",
        "Isso ocorre porque **GPUs paralelizam tarefas grandes** (quebrando-as em muitas subtarefas para milhares de núcleos). **Tarefas pequenas** não geram paralelismo suficiente, reduzindo o benefício — e, em cenários com **muitas tarefas minúsculas**, **a CPU pode ser até mais rápida** devido ao overhead de orquestração na GPU.\n",
        "\n",
        "Com a base de **tensores** e **execução em CPU/GPU** estabelecida, o próximo passo é explorar o **autograd** do PyTorch (auto-diferenciação).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1EN0YQGMw25"
      },
      "source": [
        "## 1.3 Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QCs81x6Mw25"
      },
      "source": [
        " PyTorch implementa **auto-diferenciação em modo reverso** (*autograd*), permitindo calcular **gradientes automaticamente**. A ideia é simples de usar: dado, por exemplo, $f(x) = x^2$, o cálculo diferencial diz que $f'(x)=2x$. Avaliando em $f(5)$, obtemos $f(5)=25$ e $f'(5)=10$. O próximo trecho verifica esses valores com o **autograd** do PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RJtUdQ42Mw25",
        "outputId": "e17c1fe2-1e6f-47e7-ae16-e00cdf2d0650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(25., grad_fn=<PowBackward0>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "f = x ** 2\n",
        "f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jCEgNmqNMw25",
        "outputId": "10ef5a67-4393-43ba-cb37-8b3e9699b0a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(10.)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXH6eZWDMw25"
      },
      "source": [
        "Obtivemos os valores corretos: **$f=25$** e **(x.\\texttt{grad}=10)**. A chamada `backward()` calculou automaticamente $f'(x)$ no ponto (x=5.0). Eis a lógica, linha a linha:\n",
        "\n",
        "* **Definição de variável com gradiente**\n",
        "  Criamos `x = 5.0` com `requires_grad=True`. Assim, o PyTorch **rastreia todas as operações** envolvendo `x`, construindo o **grafo de computação** necessário para executar o **backpropagation**. Nesse grafo, `x` é um **nó-folha**.\n",
        "\n",
        "* **Cálculo de $f = x ** 2$**\n",
        "  O resultado é `25.0`. Além do valor, `f` carrega o atributo **`grad_fn`** (ex.: `PowBackward0`), que **representa a operação** que gerou `f` e **informa como retropropagar** os gradientes por essa operação. É assim que o PyTorch **mantém o grafo**.\n",
        "\n",
        "* **Retropropagação**\n",
        "  `f.backward()` **propaga gradientes** a partir de `f` **até os nós-folha** (aqui, apenas `x`).\n",
        "\n",
        "* **Leitura do gradiente**\n",
        "  `x.grad` contém a **derivada de $f$ em relação a (x)**, computada no backprop (no exemplo, **10**).\n",
        "\n",
        "### Grafos dinâmicos (define-by-run)\n",
        "\n",
        "O PyTorch **cria o grafo on-the-fly** a cada *forward pass*, conforme as operações são executadas, o que suporta **modelos dinâmicos** com **loops e condicionais**.\n",
        "\n",
        "### Passo de gradiente (gradient descent) com `torch.no_grad()`\n",
        "\n",
        "Após obter gradientes, normalmente fazemos **descida do gradiente**, subtraindo uma fração do gradiente das variáveis do modelo. No exemplo de $f(x)=x^2$, isso **empurra (x) em direção a 0** (o minimizador).\n",
        "\n",
        "> Importante: **desative o rastreamento de gradiente** durante a atualização (por exemplo, usando `with torch.no_grad():`), pois **não queremos registrar** a própria atualização no grafo — e operações *in-place* em variáveis rastreadas podem **levantar exceção**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "f340hQ5DMw26"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "with torch.no_grad():\n",
        "    x -= learning_rate * x.grad  # gradient descent step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cgmJFVSJMw26",
        "outputId": "572535dc-9c9c-4b53-96aa-48cadecca140"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4., requires_grad=True)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGImkhoMMw26"
      },
      "source": [
        "No passo de **descida do gradiente**, (x) é decrementado por $0{,}1 \\times 10{,}0 = 1{,}0$, indo de **5,0** para **4,0**.\n",
        "\n",
        "Outra forma de **evitar cálculo de gradientes** é usar **`detach()`**: ele cria um **novo tensor desacoplado do grafo** (`requires_grad=False`), **apontando para os mesmos dados em memória**. Assim, você pode **atualizar esse tensor desacoplado** sem registrar a operação no grafo de computação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6ieNXDSKMw26"
      },
      "outputs": [],
      "source": [
        "x_detached = x.detach()\n",
        "x_detached -= learning_rate * x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD0e55tYMw26"
      },
      "source": [
        "Como $x_{\\text{detached}}$ e $x$ **compartilham a mesma memória**, **alterar** $x_{\\text{detached}}$ também **altera** $x$.\n",
        "\n",
        "* **`detach()`**: útil quando você precisa **executar computações sem afetar os gradientes** (ex.: avaliação, logging) ou quando deseja **controle fino** sobre **quais operações** entram no cálculo de gradientes. Cria um **tensor desacoplado do grafo** ( `requires_grad=False` ), apontando para os **mesmos dados**.\n",
        "* **`no_grad()`**: geralmente **preferido** para **inferência** ou durante o **passo de descida do gradiente**, pois fornece um **contexto** conveniente que **desativa o rastreamento** de gradientes para tudo que estiver dentro do bloco.\n",
        "\n",
        "**Antes de repetir o ciclo** *(forward → backward → atualização)*, é **essencial zerar os gradientes** de **todos os parâmetros** do modelo, pois o PyTorch **acumula** gradientes por padrão. Não é necessário `no_grad()` para isso, já que os tensores de gradiente têm `requires_grad=False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ayPQm6TWMw26",
        "outputId": "6abc4702-e7de-4828-c574-733467bc5dd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtZUNO-_Mw27"
      },
      "source": [
        "Juntando tudo, o ciclo de treinamento fica assim:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GtJzSV0TMw27"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "for iteration in range(100):\n",
        "    f = x ** 2  # forward pass\n",
        "    f.backward()  # backward pass\n",
        "    with torch.no_grad():\n",
        "        x -= learning_rate * x.grad  # gradient descent step\n",
        "    x.grad.zero_()  # reset the gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "D8XhO3lIMw27",
        "outputId": "eefff5f1-4d57-42ac-8edb-5aabd899e111"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0185e-09, requires_grad=True)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "846Pw9UvMw27"
      },
      "source": [
        "Operações **in-place** podem **economizar memória** e **evitar cópias**, mas **nem sempre** combinam bem com o **autograd**:\n",
        "\n",
        "1. **Proibição em nós-folha**\n",
        "   Você **não pode** aplicar uma operação *in-place* em um **nó-folha** (tensor com `requires_grad=True`). O PyTorch não saberia **onde armazenar** as informações do grafo. Exemplos que geram `RuntimeError`:\n",
        "\n",
        "   * `x.cos_()`\n",
        "   * `x += 1`\n",
        "\n",
        "2. **Risco de sobrescrever valores necessários ao backward**\n",
        "   Mesmo fora de nós-folha, operações *in-place* podem **apagar valores intermediários** que o autograd precisa para calcular gradientes, quebrando o **backpropagation**.\n",
        "\n",
        "**Exemplo a seguir no código**: calcular $z(t) = \\exp(t) + 1$ em $t=2$ e tentar obter os gradientes — veremos como certas escolhas *in-place* podem levar a erros durante o `backward()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5ZZ9Id4-Mw27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'t = torch.tensor(2.0, requires_grad=True)\\nz = t.exp()  # this is an intermediate result\\nz += 1  # this is an in-place operation\\nz.backward()  # ⚠️ RuntimeError!'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Novo\n",
        "\n",
        "\"\"\"t = torch.tensor(2.0, requires_grad=True)\n",
        "z = t.exp()  # this is an intermediate result\n",
        "z += 1  # this is an in-place operation\n",
        "z.backward()  # ⚠️ RuntimeError!\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxau6VQnMw27"
      },
      "source": [
        "O valor de $z$ foi calculado corretamente, mas o `backward()` lançou `RuntimeError: one of the variables needed for gradient computation has been modified by an in-place operation`. Isso ocorreu porque o resultado intermediário $z = t.\\exp()$ foi **sobrescrito** por `z += 1`. No *backward*, ao chegar na **exp**, o autograd já não tinha o valor necessário para calcular o gradiente.\n",
        "\n",
        "**Correção simples:** troque `z += 1` por `z = z + 1`. Visualmente é parecido, mas **não é in-place**: cria-se **um novo tensor**, preservando o original no **grafo de computação**.\n",
        "\n",
        "---\n",
        "\n",
        "### Por que às vezes funciona? (ex.: com `cos()`)\n",
        "\n",
        "Se você trocar `exp()` por `cos()` no exemplo, o gradiente **funciona**. O motivo é **como cada operação é implementada** e **o que ela guarda** para o *backward*:\n",
        "\n",
        "* **Operações que salvam a *saída*** (não modifique a **saída** *in-place* antes do *backward*):\n",
        "  `exp()`, `relu()`, `rsqrt()`, `sigmoid()`, `sqrt()`, `tan()`, `tanh()`.\n",
        "\n",
        "* **Operações que salvam a *entrada*** (podem tolerar modificar a **saída**, mas **não** modifique a **entrada** *in-place* antes do *backward*):\n",
        "  `abs()`, `cos()`, `log()`, `sin()`, `square()`, `var()`.\n",
        "\n",
        "* **Operações que salvam *entrada e saída*** (não modifique **nenhuma** delas *in-place*):\n",
        "  `max()`, `min()`, `norm()`, `prod()`, `sgn()`, `std()`.\n",
        "\n",
        "* **Operações que não salvam nem entrada nem saída** (seguro modificar *in-place*):\n",
        "  `ceil()`, `floor()`, `mean()`, `round()`, `sum()`.\n",
        "\n",
        "> **Regra prática:** quando usar operações *in-place* para economizar memória, verifique **o que a operação salva** para o *backward*. Se você alterar *in-place* algo que o autograd precisa (entrada, saída ou ambos), o gradiente **quebrará**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQUoS6tAMw27"
      },
      "source": [
        "OK, vamos voltar um pouco. Já discutimos todos os fundamentos do PyTorch: como criar tensores e usá-los para realizar todos os tipos de cálculos, como acelerar os cálculos com uma GPU e como usar o Autograd para calcular gradientes para gradiente descendente. Ótimo! Agora, vamos aplicar o que aprendemos até agora, construindo e treinando um modelo de regressão linear simples com o PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onSCzQ-ZMw28"
      },
      "source": [
        "# 2. Implementing Linear Regression\n",
        "\n",
        "Começaremos implementando a regressão linear usando tensores e autograd diretamente, depois simplificaremos o código usando a API de alto nível do PyTorch e também adicionaremos suporte à GPU.\n",
        "\n",
        "## 2.1 Linear Regression Using Tensors & Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwjpxLu3Mw28"
      },
      "source": [
        "Usaremos o **California Housing** (como no Cap. 9). Suponha que os dados já foram baixados com `sklearn.datasets.fetch_california_housing()` e divididos com `train_test_split()` em: `X_train`, `y_train`, `X_valid`, `y_valid`, `X_test`, `y_test`.\n",
        "\n",
        "Agora vamos **converter tudo para tensores** e **normalizar** usando **operações de tensor** (em vez de `StandardScaler`), para praticar PyTorch:\n",
        "\n",
        "* **Estatísticas no treino**: calcule **média** $\\mu_{\\text{train}}$ e **desvio-padrão** $\\sigma_{\\text{train}}$ apenas em `X_train`.\n",
        "* **Padronização**: aplique em todos os *splits* a transformação\n",
        "  $$\\tilde{X}=\\frac{X-\\mu_{\\text{train}}}{\\sigma_{\\text{train}}}$$\n",
        "\n",
        "Isso garante padronização consistente e evita **vazamento de informação**. O próximo trecho mostra a **conversão para tensores** e a **normalização** passo a passo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rDc0As07Mw28"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dsI8MU1QMw28"
      },
      "outputs": [],
      "source": [
        "X_train = torch.FloatTensor(X_train)\n",
        "X_valid = torch.FloatTensor(X_valid)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "means = X_train.mean(dim=0, keepdims=True)\n",
        "stds = X_train.std(dim=0, keepdims=True)\n",
        "X_train = (X_train - means) / stds\n",
        "X_valid = (X_valid - means) / stds\n",
        "X_test = (X_test - means) / stds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWVxNBooMw28"
      },
      "source": [
        "Também precisamos **converter os alvos para tensores**. Como as **predições** serão **vetores-coluna** (matrizes com **uma única coluna**), os **alvos** devem ter o **mesmo formato**. Porém, no NumPy os alvos vêm **unidimensionais**; portanto, é necessário **reestruturar** para **adicionar uma 2ª dimensão de tamanho 1**, formando vetores-coluna (isto é, shape $[N,,1]$).\n",
        "\n",
        "> Em resumo: converta `y_*` para tensores e **reshape** para $[N,,1]$ para alinhar com as **saídas do modelo**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "l5IOFfdKMw28"
      },
      "outputs": [],
      "source": [
        "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
        "y_valid = torch.FloatTensor(y_valid).view(-1, 1)\n",
        "y_test = torch.FloatTensor(y_test).view(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNI_wdwkMw28"
      },
      "source": [
        "Agora que os dados estão prontos, vamos criar os parâmetros do nosso modelo de regressão linear:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "l1D5vwtXMw28"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "n_features = X_train.shape[1]  # there are 8 input features\n",
        "w = torch.randn((n_features, 1), requires_grad=True)\n",
        "b = torch.tensor(0., requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgbc7G1LMw29"
      },
      "source": [
        "Definimos um vetor de **pesos** $w$ (vetor-coluna com **um peso por atributo de entrada**, aqui $8$) e um **viés** escalar $b$. Os **pesos são inicializados aleatoriamente** e o **viés em zero**. Embora, neste caso simples, também pudéssemos zerar $w$, é boa prática **inicializar pesos aleatoriamente** para **quebrar a simetria** entre unidades — princípio crucial em **redes neurais** (Cap. 9). Adotar esse hábito desde já facilita a transição para modelos mais profundos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PV6jeC_Mw29"
      },
      "source": [
        "Em seguida, vamos treinar nosso modelo, de forma muito semelhante à que fizemos no Capítulo 4, exceto que usaremos o autodiff para calcular os gradientes em vez de usar uma equação de forma fechada. Por enquanto, usaremos a descida do gradiente em lote (BGD), utilizando o conjunto de treinamento completo em cada etapa:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mZrqQziMw29"
      },
      "source": [
        "### Laço de treino: passo a passo (regressão linear)\n",
        "\n",
        "* **Taxa de aprendizado (`learning_rate`)**\n",
        "  Definimos o hiperparâmetro de **taxa de aprendizado**; experimente valores para equilibrar **convergência** e **precisão**.\n",
        "\n",
        "* **Épocas (20)**\n",
        "  Executamos **20 épocas**. Poderíamos aplicar **early stopping** (Cap. 4), mas aqui mantemos simples.\n",
        "\n",
        "* **Forward pass**\n",
        "  Calculamos as **predições** $y_{\\text{pred}}$ e a **loss MSE**:\n",
        "  $$\\mathrm{MSE}(y,\\hat y)=\\frac{1}{N}\\sum_{i=1}^{N}\\bigl(y_i-\\hat y_i\\bigr)^2.$$\n",
        "\n",
        "* **Autograd**\n",
        "  `loss.backward()` calcula **gradientes** da loss em relação a **todos os parâmetros**.\n",
        "\n",
        "* **Passo de descida do gradiente**\n",
        "  Usamos `b.grad` e `w.grad` para **atualizar** os parâmetros **dentro de** `with torch.no_grad():`.\n",
        "\n",
        "* **Zerar gradientes (essencial!)**\n",
        "  Após atualizar, **zeramos os gradientes** para não acumulá-los na próxima iteração.\n",
        "\n",
        "* **Logging**\n",
        "  Imprimimos **nº da época** e a **loss**; `item()` extrai o valor escalar.\n",
        "\n",
        "> Regra prática de formatação: **código** → `backticks`; **fórmulas** → `$...$`; se precisar de sublinhado em modo math, use `\\_`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "RjwsYhXVMw29",
        "outputId": "7b640c96-10b0-4667-82f8-b3b91fbd46c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 16.158456802368164\n",
            "Epoch 2/20, Loss: 4.87936544418335\n",
            "Epoch 3/20, Loss: 2.2552266120910645\n",
            "Epoch 4/20, Loss: 1.3307629823684692\n",
            "Epoch 5/20, Loss: 0.968069851398468\n",
            "Epoch 6/20, Loss: 0.8142682313919067\n",
            "Epoch 7/20, Loss: 0.7417048811912537\n",
            "Epoch 8/20, Loss: 0.7020704746246338\n",
            "Epoch 9/20, Loss: 0.6765921711921692\n",
            "Epoch 10/20, Loss: 0.6577967405319214\n",
            "Epoch 11/20, Loss: 0.6426153779029846\n",
            "Epoch 12/20, Loss: 0.6297224760055542\n",
            "Epoch 13/20, Loss: 0.6184943914413452\n",
            "Epoch 14/20, Loss: 0.6085969805717468\n",
            "Epoch 15/20, Loss: 0.5998218655586243\n",
            "Epoch 16/20, Loss: 0.5920187830924988\n",
            "Epoch 17/20, Loss: 0.5850692391395569\n",
            "Epoch 18/20, Loss: 0.5788734555244446\n",
            "Epoch 19/20, Loss: 0.5733454823493958\n",
            "Epoch 20/20, Loss: 0.5684100985527039\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.4\n",
        "n_epochs = 20\n",
        "for epoch in range(n_epochs):\n",
        "    y_pred = X_train @ w + b\n",
        "    loss = ((y_pred - y_train) ** 2).mean()\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        b -= learning_rate * b.grad\n",
        "        w -= learning_rate * w.grad\n",
        "        b.grad.zero_()\n",
        "        w.grad.zero_()\n",
        "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxSk1y7wMw29"
      },
      "source": [
        "Parabéns, você acabou de treinar seu primeiro modelo usando o PyTorch! Agora você pode usar o modelo para fazer previsões para alguns novos dados X_new (que devem ser representados como um tensor do PyTorch). Por exemplo, vamos fazer previsões para as três primeiras instâncias do conjunto de teste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dmXpYWi_Mw29"
      },
      "outputs": [],
      "source": [
        "X_new = X_test[:3]  # pretend these are new instances\n",
        "with torch.no_grad():\n",
        "    y_pred = X_new @ w + b  # use the trained parameters to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "lo8uiLVIMw29",
        "outputId": "a2a570df-b71b-40b4-e7f1-bf77df459d94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8916],\n",
              "        [1.6480],\n",
              "        [2.6577]])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L95N6f4HMw2-"
      },
      "source": [
        "Implementar a regressão linear usando a API de baixo nível do PyTorch não foi tão difícil, mas usar essa abordagem para modelos mais complexos seria muito complicado e trabalhoso. Portanto, o PyTorch oferece uma API de alto nível para simplificar tudo isso. Vamos reescrever nosso modelo usando essa API de alto nível."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCoQfDWYMw2-"
      },
      "source": [
        "## 2.2 Linear Regression Using PyTorch's High-Level API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xng8UX3AMw2-"
      },
      "source": [
        "PyTorch fornece uma implementação de regressão linear na classe `torch.nn.Linear`, então vamos usá-la\n",
        "> **nn = *neural network(s)*** (“rede(s) neural(is)”)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Ixb98sneMw2-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)  # to get reproducible results\n",
        "model = nn.Linear(in_features=n_features, out_features=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ITkWd4iMw2-"
      },
      "source": [
        "`nn.Linear` (atalho de `torch.nn.Linear`) é um **módulo** do PyTorch; todo módulo herda de `nn.Module`. Para **regressão linear simples**, **um único** `nn.Linear` já basta. Em redes neurais reais, você **empilha vários módulos** — pense neles como **“LEGOs matemáticos”**.\n",
        "\n",
        "**Parâmetros internos do `nn.Linear`:**\n",
        "\n",
        "* **Viés**: vetor de **bias** com **um termo por neurônio**.\n",
        "* **Pesos**: **matriz de pesos** com **uma linha por neurônio** e **uma coluna por dimensão de entrada** (ou seja, é a **transposta** da matriz/vetor de pesos usada anteriormente no capítulo e na equação abaixo).\n",
        "\n",
        "No nosso caso, com `out_features=1`, o modelo tem **um único neurônio**:\n",
        "\n",
        "* O vetor de **bias** tem **um único termo**.\n",
        "* A **matriz de pesos** tem **uma única linha**.\n",
        "\n",
        "Esses parâmetros ficam acessíveis **diretamente** como **atributos** do módulo:\n",
        "\n",
        "* `linear.weight`  → matriz de pesos $W$\n",
        "* `linear.bias`    → viés $b$\n",
        "\n",
        "> Intuição: o módulo implementa $,y = XW^\\top + b,$ (com $W$ armazenado como **linha** quando há um neurônio de saída), mantendo os parâmetros **dentro do módulo** para integração com o **autograd** e os **otimizadores**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaQrh3HLMw2-"
      },
      "source": [
        "<div align=\"center\" style=\"margin-top:20px; margin-bottom:20px;\">\n",
        "  \n",
        "![pt](Aula_Imagens\\eq_Percep.png)\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "eDGsHq2cMw2-",
        "outputId": "6bd0dd0e-f2e6-4b98-e9bb-a49211c3ed16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0.3117], requires_grad=True)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aN93Z7m5Mw2-",
        "outputId": "dac92468-63cd-40aa-cc6d-f4e3c88d1d6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5EMDyRDMw2_"
      },
      "source": [
        "### Parâmetros em `nn.Linear`: inicialização, `Parameter` e iteração\n",
        "\n",
        "* **Inicialização aleatória + reprodutibilidade**\n",
        "  Os **dois parâmetros** do módulo (`weight` e `bias`) são **inicializados aleatoriamente** por padrão — por isso utilizamos `manual_seed()` para obter **resultados reprodutíveis**.\n",
        "\n",
        "* **`torch.nn.Parameter` vs tensor comum**\n",
        "  Esses parâmetros são instâncias de **`torch.nn.Parameter`**, que **herda** de `tensor.Tensor`.\n",
        "  ➤ Consequência: você pode **usá-los como tensores normais** (mesmas operações), **mas** o PyTorch os **registra automaticamente** como **parâmetros treináveis** do módulo (para autograd e otimizadores).\n",
        "  **Diferença central:** um **`Parameter`** é um **tensor marcado** para ser **rastreado como parâmetro do modelo**; já um **tensor comum**, mesmo com `requires_grad=True`, **não** é listado como parâmetro do módulo.\n",
        "\n",
        "* **`module.parameters()` (recursivo)**\n",
        "  O método **`parameters()`** retorna um **iterador** sobre **todos os atributos do tipo `Parameter`** do módulo **e de seus submódulos (recursivamente)**.\n",
        "  Não retorna **tensores comuns**, mesmo que tenham `requires_grad=True`.\n",
        "\n",
        "> Em resumo: **`Parameter` = tensor treinável do módulo** (aparece em `parameters()` e nos otimizadores); **tensor comum** pode participar do grafo, mas **não** é tratado como **parâmetro do modelo**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uApY7S5IMw2_",
        "outputId": "61386283-c876-48d3-e485-d9094847ddac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2703,  0.2935, -0.0828,  0.3248, -0.0775,  0.0713, -0.1721,  0.2076]],\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3117], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "for param in model.parameters():\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwlIa1WNMw2_"
      },
      "source": [
        "* **Parâmetros nomeados**\n",
        "  Além de `parameters()`, há **`named_parameters()`**, que retorna um **iterador de pares (nome, parâmetro)**. Útil para **inspeção**, **debug** e **logs** (ex.: imprimir gradientes por nome).\n",
        "\n",
        "* **Chamando o módulo como função**\n",
        "  Um **módulo** (`nn.Module`) pode ser **chamado como função**: internamente ele executa o método `forward`.\n",
        "  Exemplo: passar as **duas primeiras instâncias** do *training set* ao modelo para obter **predições** iniciais.\n",
        "\n",
        "  > Como os **parâmetros ainda estão aleatórios**, as **predições serão ruins** — o comportamento esperado **antes do treino**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6Iv1T68EMw2_",
        "outputId": "01c6c679-47eb-4e69-b0dc-baf556b0f683"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.4718],\n",
              "        [ 0.1131]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(X_train[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC54OhpJMw2_"
      },
      "source": [
        "Ao usar um **módulo como função**, o PyTorch chama internamente seu **`forward()`**. No caso de `nn.Linear`, ele computa:\n",
        "[\n",
        "X ;@; W^\\top ;+; b\n",
        "]\n",
        "onde $X$ é a entrada, $W$ é `self.weight` e $b$ é `self.bias` — **exatamente** o que precisamos para **regressão linear**.\n",
        "\n",
        "O tensor de saída traz o atributo **`grad_fn`**, indicando que o **autograd** rastreou o **grafo de computação** durante as predições.\n",
        "\n",
        "**Próximo passo:** com o modelo definido, precisamos criar um **otimizador** (para **atualizar os parâmetros**) e escolher uma **função de perda** (loss).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0P7N91PDMw2_"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "mse = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctt7-v_iMw2_"
      },
      "source": [
        "O PyTorch oferece **diversos otimizadores** (detalhes no próximo capítulo). Aqui usamos o **SGD** (*stochastic gradient descent*), que atende a **SGD puro**, **mini-batch GD** e **batch GD**. Para inicializá-lo, passamos os **parâmetros do modelo** (por exemplo, `model.parameters()`) e a **taxa de aprendizado** (`lr`).\n",
        "\n",
        "Para a **função de perda**, instanciamos **`nn.MSELoss`**. Ela também é um **módulo**, então pode ser usada **como função**: recebe **predições** e **alvos** e calcula a **MSE**,\n",
        "\n",
        "  $$\\mathrm{MSE}(y,\\hat y)=\\frac{1}{N}\\sum_{i=1}^{N}\\bigl(y_i-\\hat y_i\\bigr)^2.$$\n",
        "  \n",
        "O submódulo `nn` inclui **muitas outras perdas** e utilidades de redes neurais.\n",
        "\n",
        "**A seguir:** escreveremos uma **função de treino** compacta para executar o ciclo *forward → loss → backward → update*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "MxtH7imZMw3A"
      },
      "outputs": [],
      "source": [
        "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnItmklvMw3A"
      },
      "source": [
        "### Comparando os laços de treino: baixo nível vs. alto nível (`nn` + `optim`)\n",
        "\n",
        "O novo laço é **quase idêntico** ao anterior, porém agora usamos **abstrações de nível mais alto** (módulos `nn` e otimizadores `optim`) em vez de manipular diretamente **tensores** e **autograd**. Pontos-chave:\n",
        "\n",
        "* **Critério (loss function “objeto”)**\n",
        "  Em PyTorch, é comum chamar o **objeto** de função de perda de **criterion** (para distinguir do **valor da loss** calculado a cada iteração).\n",
        "  Aqui, o **criterion** é uma instância de `nn.MSELoss`.\n",
        "\n",
        "* **`optimizer.step()`**\n",
        "  Substitui as duas linhas manuais que **atualizavam** $b$ e $w$ no código anterior (descida do gradiente).\n",
        "\n",
        "* **`optimizer.zero_grad()`**\n",
        "  Substitui as duas linhas que **zeravam** `b.grad` e `w.grad`.\n",
        "  Observação: **não é necessário** usar `with torch.no_grad():` aqui — o **otimizador** já trata corretamente o **rastro de gradientes** internamente em `step()` e `zero_grad()`.\n",
        "\n",
        "**Próximo passo:** chamar a **função de treino** para ajustar o modelo e observar a **queda da loss** ao longo das épocas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "3P-9nTClMw3A",
        "outputId": "d7c5946e-4744-4a55-81ea-7a5f2840fcad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 4.3378496170043945\n",
            "Epoch 2/20, Loss: 0.780293345451355\n",
            "Epoch 3/20, Loss: 0.6253840327262878\n",
            "Epoch 4/20, Loss: 0.6060433387756348\n",
            "Epoch 5/20, Loss: 0.5956299304962158\n",
            "Epoch 6/20, Loss: 0.5873566269874573\n",
            "Epoch 7/20, Loss: 0.5802990198135376\n",
            "Epoch 8/20, Loss: 0.5741382241249084\n",
            "Epoch 9/20, Loss: 0.5687100887298584\n",
            "Epoch 10/20, Loss: 0.5639079213142395\n",
            "Epoch 11/20, Loss: 0.5596510767936707\n",
            "Epoch 12/20, Loss: 0.5558737516403198\n",
            "Epoch 13/20, Loss: 0.5525193810462952\n",
            "Epoch 14/20, Loss: 0.5495391488075256\n",
            "Epoch 15/20, Loss: 0.5468899011611938\n",
            "Epoch 16/20, Loss: 0.544533908367157\n",
            "Epoch 17/20, Loss: 0.5424376130104065\n",
            "Epoch 18/20, Loss: 0.5405716300010681\n",
            "Epoch 19/20, Loss: 0.5389096736907959\n",
            "Epoch 20/20, Loss: 0.5374288558959961\n"
          ]
        }
      ],
      "source": [
        "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcysEhPWMw3A"
      },
      "source": [
        "Tudo bem; o modelo foi treinado e agora você pode usá-lo para fazer previsões simplesmente chamando-o como uma função (de preferência dentro de um contexto no_grad(), como vimos anteriormente):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "1kgKQv2mMw3A",
        "outputId": "88c56f54-ed1a-4abf-9b0a-b0b2595e7cbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8061],\n",
              "        [1.7116],\n",
              "        [2.6973]])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_new = X_test[:3]  # pretend these are new instances\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_new)  # use the trained model to make predictions\n",
        "\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjH6re_QMw3A"
      },
      "source": [
        "As **predições** ficam **semelhantes**, mas **não idênticas** às do modelo anterior porque o `nn.Linear` **inicializa os parâmetros de forma diferente**: usa uma **distribuição uniforme** em um **intervalo específico** tanto para os **pesos** quanto para o **viés** (detalhes sobre inicialização serão tratados no **Cap. 11**).\n",
        "\n",
        "Com essa **API de alto nível** dominada, estamos prontos para ir além da **regressão linear** e construir um **perceptron multicamadas (MLP)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WehfQp3vMw3A"
      },
      "source": [
        ">______________________Aula 34"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KXdNZOvMw3A"
      },
      "source": [
        "# 3. Implementing a Regression MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GExerbMhN4Fs"
      },
      "source": [
        "## `nn.Sequential` no PyTorch\n",
        "\n",
        "O PyTorch oferece o módulo **`nn.Sequential`**, que permite encadear múltiplos módulos de forma automática:\n",
        "\n",
        "- Ao passar uma entrada para um módulo `nn.Sequential`, ela é processada pelo **primeiro módulo**.\n",
        "- A saída do primeiro módulo é então enviada para o **segundo módulo**, e assim por diante até o último módulo.\n",
        "\n",
        "Esse recurso é especialmente útil porque muitas redes neurais são essencialmente **pilhas de módulos**, tornando o `nn.Sequential` um dos módulos mais importantes do PyTorch.\n",
        "\n",
        "No caso de um **MLP (Perceptron Multicamadas)**, ele pode ser construído como uma simples sequência de módulos: duas camadas ocultas seguidas de uma camada de saída.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tTHnm1lOMw3B"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_features, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 4),\n",
        "    nn.Softmax()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kDUNu6AMw3B"
      },
      "source": [
        "* **Camada 1 (entrada → oculta 1)**\n",
        "\n",
        "  * **Entradas**: $n_{\\text{features}}$ (aqui, **8**).\n",
        "  * **Saídas**: **50** (hiperparâmetro ajustável).\n",
        "\n",
        "* **Ativação 1** — `nn.ReLU`\n",
        "\n",
        "  * **Sem parâmetros**; atua **elemento a elemento**; **não** altera o *shape* (apenas os valores).\n",
        "\n",
        "* **Camada 2 (oculta 1 → oculta 2)**\n",
        "\n",
        "  * **Entradas**: **50** (deve **casar** com a saída da camada anterior).\n",
        "  * **Saídas**: **40** (poderia ser igual à anterior, mas aqui usamos 40 para enfatizar a **compatibilidade entre camadas**).\n",
        "\n",
        "* **Ativação 2** — `nn.ReLU`\n",
        "\n",
        "  * Mesmas observações: **sem parâmetros**, **itemwise**, **shape** preservado.\n",
        "\n",
        "* **Camada de saída (oculta 2 → saída)**\n",
        "\n",
        "  * **Entradas**: **40** (devem **casar** com a camada anterior).\n",
        "  * **Saídas**: devem **igualar a dimensionalidade dos alvos**. Como o alvo é **unidimensional**, usamos **1**.\n",
        "\n",
        "> Regra prática: a **saída de uma camada** deve **combinar** com a **entrada da próxima**; apenas a **última camada** é **restrita pelos alvos**.\n",
        "> A seguir: **treinar o modelo** exatamente como antes (definir *criterion*, *optimizer*, laço de épocas).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "bGB_3SIwMw3B",
        "outputId": "7f45a3a7-b8c3-4eca-8af0-82c6b0ee71a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 4.697454929351807\n",
            "Epoch 2/20, Loss: 4.697449684143066\n",
            "Epoch 3/20, Loss: 4.697444438934326\n",
            "Epoch 4/20, Loss: 4.697439670562744\n",
            "Epoch 5/20, Loss: 4.697434425354004\n",
            "Epoch 6/20, Loss: 4.697429656982422\n",
            "Epoch 7/20, Loss: 4.69742488861084\n",
            "Epoch 8/20, Loss: 4.697420120239258\n",
            "Epoch 9/20, Loss: 4.697415351867676\n",
            "Epoch 10/20, Loss: 4.697410583496094\n",
            "Epoch 11/20, Loss: 4.697405815124512\n",
            "Epoch 12/20, Loss: 4.69740104675293\n",
            "Epoch 13/20, Loss: 4.697396755218506\n",
            "Epoch 14/20, Loss: 4.697392463684082\n",
            "Epoch 15/20, Loss: 4.6973876953125\n",
            "Epoch 16/20, Loss: 4.697383880615234\n",
            "Epoch 17/20, Loss: 4.6973795890808105\n",
            "Epoch 18/20, Loss: 4.697375774383545\n",
            "Epoch 19/20, Loss: 4.697371482849121\n",
            "Epoch 20/20, Loss: 4.697366714477539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "c:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:634: UserWarning: Using a target size (torch.Size([11610, 1])) that is different to the input size (torch.Size([11610, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "mse = nn.MSELoss()\n",
        "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j554r0KrMw3B"
      },
      "source": [
        "Parabéns: você **treinou sua primeira rede neural no PyTorch**! 🎉\n",
        "Mas ainda estamos usando **batch gradient descent**: a cada iteração, calculamos **gradientes sobre **todo** o conjunto de treino**. Isso funciona para **datasets pequenos**, porém **não escala** bem para **dados/modelos grandes**.\n",
        "\n",
        "**Próximo passo:** migrar para **mini-batch gradient descent**, isto é, **particionar** o treinamento em **lotes menores** (mini-batches). Benefícios principais:\n",
        "\n",
        "* **Eficiência computacional**: melhor uso de **GPU/CPU** e **memória**.\n",
        "* **Atualizações mais frequentes**: gradientes **mais ruidosos**, porém **mais rápidos**, acelerando a convergência prática.\n",
        "* **Escalabilidade**: viabiliza treinar com **datasets massivos** e **modelos maiores**.\n",
        "\n",
        "> Ideia central: trocamos “um gradiente caro por época” por “**vários gradientes baratos por época**”, mantendo bom progresso e reduzindo o custo por iteração.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzlwOaTqMw3B"
      },
      "source": [
        "# 4. Implementing Mini-Batch Gradient Descent using DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnR5Ql4JtChY"
      },
      "source": [
        "## Carregando Mini-Batches com `DataLoader`\n",
        "\n",
        "Para facilitar a implementação do **Gradient Descent em mini-batches**, o PyTorch oferece a classe **`DataLoader`**, disponível em `torch.utils.data`.  \n",
        "Ela permite:\n",
        "\n",
        "- Carregar os dados em **lotes (batches)** de forma eficiente;\n",
        "- **Embaralhar** os dados a cada época, quando desejado.\n",
        "\n",
        "O `DataLoader` precisa receber um **dataset** que possua duas funções:\n",
        "\n",
        "- `__len__()` → retorna o número de amostras;\n",
        "- `__getitem__(index)` → retorna uma amostra específica (entrada + alvo).\n",
        "\n",
        "Como nossos dados já estão nos tensores `X_train` e `y_train`, podemos usar a classe **`TensorDataset`**, que encapsula os tensores e fornece a interface necessária.\n",
        "\n",
        "Após isso, utilizamos o `DataLoader` para extrair os mini-batches do dataset, incluindo a opção `shuffle=True` para embaralhar as amostras durante o treinamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "sji9HNxLMw3B"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNNzBLMSzFBb"
      },
      "source": [
        "Com um modelo maior e o treinamento sendo feito em mini-batches, é recomendável utilizar **aceleração por hardware** (GPU).  \n",
        "A abordagem é direta:\n",
        "\n",
        "- O modelo deve ser **movido para a GPU**, usando o método `.to()`, o que transfere todos os seus parâmetros para a memória da GPU.\n",
        "- A cada iteração de treinamento, o **batch também deve ser copiado para a GPU** antes do processamento.\n",
        "\n",
        "Além disso, após mover o modelo, podemos definir a **função de perda** e o **otimizador** normalmente, porém com um **learning rate menor** (por exemplo, `0.02`), já que modelos maiores exigem ajustes mais cuidadosos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "J5Auwdw_Mw3C"
      },
      "outputs": [],
      "source": [
        "# extra code – build the model just like earlier\n",
        "torch.manual_seed(42)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_features, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 40),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(40, 1)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# extra code – build the optimizer and loss function, as earlier\n",
        "learning_rate = 0.02\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQoepnzyzeqh"
      },
      "source": [
        "Para treinar o modelo utilizando **mini-batch Gradient Descent**, definimos uma função `train()` responsável por:\n",
        "\n",
        "- Iterar sobre os batches fornecidos pelo `DataLoader`;\n",
        "- Enviar os dados para a GPU antes do processamento;\n",
        "- Executar as etapas do treinamento em cada batch:\n",
        "  - Forward pass\n",
        "  - Cálculo da perda\n",
        "  - Backpropagation\n",
        "  - Atualização dos parâmetros\n",
        "\n",
        "Essa função organiza o processo de treinamento de forma clara e reutilizável, mantendo o fluxo eficiente com uso de mini-batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7ibG0e8JMw3C"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, train_loader, n_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {mean_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-55fanttzvP8"
      },
      "source": [
        "Durante o treinamento, a função percorre todo o conjunto de treinamento **época por época**, processando cada batch de forma sequencial, como já visto.\n",
        "\n",
        "A primeira linha da função é:\n",
        "\n",
        "- **`model.train()`**  \n",
        "  - Coloca o modelo (e todos os seus submódulos) em **modo de treinamento**.\n",
        "  - Por enquanto, isso não muda nada no comportamento das camadas que estamos usando.\n",
        "  - Porém, será essencial quando utilizarmos camadas como:\n",
        "    - `nn.Dropout`\n",
        "    - `nn.BatchNorm1d`\n",
        "  - Essas camadas **se comportam de maneira diferente** no treinamento e na avaliação.\n",
        "\n",
        "Para usar o modelo fora do treinamento, como em avaliação ou previsão:\n",
        "\n",
        "- **`model.eval()`** deve ser chamado primeiro.\n",
        "- O atributo **`model.training`** indica se o modelo está no modo de treinamento (`True`) ou avaliação (`False`).\n",
        "\n",
        "---\n",
        "\n",
        "### Observação Importante\n",
        "\n",
        "O PyTorch **não fornece** um loop de treinamento pronto.  \n",
        "Você precisa implementá-lo manualmente — o que oferece:\n",
        "\n",
        "✅ Mais controle  \n",
        "✅ Maior clareza  \n",
        "✅ Flexibilidade total\n",
        "\n",
        "Caso prefira um loop mais completo e automatizado, existem bibliotecas baseadas em PyTorch que fornecem:\n",
        "\n",
        "- suporte a multi-GPU\n",
        "- callbacks\n",
        "- logging de métricas\n",
        "- treinamento estruturado\n",
        "\n",
        "📦 Exemplos:\n",
        "- **PyTorch Lightning**\n",
        "- **FastAI**\n",
        "- **Catalyst**\n",
        "- **Keras** (desde a versão 3, também com suporte a PyTorch e JAX)\n",
        "\n",
        "---\n",
        "\n",
        "Após definir a função `train()`, basta chamá-la para iniciar o treinamento do modelo na GPU.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "m7eTytt5Mw3C",
        "outputId": "52e34667-d40d-4ac8-85f9-7733618cc693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 0.5900\n",
            "Epoch 2/20, Loss: 0.4046\n",
            "Epoch 3/20, Loss: 0.3801\n",
            "Epoch 4/20, Loss: 0.3629\n",
            "Epoch 5/20, Loss: 0.3529\n",
            "Epoch 6/20, Loss: 0.3520\n",
            "Epoch 7/20, Loss: 0.3408\n",
            "Epoch 8/20, Loss: 0.3427\n",
            "Epoch 9/20, Loss: 0.3406\n",
            "Epoch 10/20, Loss: 0.3378\n",
            "Epoch 11/20, Loss: 0.3304\n",
            "Epoch 12/20, Loss: 0.3267\n",
            "Epoch 13/20, Loss: 0.3244\n",
            "Epoch 14/20, Loss: 0.3221\n",
            "Epoch 15/20, Loss: 0.3186\n",
            "Epoch 16/20, Loss: 0.3149\n",
            "Epoch 17/20, Loss: 0.3123\n",
            "Epoch 18/20, Loss: 0.3111\n",
            "Epoch 19/20, Loss: 0.3088\n",
            "Epoch 20/20, Loss: 0.3072\n"
          ]
        }
      ],
      "source": [
        "train(model, optimizer, mse, train_loader, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJv-79SCMw3C"
      },
      "source": [
        "# 5. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DX7TKvA0RmL"
      },
      "source": [
        "Para avaliar o desempenho do modelo em um conjunto de dados específico, é criada uma função que:\n",
        "\n",
        "- Recebe o **modelo** e um **DataLoader** contendo os dados de avaliação.\n",
        "- Utiliza uma **função de métrica** para calcular o desempenho em cada batch.\n",
        "- Utiliza uma **função de agregação** (por padrão, a média) para combinar as métricas dos batches e obter o valor final.\n",
        "\n",
        "Essa estrutura permite avaliar o modelo de maneira eficiente, processando os dados em mini-batches e garantindo que o cálculo das métricas seja flexível e reutilizável.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "U6rB7GxPMw3C"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, metric_fn, aggregate_fn=torch.mean):\n",
        "    model.eval()\n",
        "    metrics = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            metric = metric_fn(y_pred, y_batch)\n",
        "            metrics.append(metric)\n",
        "    return aggregate_fn(torch.stack(metrics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKkVJjY16yi2"
      },
      "source": [
        "Para calcular o **MSE (Mean Squared Error)** no conjunto de validação:\n",
        "\n",
        "1. Criar um **TensorDataset** a partir dos tensores de validação `X_valid` e `y_valid`.\n",
        "2. Criar um **DataLoader** para iterar sobre o dataset de validação em mini-batches.\n",
        "3. Passar o DataLoader para a função `evaluate()` junto com a métrica desejada.\n",
        "\n",
        "Dessa forma, podemos calcular eficientemente a métrica de validação sem afetar o modelo treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WnOZYcyRMw3C",
        "outputId": "ffd36aa2-dd9f-47a4-8669-704c9e29f744"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.4080)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "valid_mse = evaluate(model, valid_loader, mse)\n",
        "valid_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjfCXLPp7RlI"
      },
      "source": [
        "Se quisermos utilizar o **RMSE (Root Mean Squared Error)** ao invés do MSE:\n",
        "\n",
        "- O PyTorch não fornece uma função pronta para RMSE.\n",
        "- No entanto, podemos implementar facilmente a função de métrica personalizada:\n",
        "\n",
        "  - Calcular o MSE de cada batch\n",
        "  - Aplicar a **raiz quadrada** do resultado\n",
        "\n",
        "Isso permite obter uma métrica mais interpretável, já que o RMSE está na mesma escala dos valores previstos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qV3Lbqb2Mw3C",
        "outputId": "9491819d-5dd2-4766-b923-1b173931859e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5668)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def rmse(y_pred, y_true):\n",
        "    return ((y_pred - y_true) ** 2).mean().sqrt()\n",
        "\n",
        "evaluate(model, valid_loader, rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-HgfMMO8kvj"
      },
      "source": [
        "O RMSE deveria ser igual à raiz quadrada do MSE. No entanto, ao calcular a raiz quadrada do MSE obtido anteriormente, podemos perceber que o resultado é diferente do esperado.\n",
        "\n",
        "Isso acontece porque o MSE pode ter sido calculado como a média das perdas de cada batch, e tirar a raiz dessa média não é o mesmo que calcular o RMSE de cada batch individualmente e depois agregar.  \n",
        "\n",
        "Portanto, é importante definir claramente como a métrica será agregada ao usar o RMSE para garantir resultados consistentes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Is this conversation helpful so far?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "38HiYl4DMw3C",
        "outputId": "ec183c2d-1736-44bb-a0ab-f80ac1a3023b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6388)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_mse.sqrt()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7v3Kqwd9s1d"
      },
      "source": [
        "O motivo da discrepância é que, em vez de calcular o RMSE sobre **todo o conjunto de validação**, calculamos o RMSE de **cada batch** e depois fizemos a média desses valores.  \n",
        "\n",
        "Essa abordagem **não é matematicamente equivalente** a calcular o RMSE considerando todas as amostras do conjunto de validação de uma só vez.\n",
        "\n",
        "Para resolver isso:\n",
        "\n",
        "- Podemos usar o **MSE como função de métrica** (`metric_fn`);\n",
        "- E definir a função de agregação (`aggregate_fn`) para calcular a **raiz quadrada da média dos MSEs**, obtendo assim o RMSE correto para todo o conjunto.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "B7DVK3iCMw3D",
        "outputId": "6ba90b3e-54cb-4710-acd6-dc6577b27a39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6388)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, valid_loader, mse,\n",
        "         aggregate_fn=lambda metrics: torch.sqrt(torch.mean(metrics)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwMtExdf9vCv"
      },
      "source": [
        "Para simplificar o cálculo de métricas e garantir precisão, podemos usar a biblioteca **TorchMetrics**, criada pela equipe do PyTorch Lightning. Ela oferece métricas de **streaming**, que permitem atualizar os resultados **batch a batch** sem perder o acompanhamento global da métrica.\n",
        "\n",
        "Principais vantagens:\n",
        "\n",
        "- Evita implementar métricas manualmente;\n",
        "- Mantém um registro contínuo das métricas ao longo de todo o conjunto de dados;\n",
        "- Funciona de forma eficiente em mini-batches.\n",
        "\n",
        "No Colab, a biblioteca não vem instalada por padrão, então é necessário executar:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "22CgJlo3Mw3D"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "def evaluate_tm(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()  # redefina a métrica no início.\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            metric.update(y_pred, y_batch)  # atualize-o a cada iteração.\n",
        "    return metric.compute()  # calcule o resultado final no fim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtPJNmjo-RzI"
      },
      "source": [
        "Com a TorchMetrics instalada, podemos:\n",
        "\n",
        "- Criar uma métrica de **RMSE em streaming**, que acompanha o desempenho ao longo de todos os batches;\n",
        "- **Mover a métrica para a GPU** para acelerar os cálculos;\n",
        "- Usá-la para **avaliar o conjunto de validação**, garantindo que o RMSE seja calculado de forma correta e eficiente para todo o dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CP_RhMNzMw3D",
        "outputId": "0c2dffd0-7774-4eb6-9a30-4eadb79457e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.6388)"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "evaluate_tm(model, valid_loader, rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "uK1RLX-LMw3D",
        "outputId": "f4eca81c-e5f7-49e8-db78-2ac7739c3c94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 0.7826, train metric: 0.8847, valid metric: 0.6690\n",
            "Epoch 2/20, train loss: 0.4362, train metric: 0.6605, valid metric: 0.6099\n",
            "Epoch 3/20, train loss: 0.3930, train metric: 0.6269, valid metric: 0.6145\n",
            "Epoch 4/20, train loss: 0.3759, train metric: 0.6132, valid metric: 0.5963\n",
            "Epoch 5/20, train loss: 0.3649, train metric: 0.6040, valid metric: 0.5911\n",
            "Epoch 6/20, train loss: 0.3598, train metric: 0.5999, valid metric: 0.5965\n",
            "Epoch 7/20, train loss: 0.3530, train metric: 0.5941, valid metric: 0.6061\n",
            "Epoch 8/20, train loss: 0.3495, train metric: 0.5911, valid metric: 0.6043\n",
            "Epoch 9/20, train loss: 0.3455, train metric: 0.5877, valid metric: 0.5723\n",
            "Epoch 10/20, train loss: 0.3415, train metric: 0.5844, valid metric: 0.6031\n",
            "Epoch 11/20, train loss: 0.3402, train metric: 0.5831, valid metric: 0.5891\n",
            "Epoch 12/20, train loss: 0.3362, train metric: 0.5799, valid metric: 0.5738\n",
            "Epoch 13/20, train loss: 0.3352, train metric: 0.5788, valid metric: 0.5886\n",
            "Epoch 14/20, train loss: 0.3310, train metric: 0.5754, valid metric: 0.5893\n",
            "Epoch 15/20, train loss: 0.3292, train metric: 0.5737, valid metric: 0.5584\n",
            "Epoch 16/20, train loss: 0.3273, train metric: 0.5722, valid metric: 0.5756\n",
            "Epoch 17/20, train loss: 0.3264, train metric: 0.5714, valid metric: 0.5749\n",
            "Epoch 18/20, train loss: 0.3237, train metric: 0.5690, valid metric: 0.5718\n",
            "Epoch 19/20, train loss: 0.3207, train metric: 0.5662, valid metric: 0.5556\n",
            "Epoch 20/20, train loss: 0.3190, train metric: 0.5649, valid metric: 0.5611\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def train2(model, optimizer, criterion, metric, train_loader, valid_loader, n_epochs):\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        metric.reset()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            model.train()\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "        history[\"train_losses\"].append(mean_loss)\n",
        "        history[\"train_metrics\"].append(metric.compute().item())\n",
        "        history[\"valid_metrics\"].append(evaluate_tm(model, valid_loader, metric).item())\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
        "    return history\n",
        "\n",
        "torch.manual_seed(42)\n",
        "learning_rate = 0.01\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_features, 50), nn.ReLU(),\n",
        "    nn.Linear(50, 40), nn.ReLU(),\n",
        "    nn.Linear(40, 30), nn.ReLU(),\n",
        "    nn.Linear(30, 1)\n",
        ")\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train2(model, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                 n_epochs)\n",
        "\n",
        "# Since we compute the training metric\n",
        "plt.plot(np.arange(n_epochs) + 0.5, history[\"train_metrics\"], \".--\",\n",
        "         label=\"Training\")\n",
        "plt.plot(np.arange(n_epochs) + 1.0, history[\"valid_metrics\"], \".-\",\n",
        "         label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.grid()\n",
        "plt.title(\"Learning curves\")\n",
        "plt.axis([0.5, 20, 0.4, 1.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppT-e2rrBXsG"
      },
      "source": [
        "Agora que conseguimos calcular métricas corretamente, podemos atualizar a função `train()` para:\n",
        "\n",
        "- Avaliar o desempenho do modelo **durante o treinamento** no conjunto de treino a cada época;\n",
        "- Avaliar o desempenho no **conjunto de validação** ao final de cada época.\n",
        "\n",
        "### Observações importantes:\n",
        "\n",
        "- Se a performance no conjunto de treino for muito melhor que no conjunto de validação, isso pode indicar:\n",
        "  - **Overfitting**;\n",
        "  - Algum **erro nos dados**, como desajuste entre o conjunto de treino e validação.\n",
        "- É recomendável **plotar as curvas de aprendizado** para analisar essas diferenças, usando:\n",
        "  - **Matplotlib**;\n",
        "  - Ou ferramentas de visualização como **TensorBoard**.\n",
        "\n",
        "---\n",
        "\n",
        "## Próximos Passos\n",
        "\n",
        "Até agora, aprendemos a:\n",
        "\n",
        "- Construir, treinar e avaliar um **MLP de regressão** com PyTorch;\n",
        "- Utilizar o modelo treinado para fazer previsões.\n",
        "\n",
        "No entanto, tratamos apenas de **modelos sequenciais simples**, formados por camadas lineares e funções de ativação ReLU.  \n",
        "\n",
        "Para construir **modelos mais complexos e não sequenciais**, será necessário criar **módulos personalizados**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOo506bsMw3D"
      },
      "source": [
        "# 6. Building Nonsequential Models Using Custom Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJT-dGRMCSG7"
      },
      "source": [
        "Alguns modelos de redes neurais precisam receber **várias entradas** que não podem ser facilmente combinadas em um único tensor. Isso ocorre, por exemplo, quando as entradas possuem **diferentes números de dimensões**, como no caso de se alimentar a rede neural com **imagens e textos** ao mesmo tempo.  \n",
        "\n",
        "Para lidar com essas situações, precisamos **adaptar o método `forward()`** do modelo para aceitar e processar múltiplas entradas separadamente.  \n",
        "\n",
        "No caso do modelo **Wide & Deep**, que combina diferentes tipos de dados (wide features e deep features), essa adaptação permite que cada entrada seja processada adequadamente antes de serem combinadas na rede.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\" style=\"margin-top:20px; margin-bottom:20px;\">\n",
        "  \n",
        "![wide_deep](Aula_Imagens\\wide_deep.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "z1xgvOF-Mw3D"
      },
      "outputs": [],
      "source": [
        "class WideAndDeep(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.deep_stack = nn.Sequential(\n",
        "            nn.Linear(n_features, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 40), nn.ReLU(),\n",
        "            nn.Linear(40, 30), nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(30 + n_features, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        deep_output = self.deep_stack(X)\n",
        "        wide_and_deep = torch.concat([X, deep_output], dim=1)\n",
        "        return self.output_layer(wide_and_deep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmwTklXvLRVl"
      },
      "source": [
        "No PyTorch, podemos incorporar **qualquer módulo** dentro de um módulo personalizado. No modelo **Wide & Deep**, a parte *deep* é uma MLP construída com **`nn.Sequential`**, e a **camada de saída** fica separada porque precisa receber a **concatenação da entrada original `X` com a saída da parte deep**. Como no código a MLP termina em 30 neurônios, a camada final recebe **`30 + n_features`** entradas.\n",
        "\n",
        "No `forward()` fazemos:\n",
        "\n",
        "1. Passar `X` pela stack *deep* (MLP) para obter `deep_output`.\n",
        "2. **Concatenar** `X` e `deep_output` ao longo do eixo de features (`dim=1`).\n",
        "3. Enviar essa concatenação para a **camada de saída**.\n",
        "\n",
        "Depois de definir o módulo, seguimos o fluxo usual: **instanciar**, **mover para GPU** (se disponível) e **treinar/avaliar** como qualquer outro modelo. Essa abordagem combina um caminho *wide* (features originais, bom para padrões lineares/co-ocorrências) com um caminho *deep* (interações **não lineares**), mantendo um pipeline simples e flexível para lidar com **entradas múltiplas** e **combinações de módulos**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Pko67YJ2Mw3D"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model = WideAndDeep(n_features).to(device)\n",
        "learning_rate = 0.002  # the model changed, so did the optimal learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">`momentum` é um “impulso” no **SGD**: em vez de atualizar só com o gradiente do passo atual, o otimizador mantém uma **velocidade** (v) que acumula gradientes recentes (média exponencial). As regras são ($v \\leftarrow \\mu v + \\nabla_\\theta L$) e ($\\theta \\leftarrow \\theta - \\text{lr}, v$), onde ($\\mu$) é o `momentum` (ex.: 0.9). Isso **suaviza ruído**, acelera na direção consistente e **reduz zigue-zague** em vales alongados. Quando você usa `momentum=0`, vira **SGD puro** (sem memória do passado). Valores típicos: **0.9** ou **0.95**; às vezes **0.99** em problemas muito ruidosos. Se ativar `nesterov=True`, aplica a variante **Nesterov**, que “antecipa” o passo usando a velocidade (pode convergir um pouco melhor).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "mFuWFRDTMw3D",
        "outputId": "847fdf66-af0b-4378-bbdf-7eb8c9601115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 1.7802, train metric: 1.3344, valid metric: 0.8690\n",
            "Epoch 2/20, train loss: 0.6201, train metric: 0.7875, valid metric: 0.9492\n",
            "Epoch 3/20, train loss: 0.5900, train metric: 0.7682, valid metric: 0.7331\n",
            "Epoch 4/20, train loss: 0.5607, train metric: 0.7488, valid metric: 0.7771\n",
            "Epoch 5/20, train loss: 0.5408, train metric: 0.7353, valid metric: 0.7967\n",
            "Epoch 6/20, train loss: 0.5244, train metric: 0.7241, valid metric: 0.7098\n",
            "Epoch 7/20, train loss: 0.5070, train metric: 0.7119, valid metric: 0.7419\n",
            "Epoch 8/20, train loss: 0.4941, train metric: 0.7030, valid metric: 0.6750\n",
            "Epoch 9/20, train loss: 0.4798, train metric: 0.6928, valid metric: 0.6762\n",
            "Epoch 10/20, train loss: 0.4657, train metric: 0.6825, valid metric: 0.6678\n",
            "Epoch 11/20, train loss: 0.4538, train metric: 0.6736, valid metric: 0.6617\n",
            "Epoch 12/20, train loss: 0.4441, train metric: 0.6665, valid metric: 0.6651\n",
            "Epoch 13/20, train loss: 0.4328, train metric: 0.6580, valid metric: 0.6803\n",
            "Epoch 14/20, train loss: 0.4232, train metric: 0.6506, valid metric: 0.6288\n",
            "Epoch 15/20, train loss: 0.4139, train metric: 0.6434, valid metric: 0.6202\n",
            "Epoch 16/20, train loss: 0.4063, train metric: 0.6374, valid metric: 0.6216\n",
            "Epoch 17/20, train loss: 0.3991, train metric: 0.6317, valid metric: 0.6129\n",
            "Epoch 18/20, train loss: 0.3937, train metric: 0.6274, valid metric: 0.6031\n",
            "Epoch 19/20, train loss: 0.3879, train metric: 0.6228, valid metric: 0.6092\n",
            "Epoch 20/20, train loss: 0.3834, train metric: 0.6193, valid metric: 0.5957\n"
          ]
        }
      ],
      "source": [
        "# extra code: train the model, exactly our previous models\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train2(model, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                 n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oraxe21Lah3"
      },
      "source": [
        "Em alguns casos, pode ser necessário enviar **subconjuntos diferentes de features** por caminhos distintos do modelo, como:\n",
        "\n",
        "- Algumas features passam pelo **caminho wide**.\n",
        "- Outras features (possivelmente sobrepostas) passam pelo **caminho deep**.\n",
        "\n",
        "Para isso, uma abordagem comum é **dividir as entradas dentro do método `forward()`**.  \n",
        "\n",
        "Isso permite controlar exatamente quais features alimentam cada parte da rede, garantindo maior flexibilidade no processamento dos dados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "xN4qtgAeMw3E"
      },
      "outputs": [],
      "source": [
        "class WideAndDeepV2(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.deep_stack = nn.Sequential(\n",
        "            nn.Linear(n_features - 2, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 40), nn.ReLU(),\n",
        "            nn.Linear(40, 30), nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(30 + 5, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X_wide = X[:, :5]\n",
        "        X_deep = X[:, 2:]\n",
        "        deep_output = self.deep_stack(X_deep)\n",
        "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
        "        return self.output_layer(wide_and_deep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`WideAndDeepV2` divide as features em dois ramos: **wide** (5 primeiras colunas) e **deep** (todas as colunas a partir da 3ª). No `__init__`, a MLP (`deep_stack`) recebe `n_features - 2` entradas (bate com `X_deep = X[:, 2:]`, que ignora as duas primeiras colunas) e produz um vetor de tamanho **30**. A camada final (`output_layer`) recebe a **concatenação** de `X_wide` (tamanho **5**) com `deep_output` (tamanho **30**), totalizando **30 + 5** entradas, e gera uma saída escalar.\n",
        "\n",
        "No `forward`:\n",
        "\n",
        "1. `X_wide = X[:, :5]` pega as 5 primeiras features (caminho “memorizações lineares”).\n",
        "2. `X_deep = X[:, 2:]` pega do índice 2 em diante (ou seja, descarta as duas primeiras) para o caminho não linear.\n",
        "3. `deep_output = self.deep_stack(X_deep)` processa no MLP.\n",
        "4. `torch.concat([X_wide, deep_output], dim=1)` junta por **colunas** (mesmo batch).\n",
        "5. `self.output_layer(...)` produz a predição.\n",
        "\n",
        "Pré-condições e cuidados: **(a)** `n_features ≥ 5` (senão quebra); **(b)** as dimensões precisam bater: `nn.Linear(n_features - 2, 50)` ⇔ `X[:, 2:]`; **(c)** a **ordem/seleção** das colunas tem de ser estável do treino à inferência; **(d)** se mudar o nº de colunas no ramo wide, ajuste o `in_features` da `output_layer`. Para classificação binária, use `BCEWithLogitsLoss` (e `sigmoid` na inferência); para regressão, `MSELoss`/`RMSE`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uW-6d5TLwWM"
      },
      "source": [
        "Embora seja possível dividir os inputs dentro do método `forward()`, muitas vezes é **mais conveniente que o modelo aceite dois tensores separados** como entrada.  \n",
        "\n",
        "Vantagens dessa abordagem:\n",
        "\n",
        "- **Mais claro e organizado**: cada tensor representa explicitamente um subconjunto de features.\n",
        "- **Evita confusões** ao separar manualmente os dados dentro do forward.\n",
        "- **Facilita a reutilização** do modelo em diferentes contextos, como combinar imagens e textos ou diferentes tipos de dados estruturados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "JUnuam2-Mw3E"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model = WideAndDeepV2(n_features).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9s4nF41HMw3E",
        "outputId": "2161426c-02a6-4d7d-8061-3ece239f16d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 1.8482, train metric: 1.3598, valid metric: 0.9100\n",
            "Epoch 2/20, train loss: 0.6282, train metric: 0.7927, valid metric: 0.8028\n",
            "Epoch 3/20, train loss: 0.5763, train metric: 0.7591, valid metric: 0.7567\n",
            "Epoch 4/20, train loss: 0.5413, train metric: 0.7356, valid metric: 0.7290\n",
            "Epoch 5/20, train loss: 0.5099, train metric: 0.7142, valid metric: 0.7011\n",
            "Epoch 6/20, train loss: 0.4841, train metric: 0.6958, valid metric: 0.6816\n",
            "Epoch 7/20, train loss: 0.4656, train metric: 0.6824, valid metric: 0.6670\n",
            "Epoch 8/20, train loss: 0.4526, train metric: 0.6728, valid metric: 0.6576\n",
            "Epoch 9/20, train loss: 0.4438, train metric: 0.6662, valid metric: 0.6539\n",
            "Epoch 10/20, train loss: 0.4380, train metric: 0.6618, valid metric: 0.6498\n",
            "Epoch 11/20, train loss: 0.4326, train metric: 0.6577, valid metric: 0.6470\n",
            "Epoch 12/20, train loss: 0.4284, train metric: 0.6546, valid metric: 0.6447\n",
            "Epoch 13/20, train loss: 0.4253, train metric: 0.6521, valid metric: 0.6452\n",
            "Epoch 14/20, train loss: 0.4216, train metric: 0.6494, valid metric: 0.6468\n",
            "Epoch 15/20, train loss: 0.4190, train metric: 0.6473, valid metric: 0.6484\n",
            "Epoch 16/20, train loss: 0.4169, train metric: 0.6458, valid metric: 0.6452\n",
            "Epoch 17/20, train loss: 0.4144, train metric: 0.6438, valid metric: 0.6459\n",
            "Epoch 18/20, train loss: 0.4118, train metric: 0.6417, valid metric: 0.6470\n",
            "Epoch 19/20, train loss: 0.4101, train metric: 0.6404, valid metric: 0.6475\n",
            "Epoch 20/20, train loss: 0.4082, train metric: 0.6389, valid metric: 0.6493\n"
          ]
        }
      ],
      "source": [
        "# extra code: train the model, exactly our previous models\n",
        "learning_rate = 0.002\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train2(model, optimizer, mse, rmse, train_loader, valid_loader,\n",
        "                 n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8whgJQhFMw3E"
      },
      "source": [
        "## 6.1 Building Models with Multiple Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BslPo-MAL74z"
      },
      "source": [
        "Alguns modelos de redes neurais precisam receber **mais de uma entrada** que não podem ser facilmente combinadas em um único tensor. Isso acontece, por exemplo, quando as entradas possuem **diferentes números de dimensões**, como ao alimentar a rede com **imagens e textos simultaneamente**.\n",
        "\n",
        "Para que o modelo **Wide & Deep** aceite **duas entradas separadas** (como ilustrado na Figura abaixo), é necessário **adaptar o método `forward()`** do modelo para processar cada entrada individualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\" style=\"margin-top:20px; margin-bottom:20px;\">\n",
        "  \n",
        "![concat](Aula_Imagens\\concat.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "I7lv_HTBMw3E"
      },
      "outputs": [],
      "source": [
        "class WideAndDeepV3(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.deep_stack = nn.Sequential(\n",
        "            nn.Linear(n_features - 2, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 40), nn.ReLU(),\n",
        "            nn.Linear(40, 30), nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(30 + 5, 1)\n",
        "\n",
        "    def forward(self, X_wide, X_deep):\n",
        "        deep_output = self.deep_stack(X_deep)\n",
        "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
        "        return self.output_layer(wide_and_deep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui, o `WideAndDeepV3` passa a **receber duas entradas separadas** no `forward(X_wide, X_deep)`: o ramo *deep* processa `X_deep` com a MLP (`deep_stack`) e o ramo *wide* usa `X_wide` **diretamente**. Depois, concatenamos os dois vetores ao longo do eixo de features (`dim=1`) e mandamos para a `output_layer`. Assim, você pode alimentar o modelo com **tensores distintos** (ex.: campos categóricos one-hot no *wide* e embeddings/numéricos no *deep*), sem forçar tudo para um único tensor. Pré-condições: `X_wide.shape[1]` precisa ser **5** (como definido na `output_layer`) e `X_deep.shape[1]` precisa ser **`n_features - 2`** (como definido na primeira `Linear`). No treino/validação, o seu `DataLoader` deve devolver **triplas** `(X_wide, X_deep, y)` e a chamada vira `y_pred = model(X_wide, X_deep)`; o resto do loop (loss, `backward`, `step`, `zero_grad`) permanece igual. Exemplo mínimo de uso: `pred = model(X[:, :5], X[:, 2:])` (desde que essas fatias respeitem as dimensões esperadas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxf7X25AMQPu"
      },
      "source": [
        "Para trabalhar com múltiplas entradas no modelo Wide & Deep, precisamos criar **datasets que retornem separadamente as entradas para o caminho wide e para o caminho deep**.\n",
        "\n",
        "Isso garante que cada subconjunto de features seja fornecido ao caminho correto da rede, mantendo a consistência e a clareza do fluxo de dados durante o treinamento e a avaliação.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "obd9jF07Mw3E"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "# Divisão da Base de dados (tensores) para Wide\n",
        "#Train\n",
        "train_data_wd = TensorDataset(X_train[:, :5], X_train[:, 2:], y_train)\n",
        "train_loader_wd = DataLoader(train_data_wd, batch_size=32, shuffle=True)\n",
        "#Valid\n",
        "valid_data_wd = TensorDataset(X_valid[:, :5], X_valid[:, 2:], y_valid)\n",
        "valid_loader_wd = DataLoader(valid_data_wd, batch_size=32)\n",
        "#Test\n",
        "test_data_wd = TensorDataset(X_test[:, :5], X_test[:, 2:], y_test)\n",
        "test_loader_wd = DataLoader(test_data_wd, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esse trecho cria **datasets e data loaders** para o modelo Wide & Deep com **duas entradas por amostra**. `TensorDataset(X_train[:, :5], X_train[:, 2:], y_train)` empacota três tensores: o **ramo wide** (`X[:, :5]`, primeiras 5 colunas), o **ramo deep** (`X[:, 2:]`, da 3ª coluna em diante) e o **alvo** `y`. O `DataLoader(..., batch_size=32, shuffle=True)` entrega lotes embaralhados de 32 amostras; nos valid/test não embaralhamos. Assim, no loop você recebe **triplas** `(X_wide, X_deep, y)` e chama `model(X_wide, X_deep)`. O `torch.manual_seed(42)` fixa a aleatoriedade (por exemplo, a ordem de embaralhamento). Atenção: as fatias usadas aqui precisam **bater** com as dimensões esperadas pelo modelo (`X_wide` com 5 colunas; `X_deep` com `n_features-2`) e devem ser **consistentes** em train/valid/test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "8urBGzhpMw3E",
        "outputId": "d6535768-ef7f-4453-9e60-1e775e2dcb29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 0.8366, train metric: 0.9148, valid metric: 0.6892\n",
            "Epoch 2/20, train loss: 0.4627, train metric: 0.6803, valid metric: 0.6455\n",
            "Epoch 3/20, train loss: 0.4319, train metric: 0.6572, valid metric: 0.6374\n",
            "Epoch 4/20, train loss: 0.4259, train metric: 0.6525, valid metric: 0.6512\n",
            "Epoch 5/20, train loss: 0.4120, train metric: 0.6420, valid metric: 0.6305\n",
            "Epoch 6/20, train loss: 0.4040, train metric: 0.6356, valid metric: 0.6287\n",
            "Epoch 7/20, train loss: 0.4005, train metric: 0.6330, valid metric: 0.6252\n",
            "Epoch 8/20, train loss: 0.3976, train metric: 0.6306, valid metric: 0.6158\n",
            "Epoch 9/20, train loss: 0.3883, train metric: 0.6230, valid metric: 0.7407\n",
            "Epoch 10/20, train loss: 0.3866, train metric: 0.6218, valid metric: 0.6063\n",
            "Epoch 11/20, train loss: 0.3752, train metric: 0.6125, valid metric: 0.5974\n",
            "Epoch 12/20, train loss: 0.3704, train metric: 0.6087, valid metric: 0.5887\n",
            "Epoch 13/20, train loss: 0.3677, train metric: 0.6063, valid metric: 0.5982\n",
            "Epoch 14/20, train loss: 0.3604, train metric: 0.6004, valid metric: 0.5841\n",
            "Epoch 15/20, train loss: 0.3592, train metric: 0.5994, valid metric: 0.6401\n",
            "Epoch 16/20, train loss: 0.3640, train metric: 0.6034, valid metric: 0.5747\n",
            "Epoch 17/20, train loss: 0.3536, train metric: 0.5947, valid metric: 0.5686\n",
            "Epoch 18/20, train loss: 0.3461, train metric: 0.5884, valid metric: 0.5679\n",
            "Epoch 19/20, train loss: 0.3436, train metric: 0.5862, valid metric: 0.5623\n",
            "Epoch 20/20, train loss: 0.3415, train metric: 0.5844, valid metric: 0.5696\n"
          ]
        }
      ],
      "source": [
        "def evaluate_multi_in(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()  # reset the metric at the beginning\n",
        "    with torch.no_grad():\n",
        "        for X_batch_wide, X_batch_deep, y_batch in data_loader:\n",
        "            X_batch_wide = X_batch_wide.to(device)\n",
        "            X_batch_deep = X_batch_deep.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(X_batch_wide, X_batch_deep)\n",
        "            metric.update(y_pred, y_batch)  # update it at each iteration\n",
        "    return metric.compute()  # compute the final result at the end\n",
        "\n",
        "def train_multi_in(model, optimizer, criterion, metric, train_loader,\n",
        "                   valid_loader, n_epochs):\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        metric.reset()\n",
        "        for *X_batch_inputs, y_batch in train_loader:\n",
        "            model.train()\n",
        "            X_batch_inputs = [X.to(device) for X in X_batch_inputs]\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(*X_batch_inputs)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "        history[\"train_losses\"].append(mean_loss)\n",
        "        history[\"train_metrics\"].append(metric.compute().item())\n",
        "        history[\"valid_metrics\"].append(\n",
        "            evaluate_multi_in(model, valid_loader, metric).item())\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
        "    return history\n",
        "\n",
        "torch.manual_seed(42)\n",
        "learning_rate = 0.01\n",
        "model = WideAndDeepV3(n_features).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train_multi_in(model, optimizer, mse, rmse, train_loader_wd,\n",
        "                         valid_loader_wd, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* `model.eval()` — coloca o modelo em **modo avaliação**: desativa comportamentos de treino (ex.: *dropout*) e faz *BatchNorm* usar **estatísticas fixas**.\n",
        "\n",
        "* `metric.reset()` — **zera o estado interno** da métrica do `torchmetrics` (contadores/somas) para começar uma nova medição limpa.\n",
        "\n",
        "* `X_batch_inputs = [X.to(device) for X in X_batch_inputs]` — move **todas as entradas do batch** (os dois ramos: `X_wide`, `X_deep`, etc.) para o **mesmo device** do modelo (GPU/CPU), mantendo a ordem; depois elas são passadas como `model(*X_batch_inputs)`.\n",
        "\n",
        "* `metric.update(y_pred, y_batch)` — **acumula** o resultado desse batch dentro da métrica (sem calcular ainda o valor final). No fim da época, `metric.compute()` usa tudo que foi acumulado para devolver a métrica **global**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGDCAjXuMaUu"
      },
      "source": [
        "Quando o modelo possui **múltiplas entradas**, é fácil confundir a ordem dos tensores, o que pode gerar **erros difíceis de depurar**.\n",
        "\n",
        "Para evitar isso, uma boa prática é **nomear cada entrada**.  \n",
        "\n",
        "Uma abordagem é criar um **dataset personalizado** que retorne um **dicionário**, onde as chaves são os nomes das entradas e os valores são os tensores correspondentes.  \n",
        "\n",
        "Isso torna o código mais legível e reduz a chance de erros ao alimentar o modelo.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "dkkt0E7KMw3E"
      },
      "outputs": [],
      "source": [
        "class WideAndDeepDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X_wide, X_deep, y):\n",
        "        self.X_wide = X_wide\n",
        "        self.X_deep = X_deep\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_dict = {\"X_wide\": self.X_wide[idx], \"X_deep\": self.X_deep[idx]}\n",
        "        return input_dict, self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV8_Ex2oMhyk"
      },
      "source": [
        "Após definir como as entradas serão retornadas (separadas ou nomeadas), o próximo passo é **criar os datasets e os data loaders**.\n",
        "\n",
        "- **Datasets**: fornecem os dados ao modelo de forma estruturada, retornando os tensores correspondentes a cada entrada.\n",
        "- **Data Loaders**: permitem iterar sobre os datasets em **mini-batches**, embaralhar os dados e facilitar o treinamento eficiente no PyTorch.\n",
        "\n",
        "Essa etapa garante que o modelo receba os dados corretamente e de forma eficiente durante o treinamento e avaliação.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "8WuTt7s8Mw3E"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "train_data_named = WideAndDeepDataset(\n",
        "    X_wide=X_train[:, :5], X_deep=X_train[:, 2:], y=y_train)\n",
        "train_loader_named = DataLoader(train_data_named, batch_size=32, shuffle=True)\n",
        "valid_data_named = WideAndDeepDataset(\n",
        "    X_wide=X_valid[:, :5], X_deep=X_valid[:, 2:], y=y_valid)\n",
        "valid_loader_named = DataLoader(valid_data_named, batch_size=32)\n",
        "test_data_named = WideAndDeepDataset(\n",
        "    X_wide=X_test[:, :5], X_deep=X_test[:, 2:], y=y_test)\n",
        "test_loader_named = DataLoader(test_data_named, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "9gXnzV15Mw3F",
        "outputId": "f5ca1d77-8ada-43de-8c68-a2a4d7ade86a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 0.8366, train metric: 0.9148, valid metric: 0.6892\n",
            "Epoch 2/20, train loss: 0.4627, train metric: 0.6803, valid metric: 0.6455\n",
            "Epoch 3/20, train loss: 0.4319, train metric: 0.6572, valid metric: 0.6374\n",
            "Epoch 4/20, train loss: 0.4259, train metric: 0.6525, valid metric: 0.6512\n",
            "Epoch 5/20, train loss: 0.4120, train metric: 0.6420, valid metric: 0.6305\n",
            "Epoch 6/20, train loss: 0.4040, train metric: 0.6356, valid metric: 0.6287\n",
            "Epoch 7/20, train loss: 0.4005, train metric: 0.6330, valid metric: 0.6252\n",
            "Epoch 8/20, train loss: 0.3976, train metric: 0.6306, valid metric: 0.6158\n",
            "Epoch 9/20, train loss: 0.3883, train metric: 0.6230, valid metric: 0.7407\n",
            "Epoch 10/20, train loss: 0.3866, train metric: 0.6218, valid metric: 0.6063\n",
            "Epoch 11/20, train loss: 0.3752, train metric: 0.6125, valid metric: 0.5974\n",
            "Epoch 12/20, train loss: 0.3704, train metric: 0.6087, valid metric: 0.5887\n",
            "Epoch 13/20, train loss: 0.3677, train metric: 0.6063, valid metric: 0.5982\n",
            "Epoch 14/20, train loss: 0.3604, train metric: 0.6004, valid metric: 0.5841\n",
            "Epoch 15/20, train loss: 0.3592, train metric: 0.5994, valid metric: 0.6401\n",
            "Epoch 16/20, train loss: 0.3640, train metric: 0.6034, valid metric: 0.5747\n",
            "Epoch 17/20, train loss: 0.3536, train metric: 0.5947, valid metric: 0.5686\n",
            "Epoch 18/20, train loss: 0.3461, train metric: 0.5884, valid metric: 0.5679\n",
            "Epoch 19/20, train loss: 0.3436, train metric: 0.5862, valid metric: 0.5623\n",
            "Epoch 20/20, train loss: 0.3415, train metric: 0.5844, valid metric: 0.5696\n"
          ]
        }
      ],
      "source": [
        "def evaluate_named(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()  # reset the metric at the beginning\n",
        "    with torch.no_grad():\n",
        "        for inputs, y_batch in data_loader:\n",
        "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(X_wide=inputs[\"X_wide\"], X_deep=inputs[\"X_deep\"])\n",
        "            metric.update(y_pred, y_batch)\n",
        "    return metric.compute()  # compute the final result at the end\n",
        "\n",
        "def train_named(model, optimizer, criterion, metric, train_loader,\n",
        "                   valid_loader, n_epochs):\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        metric.reset()\n",
        "        for inputs, y_batch in train_loader:\n",
        "            model.train()\n",
        "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(**inputs)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "        history[\"train_losses\"].append(mean_loss)\n",
        "        history[\"train_metrics\"].append(metric.compute().item())\n",
        "        history[\"valid_metrics\"].append(\n",
        "            evaluate_named(model, valid_loader, metric).item())\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
        "    return history\n",
        "\n",
        "torch.manual_seed(42)\n",
        "learning_rate = 0.01\n",
        "model = WideAndDeepV3(n_features).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train_named(model, optimizer, mse, rmse, train_loader_named,\n",
        "                      valid_loader_named, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdaO98xVMw3F"
      },
      "source": [
        "## 6.2 Building Models with Multiple Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WyUNPYGNZx7"
      },
      "source": [
        "\n",
        "Existem diversos cenários em que **uma rede neural precisa ter múltiplas saídas**:\n",
        "\n",
        "### 1. Tarefas que exigem múltiplos tipos de previsão\n",
        "- Por exemplo, localizar e classificar o objeto principal em uma imagem.\n",
        "- Isso combina **regressão** (localização) e **classificação** (tipo do objeto) na mesma rede.\n",
        "\n",
        "### 2. Multitask Learning (Aprendizado Multitarefa)\n",
        "- Quando temos **múltimas tarefas independentes** com base nos mesmos dados.\n",
        "- Treinar uma rede por tarefa é possível, mas **treinar uma única rede com uma saída por tarefa** geralmente gera melhores resultados.\n",
        "- A rede aprende **features compartilhadas úteis** para todas as tarefas.\n",
        "- Exemplo: classificar expressões faciais (sorriso, surpresa) e identificar se a pessoa usa óculos simultaneamente.\n",
        "\n",
        "### 3. Regularização com Saídas Auxiliares\n",
        "- Adicionar uma saída auxiliar ajuda a **reduzir overfitting** e melhorar a generalização.\n",
        "- A saída auxiliar força a **parte subjacente da rede** a aprender algo útil por si só.\n",
        "- Exemplo no modelo **Wide & Deep**:\n",
        "  - A saída da stack deep tem dimensão 40.\n",
        "  - O alvo tem dimensão 1.\n",
        "  - Adicionamos uma camada **`nn.Linear`** para mapear de 40 → 1 para a saída auxiliar.\n",
        "  - O método `forward()` deve calcular **tanto a saída principal quanto a saída auxiliar**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align=\"center\" style=\"margin-top:20px; margin-bottom:20px;\">\n",
        "  \n",
        "![concat](Aula_Imagens\\aux_out.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Nhd4d4HtMw3F"
      },
      "outputs": [],
      "source": [
        "class WideAndDeepV4(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.deep_stack = nn.Sequential(\n",
        "            nn.Linear(n_features - 2, 50), nn.ReLU(),\n",
        "            nn.Linear(50, 40), nn.ReLU(),\n",
        "            nn.Linear(40, 30), nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(30 + 5, 1)\n",
        "        self.aux_output_layer = nn.Linear(30, 1)\n",
        "\n",
        "    def forward(self, X_wide, X_deep):\n",
        "        deep_output = self.deep_stack(X_deep)\n",
        "        wide_and_deep = torch.concat([X_wide, deep_output], dim=1)\n",
        "        main_output = self.output_layer(wide_and_deep)\n",
        "        aux_output = self.aux_output_layer(deep_output)\n",
        "        return main_output, aux_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`WideAndDeepV4` é um **Wide & Deep com saída auxiliar**: o ramo *deep* (MLP) produz um vetor de 30 unidades; dele saem duas cabeças. A **saída principal** (`main_output`) recebe a concatenação do *wide* (5 colunas de entrada direta) com o `deep_output` (30) e passa por `Linear(35→1)`; já a **saída auxiliar** (`aux_output`) conecta **diretamente** o `deep_output` em `Linear(30→1)`. A ideia é fazer **supervisão auxiliar** no ramo profundo para estabilizar e regularizar o treino (o gradiente chega mais “perto” das camadas internas). Na prática, você treina somando perdas, por exemplo: `loss = L_main(main_output, y) + λ * L_aux(aux_output, y)`, com `λ` pequeno (ex.: `0.1`). Em inferência, usa-se só a **saída principal**. Cuidados: `X_wide` deve ter 5 features, `X_deep` deve ter `n_features-2`; as duas saídas têm shape `(batch, 1)` para regressão (ou ajuste a cabeça para classificação conforme o problema).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "GdyhKzwhMw3F",
        "outputId": "2f4e7029-a84f-40b3-ab3d-67e45252af9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 1.0693, train metric: 0.9506, valid metric: 0.7085\n",
            "Epoch 2/20, train loss: 0.5817, train metric: 0.6946, valid metric: 0.6607\n",
            "Epoch 3/20, train loss: 0.5010, train metric: 0.6581, valid metric: 0.6425\n",
            "Epoch 4/20, train loss: 0.4690, train metric: 0.6497, valid metric: 0.6654\n",
            "Epoch 5/20, train loss: 0.4503, train metric: 0.6420, valid metric: 0.6338\n",
            "Epoch 6/20, train loss: 0.4387, train metric: 0.6373, valid metric: 0.6563\n",
            "Epoch 7/20, train loss: 0.4315, train metric: 0.6330, valid metric: 0.6193\n",
            "Epoch 8/20, train loss: 0.4249, train metric: 0.6302, valid metric: 0.6167\n",
            "Epoch 9/20, train loss: 0.4116, train metric: 0.6202, valid metric: 0.6450\n",
            "Epoch 10/20, train loss: 0.4085, train metric: 0.6198, valid metric: 0.5938\n",
            "Epoch 11/20, train loss: 0.4073, train metric: 0.6196, valid metric: 0.5959\n",
            "Epoch 12/20, train loss: 0.3914, train metric: 0.6077, valid metric: 0.6074\n",
            "Epoch 13/20, train loss: 0.3847, train metric: 0.6033, valid metric: 0.5813\n",
            "Epoch 14/20, train loss: 0.3848, train metric: 0.6047, valid metric: 0.6032\n",
            "Epoch 15/20, train loss: 0.3745, train metric: 0.5965, valid metric: 0.5729\n",
            "Epoch 16/20, train loss: 0.3693, train metric: 0.5929, valid metric: 0.6017\n",
            "Epoch 17/20, train loss: 0.3670, train metric: 0.5919, valid metric: 0.6049\n",
            "Epoch 18/20, train loss: 0.3612, train metric: 0.5873, valid metric: 0.5677\n",
            "Epoch 19/20, train loss: 0.3597, train metric: 0.5863, valid metric: 0.5738\n",
            "Epoch 20/20, train loss: 0.3556, train metric: 0.5831, valid metric: 0.6082\n"
          ]
        }
      ],
      "source": [
        "import torchmetrics\n",
        "\n",
        "def evaluate_multi_out(model, data_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for inputs, y_batch in data_loader:\n",
        "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred, _ = model(**inputs)\n",
        "            metric.update(y_pred, y_batch)\n",
        "    return metric.compute()\n",
        "\n",
        "def train_multi_out(model, optimizer, criterion, metric, train_loader,\n",
        "                   valid_loader, n_epochs):\n",
        "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        metric.reset()\n",
        "        for inputs, y_batch in train_loader:\n",
        "            model.train()\n",
        "            inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred, y_pred_aux = model(**inputs)\n",
        "            main_loss = criterion(y_pred, y_batch)\n",
        "            aux_loss = criterion(y_pred_aux, y_batch)\n",
        "            loss = 0.8 * main_loss + 0.2 * aux_loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            metric.update(y_pred, y_batch)\n",
        "        mean_loss = total_loss / len(train_loader)\n",
        "        history[\"train_losses\"].append(mean_loss)\n",
        "        history[\"train_metrics\"].append(metric.compute().item())\n",
        "        history[\"valid_metrics\"].append(\n",
        "            evaluate_multi_out(model, valid_loader, metric).item())\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
        "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
        "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
        "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
        "    return history\n",
        "\n",
        "torch.manual_seed(42)\n",
        "learning_rate = 0.01\n",
        "model = WideAndDeepV4(n_features).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
        "mse = nn.MSELoss()\n",
        "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
        "history = train_multi_out(model, optimizer, mse, rmse, train_loader_named,\n",
        "                          valid_loader_named, n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* **Entradas nomeadas (dict) e expansão com `**`**\n",
        "  Agora o `DataLoader` entrega `inputs` como **dicionário** (ex.: `{\"X_wide\": ..., \"X_deep\": ...}`), não mais tupla.\n",
        "  Por isso usamos:\n",
        "\n",
        "  ```python\n",
        "  inputs = {name: X.to(device) for name, X in inputs.items()}\n",
        "  y_pred, y_pred_aux = model(**inputs)   # chaves do dict devem bater com a assinatura do forward\n",
        "  ```\n",
        "\n",
        "  > Diferente de antes, quando fazíamos `model(X_wide, X_deep)` diretamente.\n",
        "\n",
        "* **Duas saídas (principal + auxiliar)**\n",
        "  O modelo retorna **duas predições**: `y_pred` (principal) e `y_pred_aux` (auxiliar).\n",
        "  Na avaliação, só usamos a **principal**:\n",
        "\n",
        "  ```python\n",
        "  y_pred, _ = model(**inputs)\n",
        "  metric.update(y_pred, y_batch)\n",
        "  ```\n",
        "\n",
        "* **Perda composta com pesos**\n",
        "  O treino combina as duas perdas:\n",
        "\n",
        "  ```python\n",
        "  main_loss = criterion(y_pred, y_batch)\n",
        "  aux_loss  = criterion(y_pred_aux, y_batch)\n",
        "  loss = 0.8 * main_loss + 0.2 * aux_loss\n",
        "  ```\n",
        "\n",
        "  > Antes havia uma única perda; aqui há **supervisão auxiliar** ponderada.\n",
        "\n",
        "* **DataLoaders “named”**\n",
        "  Usamos `train_loader_named/valid_loader_named`, que **entregam** `(inputs_dict, y)` já com nomes compatíveis com `forward`.\n",
        "\n",
        "  > Antes os loaders entregavam tuplas (`X_wide, X_deep, y`).\n",
        "\n",
        "\n",
        "Usar **entradas nomeadas** (dict + `**inputs`) evita bugs de ordem dos tensores, deixa o código mais legível, e facilita escalar o modelo (adicionar/remover ramos como texto/imagem/tabular sem refatorar chamadas). Já a **saída auxiliar** com perda ponderada fornece sinal de gradiente mais curto para o ramo deep, atua como regularização, estabiliza o início do treino e permite monitorar melhor onde está o gargalo (comparando `main_loss` e `aux_loss`). Em suma: nomes trazem **manutenibilidade e segurança**; a cabeça auxiliar traz **estabilidade e generalização**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRjkXy2RMw3F"
      },
      "source": [
        "# 7. Building an Image Classifier with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwisMObnOAJe"
      },
      "source": [
        "Neste capítulo, continuaremos trabalhando com o **dataset Fashion MNIST**. A primeira etapa é fazer o download dos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPKxBYnOMw3F"
      },
      "source": [
        "## 7.1 Using TorchVision to Load the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8MvBcAnO0Q3"
      },
      "source": [
        "A **TorchVision** é uma biblioteca essencial no ecossistema PyTorch para **visão computacional**. Ela oferece:\n",
        "\n",
        "- Funções utilitárias para **baixar datasets comuns**, como MNIST e Fashion MNIST.  \n",
        "- **Modelos pré-treinados** para diversas tarefas de visão computacional (ver Capítulo 12).  \n",
        "- Funções para **transformar imagens**, incluindo corte, rotação, redimensionamento, entre outras.  \n",
        "\n",
        "> Observação: No Google Colab, a TorchVision já vem pré-instalada.\n",
        "\n",
        "No caso do Fashion MNIST:\n",
        "\n",
        "- O dataset já vem dividido em:\n",
        "  - **Treinamento:** 60.000 imagens  \n",
        "  - **Teste:** 10.000 imagens  \n",
        "- Para validação, podemos reservar **5.000 imagens** do conjunto de treinamento usando a função `random_split()` do PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "HrCdqjYPMw3F",
        "outputId": "8075d05d-cdf5-43eb-90d8-f0330b78797a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:20<00:00, 1.28MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 116kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:05<00:00, 821kB/s] \n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 19.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms.v2 as T\n",
        "\n",
        "toTensor = T.Compose([T.ToImage(), T.ToDtype(torch.float32, scale=True)])\n",
        "\n",
        "train_and_valid_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"datasets\", train=True, download=True, transform=toTensor)\n",
        "test_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"datasets\", train=False, download=True, transform=toTensor)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "train_data, valid_data = torch.utils.data.random_split(\n",
        "    train_and_valid_data, [55_000, 5_000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esse código baixa o **FashionMNIST** via TorchVision, **converte as imagens para tensor `float32` em [0,1]** e separa treino/validação de forma reprodutível. O `toTensor` usa `transforms.v2`: `T.ToImage()` garante o formato de imagem e `T.ToDtype(torch.float32, scale=True)` faz o cast para `float32` **escalando** de 0–255 para **0–1**. Em seguida, `FashionMNIST(..., train=True/False, download=True, transform=toTensor)` cria os datasets de **treino (60k)** e **teste (10k)**, salvando em `datasets/`. As imagens são **1×28×28** (tons de cinza) e o transform é aplicado **on the fly** quando você lê cada amostra. Por fim, com `torch.manual_seed(42)` e `random_split(train_and_valid_data, [55_000, 5_000])`, você reserva **5.000** para **validação** e mantém **55.000** para **treino**, com divisão **determinística** por causa da seed. (Se quiser padronizar ainda mais, depois pode adicionar `T.Normalize(mean, std)` ao `Compose`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "XbwvaWVcMw3F"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FnGjPgMw3F"
      },
      "source": [
        "Cada entrada é uma tupla (imagem, alvo):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "dxRQ3KcHMw3G"
      },
      "outputs": [],
      "source": [
        "X_sample, y_sample = train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KF41mm8Mw3G"
      },
      "source": [
        "Cada imagem tem um formato [canais, linhas, colunas]. Imagens em tons de cinza, como as do Fashion MNIST, possuem um único canal (enquanto imagens RGB possuem 3, e outros tipos de imagens, como imagens de satélite, podem ter muito mais). Imagens de moda são em tons de cinza e têm 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "zDv59Z5EMw3G",
        "outputId": "0821ae7e-bbe4-40ce-a67d-55fcd4b6f89e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_sample.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "KbSgYYXuMw3G",
        "outputId": "851c8f57-d60a-4982-a857-6d01fde14f9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_sample.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "kyxM4YY0Mw3G",
        "outputId": "e865f83b-29b8-4da6-e1f5-cacc84c18a42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ankle boot'"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_and_valid_data.classes[y_sample]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAPS6LrzMw3G"
      },
      "source": [
        "## 7.2 Building the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf1aLV1bPG-O"
      },
      "source": [
        "Vamos criar um **módulo personalizado** em PyTorch para uma **MLP (Multi-Layer Perceptron)** com **duas camadas ocultas**.  \n",
        "\n",
        "Este módulo servirá para classificar as imagens do Fashion MNIST, permitindo:\n",
        "\n",
        "- Definir facilmente o número de **neurônios em cada camada**.\n",
        "- Aplicar funções de ativação entre as camadas.\n",
        "- Facilitar o **treinamento e avaliação** do modelo usando o framework PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "sVdhxkiSMw3G"
      },
      "outputs": [],
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_classes):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),                   #Embedding\n",
        "            nn.Linear(n_inputs, n_hidden1),   \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden1, n_hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.mlp(X)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=300, n_hidden2=100,\n",
        "                        n_classes=10).to(device)\n",
        "xentropy = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`ImageClassifier` é uma MLP para imagens 1×28×28. A primeira camada é `nn.Flatten()`, que transforma cada imagem de shape `(N, C, H, W)` em `(N, C*H*W)` — aqui vira `(N, 784)`. Em seguida, os dados passam por `Linear→ReLU→Linear→ReLU→Linear`, mapeando `784→300→100→10`. O método `forward(self, X)` apenas define o **caminho de passagem**: ele aplica `self.mlp(X)` e retorna os **logits** (pontuações não normalizadas) de shape `(batch, 10)`. Já `xentropy = nn.CrossEntropyLoss()` é a **perda de classificação multiclasse** que **espera logits** e rótulos inteiros (`torch.long`, valores 0–9); ela combina **LogSoftmax + NLLLoss** internamente, então **não aplique Softmax antes**. No treino, você passa `(logits, targets)` para a perda; na inferência, use `logits.argmax(dim=1)` (ou `softmax(logits, dim=1)` para probabilidades).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">**Logits** são as **pontuações não normalizadas** que o modelo produz antes de virar probabilidade. Para classificação multiclasse, o vetor de logits (um por classe) é convertido em probabilidades pelo **softmax**; para binária (saída de 1 unidade), usa-se o **sigmoid**. Eles podem ser quaisquer números reais (negativos/positivos) e indicam “preferência relativa”: quanto maior o logit de uma classe, maior a probabilidade após a normalização. Em PyTorch, `nn.CrossEntropyLoss` **já espera logits** e aplica `log_softmax` + NLL internamente (por isso **não** aplique softmax antes); de forma análoga, `BCEWithLogitsLoss` espera logits e incorpora o sigmoid com melhorias de **estabilidade numérica**. Em inferência, você pega a classe com `logits.argmax(dim=1)` ou transforma em probabilidades com `softmax(logits, dim=1)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "hHZo7Pf5Mw3G",
        "outputId": "9d957b15-58c2-431e-d287-c354f0ac2669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, train loss: 0.6060, train metric: 0.7810, valid metric: 0.8426\n",
            "Epoch 2/20, train loss: 0.4059, train metric: 0.8493, valid metric: 0.8418\n",
            "Epoch 3/20, train loss: 0.3628, train metric: 0.8665, valid metric: 0.8540\n",
            "Epoch 4/20, train loss: 0.3350, train metric: 0.8769, valid metric: 0.8624\n",
            "Epoch 5/20, train loss: 0.3143, train metric: 0.8845, valid metric: 0.8770\n",
            "Epoch 6/20, train loss: 0.2984, train metric: 0.8872, valid metric: 0.8636\n",
            "Epoch 7/20, train loss: 0.2847, train metric: 0.8923, valid metric: 0.8734\n",
            "Epoch 8/20, train loss: 0.2737, train metric: 0.8970, valid metric: 0.8752\n",
            "Epoch 9/20, train loss: 0.2624, train metric: 0.9015, valid metric: 0.8834\n",
            "Epoch 10/20, train loss: 0.2525, train metric: 0.9043, valid metric: 0.8704\n",
            "Epoch 11/20, train loss: 0.2444, train metric: 0.9077, valid metric: 0.8834\n",
            "Epoch 12/20, train loss: 0.2360, train metric: 0.9099, valid metric: 0.8854\n",
            "Epoch 13/20, train loss: 0.2280, train metric: 0.9128, valid metric: 0.8882\n",
            "Epoch 14/20, train loss: 0.2211, train metric: 0.9159, valid metric: 0.8678\n",
            "Epoch 15/20, train loss: 0.2170, train metric: 0.9170, valid metric: 0.8802\n",
            "Epoch 16/20, train loss: 0.2076, train metric: 0.9205, valid metric: 0.8874\n",
            "Epoch 17/20, train loss: 0.2037, train metric: 0.9229, valid metric: 0.8916\n",
            "Epoch 18/20, train loss: 0.1991, train metric: 0.9236, valid metric: 0.8854\n",
            "Epoch 19/20, train loss: 0.1921, train metric: 0.9268, valid metric: 0.8828\n",
            "Epoch 20/20, train loss: 0.1869, train metric: 0.9292, valid metric: 0.8678\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
        "_ = train2(model, optimizer, xentropy, accuracy, train_loader, valid_loader,\n",
        "           n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esse trecho configura o treino e o executa. Primeiro, `optimizer = torch.optim.SGD(model.parameters(), lr=0.1)` escolhe o **otimizador SGD** com taxa de aprendizado 0.1 para atualizar os **parâmetros** do `model`. Depois, `accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)` cria a **métrica de acurácia** para classificação multiclasse com 10 classes e a move para o mesmo **device** (CPU/GPU) do modelo. Por fim, `train2(model, optimizer, xentropy, accuracy, train_loader, valid_loader, n_epochs)` roda o laço de treino por `n_epochs`, usando a **perda** `xentropy` (CrossEntropyLoss) e medindo **acurácia** em treino/validação; o retorno é atribuído a `_` para indicar que não será usado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "DE8fyBiGMw3G",
        "outputId": "2410dcfc-1240-4180-a050-1ab4dee71c93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 4, 2])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()\n",
        "X_new, y_new = next(iter(valid_loader))\n",
        "X_new = X_new[:3].to(device)\n",
        "with torch.no_grad():\n",
        "    y_pred_logits = model(X_new)\n",
        "y_pred = y_pred_logits.argmax(dim=1)  # index of the largest logit\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`model.eval()` coloca o modelo em **modo avaliação** (desativa dropout e usa estatísticas fixas de BatchNorm). `X_new, y_new = next(iter(valid_loader))` pega **um batch** do validador; `X_new = X_new[:3].to(device)` mantém só as **3 primeiras amostras** e move para CPU/GPU. Com `torch.no_grad()` desligamos gradientes (mais rápido, sem ocupar memória) e calculamos `y_pred_logits = model(X_new)`, que são os **logits** (pontuações não normalizadas). Por fim, `y_pred = y_pred_logits.argmax(dim=1)` escolhe, para cada amostra, o **índice da classe com maior logit** (não precisa `softmax` para isso), produzindo os rótulos previstos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "WAQutlS0Mw3G",
        "outputId": "13a578eb-8201-482a-9aba-dcdc1272e448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Sneaker', 'Coat', 'Pullover']"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[train_and_valid_data.classes[index] for index in y_pred]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwnq_-0rMw3H"
      },
      "source": [
        "Vamos verificar se o modelo fez as previsões corretas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "OJxeSvg_Mw3H",
        "outputId": "9ec85908-b0c8-44cd-f14b-50ed154fd84b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 4, 2])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_new[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg7X2javMw3H"
      },
      "source": [
        "Perfeito! 😃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "fIpem5cSMw3H",
        "outputId": "fdd284d6-781f-4604-8290-c712bdbdd775"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0010, 0.0000, 0.9150, 0.0000,\n",
              "         0.0840],\n",
              "        [0.0000, 0.0000, 0.0030, 0.0000, 0.9970, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0010, 0.0000, 0.7710, 0.0000, 0.0720, 0.0000, 0.1550, 0.0000, 0.0000,\n",
              "         0.0000]])"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "y_proba = F.softmax(y_pred_logits, dim=1)\n",
        "if device == \"mps\":\n",
        "    y_proba = y_proba.cpu()\n",
        "y_proba.round(decimals=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Aplica **softmax** para transformar os **logits** em **probabilidades** por classe (cada linha soma 1), movendo para CPU se o device for **MPS** (GPU Apple) e arredondando para 3 casas:\n",
        "* `F.softmax(y_pred_logits, dim=1)`: converte logits → probabilidades ao longo do eixo das classes (`dim=1`), mantendo shape `(batch, n_classes)`.\n",
        "* `if device == \"mps\": y_proba = y_proba.cpu()`: traz o tensor para CPU (útil para exibir/usar com libs que não lidam com MPS).\n",
        "* `y_proba.round(decimals=3)`: arredonda só para visualização (não use arredondado para cálculos).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "IB2i52ayMw3H",
        "outputId": "e7f95100-8cc9-46b4-c636-71a11ccc61e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9150, 0.0840, 0.0010, 0.0000],\n",
              "        [0.9970, 0.0030, 0.0000, 0.0000],\n",
              "        [0.7720, 0.1550, 0.0720, 0.0010]])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_top4_values, y_top4_indices = torch.topk(y_pred_logits, k=4, dim=1)\n",
        "y_top4_probas = F.softmax(y_top4_values, dim=1)\n",
        "if device == \"mps\":\n",
        "    y_top4_probas = y_top4_probas.cpu()\n",
        "y_top4_probas.round(decimals=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Você está pegando o **Top-4 por amostra** e exibindo as probabilidades dessas 4 classes:\n",
        "\n",
        "* `torch.topk(y_pred_logits, k=4, dim=1)` retorna **dois tensores**:\n",
        "\n",
        "  * `y_top4_values`: os **4 maiores logits** por linha (amostra)\n",
        "  * `y_top4_indices`: os **índices das classes** correspondentes\n",
        "* `F.softmax(y_top4_values, dim=1)` aplica **softmax só nesses 4 logits**, gerando probabilidades **renormalizadas entre as Top-4** (a massa das demais classes é descartada).\n",
        "\n",
        "  > Se você fizesse softmax nos **logits completos** e depois selecionasse as Top-4, as probabilidades poderiam ser levemente diferentes; aqui elas são reescaladas para somar 1 **apenas** entre as quatro escolhidas.\n",
        "* `if device == \"mps\": y_top4_probas = y_top4_probas.cpu()` move para CPU (útil em Macs com GPU Apple).\n",
        "* `.round(3)` apenas **arredonda para mostrar** (não use arredondado para cálculos).\n",
        "\n",
        "Use `y_top4_indices` para mapear para os rótulos de classe (ex.: nomes do Fashion-MNIST).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "0dTABrlnMw3H",
        "outputId": "52e0baa3-b161-4fc3-95a0-ada1f780f798"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[7, 9, 5, 8],\n",
              "        [4, 2, 6, 0],\n",
              "        [2, 6, 4, 0]])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_top4_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "h54XnU_gMw3H",
        "outputId": "9acea41b-a0fd-44b4-b00f-b9059c8f7b27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "266610"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum([param.numel() for param in model.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Isso retorna o **número total de parâmetros** do modelo.\n",
        "* `model.parameters()` — iterador sobre **todos os `nn.Parameter`** do modelo (treináveis por padrão).\n",
        "* `param.numel()` — quantidade de **elementos** naquele tensor de pesos/viés.\n",
        "* `sum([...])` — soma tudo → **total de parâmetros** (treináveis + não-treináveis; se quiser só treináveis, filtre)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bFGLJ67Mw3H"
      },
      "source": [
        "# 8. Fine-Tuning Neural Network Hyperparameter with Optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGn-wYFDZ6qq"
      },
      "source": [
        "Além de definir manualmente os **hiperparâmetros** (como visto no Capítulo 9), é possível realizar uma **busca automática** para encontrar valores mais adequados.  \n",
        "\n",
        "### Abordagens possíveis:\n",
        "1. **Converter o modelo PyTorch em um estimador Scikit-Learn**:\n",
        "   - Criando uma classe customizada ou usando uma **wrapper library** como [Skorch](https://skorch.readthedocs.io).  \n",
        "   - Permite usar métodos como **GridSearchCV** ou **RandomizedSearchCV** para ajustar hiperparâmetros.\n",
        "\n",
        "2. **Usar bibliotecas dedicadas de fine-tuning** (geralmente mais eficientes):\n",
        "   - [Optuna](https://optuna.org)  \n",
        "   - [Ray Tune](https://docs.ray.io)  \n",
        "   - [Hyperopt](https://hyperopt.github.io/hyperopt)  \n",
        "\n",
        "Essas bibliotecas oferecem **estratégias poderosas de busca** e são altamente customizáveis.\n",
        "\n",
        "### Exemplo com Optuna:\n",
        "1. **Instalação** (no Colab):\n",
        "   ```python\n",
        "   %pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from optuna) (2.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from optuna) (25.0)\n",
            "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
            "  Downloading sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
            "  Downloading greenlet-3.2.4-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\estev\\onedrive\\área de trabalho\\ncia\\.ncia\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "Downloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
            "Downloading sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl (2.1 MB)\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
            "   --------- ------------------------------ 0.5/2.1 MB 216.5 kB/s eta 0:00:08\n",
            "   --------- ------------------------------ 0.5/2.1 MB 216.5 kB/s eta 0:00:08\n",
            "   --------- ------------------------------ 0.5/2.1 MB 216.5 kB/s eta 0:00:08\n",
            "   --------- ------------------------------ 0.5/2.1 MB 216.5 kB/s eta 0:00:08\n",
            "   --------- ------------------------------ 0.5/2.1 MB 216.5 kB/s eta 0:00:08\n",
            "   --------- ------------------------------ 0.5/2.1 MB 216.5 kB/s eta 0:00:08\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   -------------- ------------------------- 0.8/2.1 MB 214.1 kB/s eta 0:00:07\n",
            "   ------------------- -------------------- 1.0/2.1 MB 160.0 kB/s eta 0:00:07\n",
            "   ------------------- -------------------- 1.0/2.1 MB 160.0 kB/s eta 0:00:07\n",
            "   ------------------- -------------------- 1.0/2.1 MB 160.0 kB/s eta 0:00:07\n",
            "   ------------------- -------------------- 1.0/2.1 MB 160.0 kB/s eta 0:00:07\n",
            "   ------------------- -------------------- 1.0/2.1 MB 160.0 kB/s eta 0:00:07\n",
            "   ------------------------ --------------- 1.3/2.1 MB 178.4 kB/s eta 0:00:05\n",
            "   ------------------------ --------------- 1.3/2.1 MB 178.4 kB/s eta 0:00:05\n",
            "   ------------------------ --------------- 1.3/2.1 MB 178.4 kB/s eta 0:00:05\n",
            "   ----------------------------- ---------- 1.6/2.1 MB 200.0 kB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 1.6/2.1 MB 200.0 kB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 1.6/2.1 MB 200.0 kB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 1.6/2.1 MB 200.0 kB/s eta 0:00:03\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 1.8/2.1 MB 213.1 kB/s eta 0:00:02\n",
            "   ---------------------------------------  2.1/2.1 MB 182.1 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.1/2.1 MB 181.4 kB/s  0:00:15\n",
            "Downloading greenlet-3.2.4-cp313-cp313-win_amd64.whl (299 kB)\n",
            "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
            "\n",
            "   ---------------------------------------- 0/6 [Mako]\n",
            "   ---------------------------------------- 0/6 [Mako]\n",
            "   ---------------------------------------- 0/6 [Mako]\n",
            "   ---------------------------------------- 0/6 [Mako]\n",
            "   ---------------------------------------- 0/6 [Mako]\n",
            "   ---------------------------------------- 0/6 [Mako]\n",
            "   ------ --------------------------------- 1/6 [greenlet]\n",
            "   ------ --------------------------------- 1/6 [greenlet]\n",
            "   ------ --------------------------------- 1/6 [greenlet]\n",
            "   ------ --------------------------------- 1/6 [greenlet]\n",
            "   ------------- -------------------------- 2/6 [colorlog]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   -------------------------- ------------- 4/6 [alembic]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   --------------------------------- ------ 5/6 [optuna]\n",
            "   ---------------------------------------- 6/6 [optuna]\n",
            "\n",
            "Successfully installed Mako-1.3.10 alembic-1.17.2 colorlog-6.10.1 greenlet-3.2.4 optuna-4.6.0 sqlalchemy-2.0.44\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "0-MbeSeYMw3H"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
        "    n_hidden = trial.suggest_int(\"n_hidden\", 20, 300)\n",
        "\n",
        "\n",
        "    model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=n_hidden,\n",
        "                            n_hidden2=n_hidden, n_classes=10).to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    xentropy = nn.CrossEntropyLoss()\n",
        "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
        "    accuracy = accuracy.to(device)\n",
        "    \n",
        "    history = train2(model, optimizer, xentropy, accuracy, train_loader,\n",
        "                     valid_loader, n_epochs=10)\n",
        "    validation_accuracy = max(history[\"valid_metrics\"])\n",
        "    \n",
        "    return validation_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eACfMNX6akb6"
      },
      "source": [
        "\n",
        "### 1. Métodos para sugerir hiperparâmetros:\n",
        "- **`suggest_float()`**: pede um valor float dentro de um intervalo.  \n",
        "- **`suggest_int()`**: pede um valor inteiro dentro de um intervalo.  \n",
        "- **`suggest_categorical()`**: pede um valor dentre opções categóricas.  \n",
        "\n",
        "Exemplo para **learning_rate**:  \n",
        "- Intervalo: `10^-5` a `10^-1`  \n",
        "- Uso de `log=True` para explorar múltiplas escalas (log distribution).  \n",
        "  - Sem isso, a amostragem uniforme poderia **ignorar valores muito pequenos**, prejudicando a busca.\n",
        "\n",
        "### 2. Criando e rodando a busca:\n",
        "1. Criar um **Study** com `optuna.create_study()`.  \n",
        "   - Definir `direction=\"maximize\"` se a função objetivo retorna uma métrica que deve ser maximizada (por padrão, Optuna minimiza).  \n",
        "2. Rodar a otimização com `study.optimize()`, passando:\n",
        "   - A **função objetivo** definida anteriormente.  \n",
        "   - O **número de trials** (quantas vezes Optuna chama a função objetivo).\n",
        "\n",
        "### 3. Garantindo reprodutibilidade:\n",
        "- Definir **random seed do PyTorch**.  \n",
        "- Definir **random seed do sampler do Optuna**.  \n",
        "\n",
        "Essa abordagem permite que Optuna explore de forma eficiente diferentes combinações de hiperparâmetros e encontre as melhores configurações para o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "riEZrGzZMw3I",
        "outputId": "5112411c-704e-40a9-a8de-d1dd5d1f65b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:04:34,960] A new study created in memory with name: no-name-2b058bee-648c-48a4-8c36-78a0a0289afa\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, train loss: 2.2769, train metric: 0.1471, valid metric: 0.1860\n",
            "Epoch 2/10, train loss: 2.2093, train metric: 0.2794, valid metric: 0.3500\n",
            "Epoch 3/10, train loss: 2.1164, train metric: 0.4109, valid metric: 0.4554\n",
            "Epoch 4/10, train loss: 1.9776, train metric: 0.5137, valid metric: 0.5560\n",
            "Epoch 5/10, train loss: 1.7867, train metric: 0.5826, valid metric: 0.6026\n",
            "Epoch 6/10, train loss: 1.5775, train metric: 0.6184, valid metric: 0.6228\n",
            "Epoch 7/10, train loss: 1.3978, train metric: 0.6288, valid metric: 0.6326\n",
            "Epoch 8/10, train loss: 1.2605, train metric: 0.6360, valid metric: 0.6372\n",
            "Epoch 9/10, train loss: 1.1572, train metric: 0.6467, valid metric: 0.6424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:06:30,628] Trial 0 finished with value: 0.6435999870300293 and parameters: {'learning_rate': 0.00031489116479568613, 'n_hidden': 287}. Best is trial 0 with value: 0.6435999870300293.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, train loss: 1.0782, train metric: 0.6537, valid metric: 0.6436\n",
            "Epoch 1/10, train loss: 1.1459, train metric: 0.6229, valid metric: 0.7338\n",
            "Epoch 2/10, train loss: 0.6108, train metric: 0.7841, valid metric: 0.8000\n",
            "Epoch 3/10, train loss: 0.5203, train metric: 0.8170, valid metric: 0.8094\n",
            "Epoch 4/10, train loss: 0.4810, train metric: 0.8302, valid metric: 0.8310\n",
            "Epoch 5/10, train loss: 0.4558, train metric: 0.8403, valid metric: 0.8352\n",
            "Epoch 6/10, train loss: 0.4388, train metric: 0.8460, valid metric: 0.8442\n",
            "Epoch 7/10, train loss: 0.4240, train metric: 0.8512, valid metric: 0.8412\n",
            "Epoch 8/10, train loss: 0.4123, train metric: 0.8565, valid metric: 0.8510\n",
            "Epoch 9/10, train loss: 0.3999, train metric: 0.8599, valid metric: 0.8534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:09:39,046] Trial 1 finished with value: 0.8539999723434448 and parameters: {'learning_rate': 0.008471801418819975, 'n_hidden': 188}. Best is trial 1 with value: 0.8539999723434448.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, train loss: 0.3897, train metric: 0.8637, valid metric: 0.8540\n",
            "Epoch 1/10, train loss: 2.3069, train metric: 0.1144, valid metric: 0.1082\n",
            "Epoch 2/10, train loss: 2.2993, train metric: 0.1231, valid metric: 0.1294\n",
            "Epoch 3/10, train loss: 2.2914, train metric: 0.1606, valid metric: 0.1710\n",
            "Epoch 4/10, train loss: 2.2836, train metric: 0.1839, valid metric: 0.1840\n",
            "Epoch 5/10, train loss: 2.2762, train metric: 0.1891, valid metric: 0.1856\n",
            "Epoch 6/10, train loss: 2.2692, train metric: 0.1910, valid metric: 0.1898\n",
            "Epoch 7/10, train loss: 2.2623, train metric: 0.1933, valid metric: 0.1932\n",
            "Epoch 8/10, train loss: 2.2554, train metric: 0.2000, valid metric: 0.2022\n",
            "Epoch 9/10, train loss: 2.2485, train metric: 0.2122, valid metric: 0.2160\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:15:49,387] Trial 2 finished with value: 0.23340000212192535 and parameters: {'learning_rate': 4.207988669606632e-05, 'n_hidden': 63}. Best is trial 1 with value: 0.8539999723434448.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, train loss: 2.2414, train metric: 0.2299, valid metric: 0.2334\n",
            "Epoch 1/10, train loss: 2.3035, train metric: 0.1373, valid metric: 0.1526\n",
            "Epoch 2/10, train loss: 2.3005, train metric: 0.1569, valid metric: 0.1724\n",
            "Epoch 3/10, train loss: 2.2975, train metric: 0.1755, valid metric: 0.1896\n",
            "Epoch 4/10, train loss: 2.2945, train metric: 0.1941, valid metric: 0.2132\n",
            "Epoch 5/10, train loss: 2.2914, train metric: 0.2105, valid metric: 0.2288\n",
            "Epoch 6/10, train loss: 2.2884, train metric: 0.2261, valid metric: 0.2418\n",
            "Epoch 7/10, train loss: 2.2853, train metric: 0.2419, valid metric: 0.2580\n",
            "Epoch 8/10, train loss: 2.2823, train metric: 0.2581, valid metric: 0.2742\n",
            "Epoch 9/10, train loss: 2.2792, train metric: 0.2736, valid metric: 0.2918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:20:02,307] Trial 3 finished with value: 0.30959999561309814 and parameters: {'learning_rate': 1.7073967431528103e-05, 'n_hidden': 263}. Best is trial 1 with value: 0.8539999723434448.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, train loss: 2.2761, train metric: 0.2897, valid metric: 0.3096\n",
            "Epoch 1/10, train loss: 1.8379, train metric: 0.4869, valid metric: 0.6208\n",
            "Epoch 2/10, train loss: 0.9751, train metric: 0.6666, valid metric: 0.6978\n",
            "Epoch 3/10, train loss: 0.7608, train metric: 0.7253, valid metric: 0.7416\n",
            "Epoch 4/10, train loss: 0.6704, train metric: 0.7639, valid metric: 0.7720\n",
            "Epoch 5/10, train loss: 0.6108, train metric: 0.7913, valid metric: 0.7904\n",
            "Epoch 6/10, train loss: 0.5686, train metric: 0.8053, valid metric: 0.8052\n",
            "Epoch 7/10, train loss: 0.5385, train metric: 0.8164, valid metric: 0.8082\n",
            "Epoch 8/10, train loss: 0.5158, train metric: 0.8243, valid metric: 0.8216\n",
            "Epoch 9/10, train loss: 0.4988, train metric: 0.8280, valid metric: 0.8220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:24:06,692] Trial 4 finished with value: 0.8220000267028809 and parameters: {'learning_rate': 0.002537815508265664, 'n_hidden': 218}. Best is trial 1 with value: 0.8539999723434448.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10, train loss: 0.4842, train metric: 0.8330, valid metric: 0.8092\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "study.optimize(objective, n_trials=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">Aumentar a acurácia ou reduzir a Loss?\n",
        "* Se o **objetivo do produto** é “acertar mais”, prefira **maximizar a métrica de validação** que representa esse objetivo (**acurácia**, **F1**, **AUC**, **QWK** em tarefas ordinais etc.). Você continuará treinando com **loss** (p/ gradiente), mas o **critério de escolha** no Optuna é a **métrica** que importa no fim.\n",
        "\n",
        "* **Quando escolher minimizar loss?**\n",
        "  Quando a métrica é **instável** em poucos dados (acurácia pulando muito), quando há **muito desbalanceamento** e você ainda não definiu limiar/ métrica final, ou em **regressão** (use **RMSE/MAE** como loss e como objetivo). A loss é **suave e estável**, costuma correlacionar com generalização.\n",
        "\n",
        "* **Quando escolher maximizar acurácia (ou outra métrica)?**\n",
        "  Quando a métrica já está **bem definida e alinhada ao negócio** (ex.: acurácia top-1 em classificação balanceada; **F1** em positivos raros; **AUC-ROC/PR** se você compara curvas; **QWK** em DR ordinal). A seleção por métrica garante que o melhor modelo é o que **de fato performa melhor** do jeito que você será avaliado.\n",
        "\n",
        "* **Regra prática:**\n",
        "  Treine **sempre** com uma loss apropriada (p.ex., CrossEntropy com pesos/focal em classe rara). No Optuna, **otimize a métrica de validação** que você reportará. Exceções: dados muito pequenos/ruidosos → minimize loss; regressão → minimize **RMSE/MAE**; ordinais (DR 0–4) → maximize **QWK**; desbalanceado severo → maximize **F1** ou **AUCPR**.\n",
        "\n",
        "* **Dica final:** mesmo otimizando por métrica, monitore a **loss** (early stopping) e use a métrica como **critério de melhor checkpoint**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_8v0KaNa1Sx"
      },
      "source": [
        "### 1. Algoritmo padrão: TPE\n",
        "- Optuna utiliza por padrão o **Tree-structured Parzen Estimator (TPE)** para otimização de hiperparâmetros.  \n",
        "- TPE é um **algoritmo de otimização sequencial baseado em modelo**:\n",
        "  - Aprende com os **resultados anteriores** para selecionar melhores hiperparâmetros.  \n",
        "  - Inicialmente começa com valores **aleatórios**.  \n",
        "  - Progressivamente foca a busca nas **regiões mais promissoras** do espaço de hiperparâmetros.\n",
        "\n",
        "### 2. Vantagens do TPE:\n",
        "- Permite encontrar **hiperparâmetros melhores do que a busca aleatória (random search)** no mesmo tempo.  \n",
        "- Explora eficientemente o espaço de busca, evitando gastar muitas tentativas em regiões pouco promissoras.\n",
        "\n",
        "### 3. Resultados da otimização:\n",
        "- Após a execução do estudo (`study.optimize()`), é possível acessar:\n",
        "  - **Melhores hiperparâmetros encontrados**.  \n",
        "  - **Acurácia de validação** ou outra métrica associada a esses hiperparâmetros.  \n",
        "\n",
        "Essa abordagem torna a busca de hiperparâmetros mais inteligente e eficiente, aproveitando os resultados de testes anteriores para melhorar continuamente a escolha dos próximos parâmetros a serem testados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "51FHyN5WMw3I",
        "outputId": "3ccb5a59-a147-4a71-f937-497f324fe00b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': 0.008471801418819975, 'n_hidden': 188}"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "O0Su9hYEMw3I",
        "outputId": "a68106b2-cbd5-44dc-f9d6-bad980c011d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8539999723434448"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWFWGvMibIsK"
      },
      "source": [
        "### 1. Número de trials\n",
        "- A performance do modelo melhora à medida que aumentamos o número de trials (`n_trials`).  \n",
        "- Exemplo: aumentar `n_trials` para 50 ou mais geralmente gera resultados melhores, mas leva mais tempo para executar.  \n",
        "- Alternativa: rodar `optimize()` repetidamente e interromper quando o desempenho for satisfatório.\n",
        "\n",
        "### 2. Paralelização de trials\n",
        "- Optuna pode executar trials **em paralelo em múltiplas máquinas**, oferecendo um **aumento quase linear na velocidade**.  \n",
        "- Passos para paralelização:\n",
        "  1. Configurar um **banco de dados SQL** (ex.: SQLite ou PostgreSQL).  \n",
        "  2. Definir o parâmetro `storage` na função `create_study()` apontando para esse banco de dados.  \n",
        "  3. Definir `study_name` e `load_if_exists=True`.  \n",
        "  4. Copiar o script de tuning para cada máquina e executar.  \n",
        "     - Se usar seeds aleatórias, garanta que sejam **diferentes em cada máquina**.  \n",
        "  5. Todos os scripts trabalham em paralelo, lendo e escrevendo resultados no banco de dados.  \n",
        "- Benefício adicional: mantém **um log completo de todos os resultados dos experimentos**.\n",
        "\n",
        "### 3. Boas práticas com a função objetivo\n",
        "- Em exemplos anteriores, a função `objective()` tinha acesso direto ao conjunto de treino e validação via variáveis globais.  \n",
        "- Forma mais limpa: **passar os datasets como argumentos adicionais** à função objetivo, garantindo melhor organização e modularidade do código:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "c2YG3P9fMw3I"
      },
      "outputs": [],
      "source": [
        "def objective(trial, train_loader, valid_loader):\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
        "    n_hidden = trial.suggest_int(\"n_hidden\", 20, 300)\n",
        "\n",
        "\n",
        "    model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=n_hidden,\n",
        "                            n_hidden2=n_hidden, n_classes=10).to(device)\n",
        "    \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    xentropy = nn.CrossEntropyLoss()\n",
        "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
        "\n",
        "    accuracy = accuracy.to(device)\n",
        "\n",
        "    best_validation_accuracy = 0.0\n",
        "    for epoch in range(n_epochs):\n",
        "        history = train2(model, optimizer, xentropy, accuracy, train_loader,\n",
        "                         valid_loader, n_epochs=1)\n",
        "        \n",
        "        validation_accuracy = max(history[\"valid_metrics\"])\n",
        "        if validation_accuracy > best_validation_accuracy:\n",
        "            best_validation_accuracy = validation_accuracy\n",
        "            \n",
        "        trial.report(validation_accuracy, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "    return best_validation_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**O que essa `objective` faz (bem direto):** em cada *trial* do Optuna, ela amostra **`learning_rate`** (log-uniforme) e **`n_hidden`**, constrói o `ImageClassifier` (duas camadas ocultas iguais), cria **SGD** e **CrossEntropy**, define a **acurácia** do `torchmetrics` e treina por `n_epochs` **dentro** da própria `objective`. A cada época, chama `train2(..., n_epochs=1)`, lê a **acurácia de validação** (`validation_accuracy`), guarda o **melhor valor** em `best_validation_accuracy`, e informa progresso ao Optuna via `trial.report(validation_accuracy, epoch)`. Se o *pruner* decidir podar (`trial.should_prune()`), a *trial* é interrompida com `TrialPruned()`. No fim, retorna a **melhor acurácia de validação** do trial — por isso o seu `study` deve usar `direction=\"maximize\"`.\n",
        "\n",
        "**Por que funciona:** o Optuna compara o valor retornado (maior = melhor) entre *trials* com diferentes hiperparâmetros e converge para combinações promissoras, podando cedo as ruins.\n",
        "\n",
        "**Duas notas úteis:**\n",
        "\n",
        "* Garanta que `train2` **reseta** corretamente a métrica por época (ex.: `metric.reset()`), senão a acurácia pode acumular indevidamente.\n",
        "* Para reprodutibilidade, fixe **seeds** (PyTorch e Optuna) e considere um *pruner* (ex.: `MedianPruner`) para acelerar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "EmCnMz93Mw3I"
      },
      "outputs": [],
      "source": [
        "objective_with_data = lambda trial: objective(\n",
        "    trial, train_loader=train_loader, valid_loader=valid_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GuFzywKbXtM"
      },
      "source": [
        "Para definir os argumentos extras (os carregadores de conjuntos de dados, neste caso), basta criar uma função lambda quando necessário e passá-la para o método optimize(). Como alternativa, você pode usar a função functools.partial(), que cria uma função wrapper fina em torno do callable fornecido para fornecer valores padrão para qualquer número de argumentos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "yl84rxyvMw3I"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "objective_with_data = partial(objective, train_loader=train_loader,\n",
        "                              valid_loader=valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "nwA5xEVoMw3I",
        "outputId": "71319d15-ce98-4edb-9d54-b056b8e29bd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:24:06,794] A new study created in memory with name: no-name-0d50ad86-8859-4f57-af8b-ddd6286c2a29\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.2769, train metric: 0.1471, valid metric: 0.1860\n",
            "Epoch 1/1, train loss: 2.2093, train metric: 0.2794, valid metric: 0.3500\n",
            "Epoch 1/1, train loss: 2.1164, train metric: 0.4109, valid metric: 0.4554\n",
            "Epoch 1/1, train loss: 1.9776, train metric: 0.5137, valid metric: 0.5560\n",
            "Epoch 1/1, train loss: 1.7867, train metric: 0.5826, valid metric: 0.6026\n",
            "Epoch 1/1, train loss: 1.5775, train metric: 0.6184, valid metric: 0.6228\n",
            "Epoch 1/1, train loss: 1.3978, train metric: 0.6288, valid metric: 0.6326\n",
            "Epoch 1/1, train loss: 1.2605, train metric: 0.6360, valid metric: 0.6372\n",
            "Epoch 1/1, train loss: 1.1572, train metric: 0.6467, valid metric: 0.6424\n",
            "Epoch 1/1, train loss: 1.0782, train metric: 0.6537, valid metric: 0.6436\n",
            "Epoch 1/1, train loss: 1.0162, train metric: 0.6611, valid metric: 0.6530\n",
            "Epoch 1/1, train loss: 0.9665, train metric: 0.6689, valid metric: 0.6622\n",
            "Epoch 1/1, train loss: 0.9258, train metric: 0.6761, valid metric: 0.6700\n",
            "Epoch 1/1, train loss: 0.8919, train metric: 0.6835, valid metric: 0.6782\n",
            "Epoch 1/1, train loss: 0.8629, train metric: 0.6897, valid metric: 0.6848\n",
            "Epoch 1/1, train loss: 0.8381, train metric: 0.6954, valid metric: 0.6876\n",
            "Epoch 1/1, train loss: 0.8166, train metric: 0.7009, valid metric: 0.6932\n",
            "Epoch 1/1, train loss: 0.7974, train metric: 0.7073, valid metric: 0.6964\n",
            "Epoch 1/1, train loss: 0.7802, train metric: 0.7119, valid metric: 0.7090\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:35:58,953] Trial 0 finished with value: 0.7089999914169312 and parameters: {'learning_rate': 0.00031489116479568613, 'n_hidden': 287}. Best is trial 0 with value: 0.7089999914169312.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.7647, train metric: 0.7196, valid metric: 0.7082\n",
            "Epoch 1/1, train loss: 1.1485, train metric: 0.6157, valid metric: 0.7330\n",
            "Epoch 1/1, train loss: 0.6133, train metric: 0.7863, valid metric: 0.8082\n",
            "Epoch 1/1, train loss: 0.5200, train metric: 0.8179, valid metric: 0.8132\n",
            "Epoch 1/1, train loss: 0.4783, train metric: 0.8312, valid metric: 0.8236\n",
            "Epoch 1/1, train loss: 0.4533, train metric: 0.8401, valid metric: 0.8026\n",
            "Epoch 1/1, train loss: 0.4356, train metric: 0.8465, valid metric: 0.8444\n",
            "Epoch 1/1, train loss: 0.4209, train metric: 0.8511, valid metric: 0.8274\n",
            "Epoch 1/1, train loss: 0.4082, train metric: 0.8563, valid metric: 0.8398\n",
            "Epoch 1/1, train loss: 0.3981, train metric: 0.8603, valid metric: 0.8526\n",
            "Epoch 1/1, train loss: 0.3881, train metric: 0.8640, valid metric: 0.8584\n",
            "Epoch 1/1, train loss: 0.3782, train metric: 0.8663, valid metric: 0.8544\n",
            "Epoch 1/1, train loss: 0.3697, train metric: 0.8694, valid metric: 0.8570\n",
            "Epoch 1/1, train loss: 0.3630, train metric: 0.8710, valid metric: 0.8574\n",
            "Epoch 1/1, train loss: 0.3553, train metric: 0.8745, valid metric: 0.8498\n",
            "Epoch 1/1, train loss: 0.3477, train metric: 0.8766, valid metric: 0.8674\n",
            "Epoch 1/1, train loss: 0.3420, train metric: 0.8785, valid metric: 0.8514\n",
            "Epoch 1/1, train loss: 0.3364, train metric: 0.8805, valid metric: 0.8682\n",
            "Epoch 1/1, train loss: 0.3296, train metric: 0.8819, valid metric: 0.8672\n",
            "Epoch 1/1, train loss: 0.3245, train metric: 0.8842, valid metric: 0.8670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:45:16,800] Trial 1 finished with value: 0.8682000041007996 and parameters: {'learning_rate': 0.008471801418819975, 'n_hidden': 188}. Best is trial 1 with value: 0.8682000041007996.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.3192, train metric: 0.8864, valid metric: 0.8632\n",
            "Epoch 1/1, train loss: 2.2998, train metric: 0.1078, valid metric: 0.1152\n",
            "Epoch 1/1, train loss: 2.2923, train metric: 0.1305, valid metric: 0.1432\n",
            "Epoch 1/1, train loss: 2.2856, train metric: 0.1605, valid metric: 0.1704\n",
            "Epoch 1/1, train loss: 2.2797, train metric: 0.1872, valid metric: 0.1912\n",
            "Epoch 1/1, train loss: 2.2744, train metric: 0.2091, valid metric: 0.2114\n",
            "Epoch 1/1, train loss: 2.2693, train metric: 0.2230, valid metric: 0.2216\n",
            "Epoch 1/1, train loss: 2.2643, train metric: 0.2332, valid metric: 0.2320\n",
            "Epoch 1/1, train loss: 2.2591, train metric: 0.2408, valid metric: 0.2380\n",
            "Epoch 1/1, train loss: 2.2538, train metric: 0.2457, valid metric: 0.2424\n",
            "Epoch 1/1, train loss: 2.2481, train metric: 0.2493, valid metric: 0.2456\n",
            "Epoch 1/1, train loss: 2.2422, train metric: 0.2511, valid metric: 0.2464\n",
            "Epoch 1/1, train loss: 2.2360, train metric: 0.2534, valid metric: 0.2476\n",
            "Epoch 1/1, train loss: 2.2296, train metric: 0.2538, valid metric: 0.2478\n",
            "Epoch 1/1, train loss: 2.2229, train metric: 0.2535, valid metric: 0.2482\n",
            "Epoch 1/1, train loss: 2.2160, train metric: 0.2544, valid metric: 0.2492\n",
            "Epoch 1/1, train loss: 2.2086, train metric: 0.2549, valid metric: 0.2508\n",
            "Epoch 1/1, train loss: 2.2009, train metric: 0.2561, valid metric: 0.2522\n",
            "Epoch 1/1, train loss: 2.1928, train metric: 0.2572, valid metric: 0.2520\n",
            "Epoch 1/1, train loss: 2.1842, train metric: 0.2583, valid metric: 0.2526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:52:00,862] Trial 2 finished with value: 0.25380000472068787 and parameters: {'learning_rate': 4.207988669606632e-05, 'n_hidden': 63}. Best is trial 1 with value: 0.8682000041007996.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.1751, train metric: 0.2610, valid metric: 0.2538\n",
            "Epoch 1/1, train loss: 2.3015, train metric: 0.0997, valid metric: 0.1028\n",
            "Epoch 1/1, train loss: 2.2984, train metric: 0.1009, valid metric: 0.1050\n",
            "Epoch 1/1, train loss: 2.2953, train metric: 0.1051, valid metric: 0.1124\n",
            "Epoch 1/1, train loss: 2.2923, train metric: 0.1160, valid metric: 0.1270\n",
            "Epoch 1/1, train loss: 2.2894, train metric: 0.1347, valid metric: 0.1496\n",
            "Epoch 1/1, train loss: 2.2865, train metric: 0.1548, valid metric: 0.1652\n",
            "Epoch 1/1, train loss: 2.2837, train metric: 0.1699, valid metric: 0.1800\n",
            "Epoch 1/1, train loss: 2.2808, train metric: 0.1800, valid metric: 0.1872\n",
            "Epoch 1/1, train loss: 2.2780, train metric: 0.1855, valid metric: 0.1916\n",
            "Epoch 1/1, train loss: 2.2752, train metric: 0.1893, valid metric: 0.1960\n",
            "Epoch 1/1, train loss: 2.2725, train metric: 0.1950, valid metric: 0.2008\n",
            "Epoch 1/1, train loss: 2.2697, train metric: 0.2000, valid metric: 0.2026\n",
            "Epoch 1/1, train loss: 2.2669, train metric: 0.2058, valid metric: 0.2046\n",
            "Epoch 1/1, train loss: 2.2641, train metric: 0.2124, valid metric: 0.2116\n",
            "Epoch 1/1, train loss: 2.2613, train metric: 0.2177, valid metric: 0.2166\n",
            "Epoch 1/1, train loss: 2.2585, train metric: 0.2238, valid metric: 0.2224\n",
            "Epoch 1/1, train loss: 2.2556, train metric: 0.2289, valid metric: 0.2264\n",
            "Epoch 1/1, train loss: 2.2528, train metric: 0.2339, valid metric: 0.2314\n",
            "Epoch 1/1, train loss: 2.2498, train metric: 0.2383, valid metric: 0.2342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 09:58:26,783] Trial 3 finished with value: 0.23960000276565552 and parameters: {'learning_rate': 1.7073967431528103e-05, 'n_hidden': 263}. Best is trial 1 with value: 0.8682000041007996.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.2469, train metric: 0.2429, valid metric: 0.2396\n",
            "Epoch 1/1, train loss: 1.8945, train metric: 0.4924, valid metric: 0.6290\n",
            "Epoch 1/1, train loss: 1.0016, train metric: 0.6583, valid metric: 0.6772\n",
            "Epoch 1/1, train loss: 0.7747, train metric: 0.7114, valid metric: 0.7248\n",
            "Epoch 1/1, train loss: 0.6864, train metric: 0.7551, valid metric: 0.7596\n",
            "Epoch 1/1, train loss: 0.6268, train metric: 0.7833, valid metric: 0.7806\n",
            "Epoch 1/1, train loss: 0.5830, train metric: 0.7999, valid metric: 0.7862\n",
            "Epoch 1/1, train loss: 0.5500, train metric: 0.8113, valid metric: 0.8064\n",
            "Epoch 1/1, train loss: 0.5253, train metric: 0.8184, valid metric: 0.8128\n",
            "Epoch 1/1, train loss: 0.5061, train metric: 0.8237, valid metric: 0.8202\n",
            "Epoch 1/1, train loss: 0.4908, train metric: 0.8291, valid metric: 0.8206\n",
            "Epoch 1/1, train loss: 0.4778, train metric: 0.8337, valid metric: 0.8152\n",
            "Epoch 1/1, train loss: 0.4681, train metric: 0.8376, valid metric: 0.8304\n",
            "Epoch 1/1, train loss: 0.4591, train metric: 0.8403, valid metric: 0.8318\n",
            "Epoch 1/1, train loss: 0.4516, train metric: 0.8429, valid metric: 0.8314\n",
            "Epoch 1/1, train loss: 0.4448, train metric: 0.8459, valid metric: 0.8346\n",
            "Epoch 1/1, train loss: 0.4380, train metric: 0.8474, valid metric: 0.8358\n",
            "Epoch 1/1, train loss: 0.4321, train metric: 0.8497, valid metric: 0.8378\n",
            "Epoch 1/1, train loss: 0.4272, train metric: 0.8512, valid metric: 0.8368\n",
            "Epoch 1/1, train loss: 0.4223, train metric: 0.8528, valid metric: 0.8408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:04:46,709] Trial 4 finished with value: 0.8428000211715698 and parameters: {'learning_rate': 0.002537815508265664, 'n_hidden': 218}. Best is trial 1 with value: 0.8682000041007996.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.4174, train metric: 0.8555, valid metric: 0.8428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:05:06,120] Trial 5 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.3017, train metric: 0.1007, valid metric: 0.1056\n",
            "Epoch 1/1, train loss: 0.8584, train metric: 0.7026, valid metric: 0.7982\n",
            "Epoch 1/1, train loss: 0.5069, train metric: 0.8211, valid metric: 0.8232\n",
            "Epoch 1/1, train loss: 0.4498, train metric: 0.8401, valid metric: 0.8354\n",
            "Epoch 1/1, train loss: 0.4184, train metric: 0.8515, valid metric: 0.8516\n",
            "Epoch 1/1, train loss: 0.3934, train metric: 0.8579, valid metric: 0.8486\n",
            "Epoch 1/1, train loss: 0.3755, train metric: 0.8661, valid metric: 0.8532\n",
            "Epoch 1/1, train loss: 0.3605, train metric: 0.8696, valid metric: 0.8654\n",
            "Epoch 1/1, train loss: 0.3481, train metric: 0.8739, valid metric: 0.8666\n",
            "Epoch 1/1, train loss: 0.3361, train metric: 0.8778, valid metric: 0.8712\n",
            "Epoch 1/1, train loss: 0.3266, train metric: 0.8820, valid metric: 0.8696\n",
            "Epoch 1/1, train loss: 0.3184, train metric: 0.8843, valid metric: 0.8704\n",
            "Epoch 1/1, train loss: 0.3097, train metric: 0.8867, valid metric: 0.8798\n",
            "Epoch 1/1, train loss: 0.3024, train metric: 0.8904, valid metric: 0.8678\n",
            "Epoch 1/1, train loss: 0.2953, train metric: 0.8916, valid metric: 0.8726\n",
            "Epoch 1/1, train loss: 0.2890, train metric: 0.8934, valid metric: 0.8710\n",
            "Epoch 1/1, train loss: 0.2827, train metric: 0.8961, valid metric: 0.8714\n",
            "Epoch 1/1, train loss: 0.2762, train metric: 0.8988, valid metric: 0.8822\n",
            "Epoch 1/1, train loss: 0.2704, train metric: 0.9007, valid metric: 0.8798\n",
            "Epoch 1/1, train loss: 0.2648, train metric: 0.9031, valid metric: 0.8742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:11:36,842] Trial 6 finished with value: 0.8867999911308289 and parameters: {'learning_rate': 0.021368329072358756, 'n_hidden': 79}. Best is trial 6 with value: 0.8867999911308289.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.2600, train metric: 0.9045, valid metric: 0.8868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:11:55,404] Trial 7 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.2926, train metric: 0.1081, valid metric: 0.1064\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:12:16,031] Trial 8 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.2836, train metric: 0.1225, valid metric: 0.1526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:12:37,170] Trial 9 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 2.2631, train metric: 0.2483, valid metric: 0.3554\n",
            "Epoch 1/1, train loss: 0.6987, train metric: 0.7437, valid metric: 0.8334\n",
            "Epoch 1/1, train loss: 0.4549, train metric: 0.8365, valid metric: 0.8278\n",
            "Epoch 1/1, train loss: 0.4155, train metric: 0.8482, valid metric: 0.8504\n",
            "Epoch 1/1, train loss: 0.3915, train metric: 0.8569, valid metric: 0.8400\n",
            "Epoch 1/1, train loss: 0.3730, train metric: 0.8627, valid metric: 0.8458\n",
            "Epoch 1/1, train loss: 0.3614, train metric: 0.8673, valid metric: 0.8576\n",
            "Epoch 1/1, train loss: 0.3489, train metric: 0.8711, valid metric: 0.8520\n",
            "Epoch 1/1, train loss: 0.3423, train metric: 0.8746, valid metric: 0.8584\n",
            "Epoch 1/1, train loss: 0.3348, train metric: 0.8766, valid metric: 0.8492\n",
            "Epoch 1/1, train loss: 0.3294, train metric: 0.8776, valid metric: 0.8642\n",
            "Epoch 1/1, train loss: 0.3232, train metric: 0.8813, valid metric: 0.8634\n",
            "Epoch 1/1, train loss: 0.3200, train metric: 0.8823, valid metric: 0.8666\n",
            "Epoch 1/1, train loss: 0.3142, train metric: 0.8841, valid metric: 0.8576\n",
            "Epoch 1/1, train loss: 0.3103, train metric: 0.8851, valid metric: 0.8572\n",
            "Epoch 1/1, train loss: 0.3056, train metric: 0.8860, valid metric: 0.8620\n",
            "Epoch 1/1, train loss: 0.3025, train metric: 0.8882, valid metric: 0.8638\n",
            "Epoch 1/1, train loss: 0.3001, train metric: 0.8901, valid metric: 0.8712\n",
            "Epoch 1/1, train loss: 0.2959, train metric: 0.8899, valid metric: 0.8594\n",
            "Epoch 1/1, train loss: 0.2915, train metric: 0.8911, valid metric: 0.8690\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:21:29,198] Trial 10 finished with value: 0.8712000250816345 and parameters: {'learning_rate': 0.08165528450509137, 'n_hidden': 21}. Best is trial 6 with value: 0.8867999911308289.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.2903, train metric: 0.8910, valid metric: 0.8624\n",
            "Epoch 1/1, train loss: 0.6527, train metric: 0.7611, valid metric: 0.8132\n",
            "Epoch 1/1, train loss: 0.4503, train metric: 0.8352, valid metric: 0.8466\n",
            "Epoch 1/1, train loss: 0.4137, train metric: 0.8498, valid metric: 0.8394\n",
            "Epoch 1/1, train loss: 0.3905, train metric: 0.8579, valid metric: 0.8526\n",
            "Epoch 1/1, train loss: 0.3757, train metric: 0.8619, valid metric: 0.8458\n",
            "Epoch 1/1, train loss: 0.3639, train metric: 0.8650, valid metric: 0.8576\n",
            "Epoch 1/1, train loss: 0.3540, train metric: 0.8692, valid metric: 0.8544\n",
            "Epoch 1/1, train loss: 0.3479, train metric: 0.8723, valid metric: 0.8622\n",
            "Epoch 1/1, train loss: 0.3383, train metric: 0.8766, valid metric: 0.8430\n",
            "Epoch 1/1, train loss: 0.3310, train metric: 0.8788, valid metric: 0.8554\n",
            "Epoch 1/1, train loss: 0.3270, train metric: 0.8802, valid metric: 0.8578\n",
            "Epoch 1/1, train loss: 0.3208, train metric: 0.8809, valid metric: 0.8626\n",
            "Epoch 1/1, train loss: 0.3181, train metric: 0.8834, valid metric: 0.8636\n",
            "Epoch 1/1, train loss: 0.3121, train metric: 0.8839, valid metric: 0.8648\n",
            "Epoch 1/1, train loss: 0.3093, train metric: 0.8853, valid metric: 0.8604\n",
            "Epoch 1/1, train loss: 0.3066, train metric: 0.8870, valid metric: 0.8614\n",
            "Epoch 1/1, train loss: 0.3024, train metric: 0.8875, valid metric: 0.8464\n",
            "Epoch 1/1, train loss: 0.2975, train metric: 0.8896, valid metric: 0.8584\n",
            "Epoch 1/1, train loss: 0.2953, train metric: 0.8907, valid metric: 0.8598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:35:32,448] Trial 11 finished with value: 0.864799976348877 and parameters: {'learning_rate': 0.07553503645583182, 'n_hidden': 21}. Best is trial 6 with value: 0.8867999911308289.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.2934, train metric: 0.8900, valid metric: 0.8616\n",
            "Epoch 1/1, train loss: 0.6195, train metric: 0.7764, valid metric: 0.8170\n",
            "Epoch 1/1, train loss: 0.4180, train metric: 0.8470, valid metric: 0.8514\n",
            "Epoch 1/1, train loss: 0.3728, train metric: 0.8639, valid metric: 0.8418\n",
            "Epoch 1/1, train loss: 0.3476, train metric: 0.8711, valid metric: 0.8662\n",
            "Epoch 1/1, train loss: 0.3276, train metric: 0.8796, valid metric: 0.8694\n",
            "Epoch 1/1, train loss: 0.3118, train metric: 0.8847, valid metric: 0.8720\n",
            "Epoch 1/1, train loss: 0.2984, train metric: 0.8889, valid metric: 0.8734\n",
            "Epoch 1/1, train loss: 0.2870, train metric: 0.8918, valid metric: 0.8756\n",
            "Epoch 1/1, train loss: 0.2770, train metric: 0.8968, valid metric: 0.8490\n",
            "Epoch 1/1, train loss: 0.2694, train metric: 0.8987, valid metric: 0.8780\n",
            "Epoch 1/1, train loss: 0.2586, train metric: 0.9035, valid metric: 0.8754\n",
            "Epoch 1/1, train loss: 0.2516, train metric: 0.9064, valid metric: 0.8782\n",
            "Epoch 1/1, train loss: 0.2430, train metric: 0.9088, valid metric: 0.8814\n",
            "Epoch 1/1, train loss: 0.2356, train metric: 0.9109, valid metric: 0.8852\n",
            "Epoch 1/1, train loss: 0.2320, train metric: 0.9116, valid metric: 0.8864\n",
            "Epoch 1/1, train loss: 0.2251, train metric: 0.9146, valid metric: 0.8718\n",
            "Epoch 1/1, train loss: 0.2203, train metric: 0.9171, valid metric: 0.8828\n",
            "Epoch 1/1, train loss: 0.2139, train metric: 0.9191, valid metric: 0.8858\n",
            "Epoch 1/1, train loss: 0.2087, train metric: 0.9215, valid metric: 0.8866\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 10:51:40,569] Trial 12 finished with value: 0.8866000175476074 and parameters: {'learning_rate': 0.08525846269447779, 'n_hidden': 116}. Best is trial 6 with value: 0.8867999911308289.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.2031, train metric: 0.9227, valid metric: 0.8750\n",
            "Epoch 1/1, train loss: 0.8659, train metric: 0.7036, valid metric: 0.7852\n",
            "Epoch 1/1, train loss: 0.5123, train metric: 0.8183, valid metric: 0.8178\n",
            "Epoch 1/1, train loss: 0.4578, train metric: 0.8373, valid metric: 0.8270\n",
            "Epoch 1/1, train loss: 0.4268, train metric: 0.8485, valid metric: 0.8450\n",
            "Epoch 1/1, train loss: 0.4040, train metric: 0.8563, valid metric: 0.8486\n",
            "Epoch 1/1, train loss: 0.3879, train metric: 0.8620, valid metric: 0.8466\n",
            "Epoch 1/1, train loss: 0.3715, train metric: 0.8668, valid metric: 0.8324\n",
            "Epoch 1/1, train loss: 0.3584, train metric: 0.8709, valid metric: 0.8570\n",
            "Epoch 1/1, train loss: 0.3457, train metric: 0.8763, valid metric: 0.8674\n",
            "Epoch 1/1, train loss: 0.3347, train metric: 0.8788, valid metric: 0.8684\n",
            "Epoch 1/1, train loss: 0.3259, train metric: 0.8816, valid metric: 0.8664\n",
            "Epoch 1/1, train loss: 0.3171, train metric: 0.8862, valid metric: 0.8760\n",
            "Epoch 1/1, train loss: 0.3087, train metric: 0.8877, valid metric: 0.8750\n",
            "Epoch 1/1, train loss: 0.3015, train metric: 0.8907, valid metric: 0.8790\n",
            "Epoch 1/1, train loss: 0.2943, train metric: 0.8932, valid metric: 0.8750\n",
            "Epoch 1/1, train loss: 0.2883, train metric: 0.8958, valid metric: 0.8752\n",
            "Epoch 1/1, train loss: 0.2819, train metric: 0.8977, valid metric: 0.8666\n",
            "Epoch 1/1, train loss: 0.2767, train metric: 0.9001, valid metric: 0.8704\n",
            "Epoch 1/1, train loss: 0.2727, train metric: 0.9001, valid metric: 0.8638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:01:08,389] Trial 13 finished with value: 0.8790000081062317 and parameters: {'learning_rate': 0.01891149541864801, 'n_hidden': 116}. Best is trial 6 with value: 0.8867999911308289.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.2670, train metric: 0.9019, valid metric: 0.8784\n",
            "Epoch 1/1, train loss: 0.9440, train metric: 0.6748, valid metric: 0.7728\n",
            "Epoch 1/1, train loss: 0.5292, train metric: 0.8132, valid metric: 0.8268\n",
            "Epoch 1/1, train loss: 0.4689, train metric: 0.8347, valid metric: 0.8306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:02:40,287] Trial 14 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.4349, train metric: 0.8447, valid metric: 0.8292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:03:02,412] Trial 15 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 1.7583, train metric: 0.4682, valid metric: 0.6020\n",
            "Epoch 1/1, train loss: 0.7498, train metric: 0.7337, valid metric: 0.8142\n",
            "Epoch 1/1, train loss: 0.4690, train metric: 0.8333, valid metric: 0.8286\n",
            "Epoch 1/1, train loss: 0.4173, train metric: 0.8513, valid metric: 0.8294\n",
            "Epoch 1/1, train loss: 0.3842, train metric: 0.8616, valid metric: 0.8580\n",
            "Epoch 1/1, train loss: 0.3615, train metric: 0.8695, valid metric: 0.8562\n",
            "Epoch 1/1, train loss: 0.3442, train metric: 0.8756, valid metric: 0.8658\n",
            "Epoch 1/1, train loss: 0.3292, train metric: 0.8790, valid metric: 0.8742\n",
            "Epoch 1/1, train loss: 0.3160, train metric: 0.8836, valid metric: 0.8752\n",
            "Epoch 1/1, train loss: 0.3060, train metric: 0.8876, valid metric: 0.8788\n",
            "Epoch 1/1, train loss: 0.2967, train metric: 0.8900, valid metric: 0.8830\n",
            "Epoch 1/1, train loss: 0.2874, train metric: 0.8939, valid metric: 0.8734\n",
            "Epoch 1/1, train loss: 0.2793, train metric: 0.8978, valid metric: 0.8538\n",
            "Epoch 1/1, train loss: 0.2717, train metric: 0.9002, valid metric: 0.8606\n",
            "Epoch 1/1, train loss: 0.2646, train metric: 0.9021, valid metric: 0.8818\n",
            "Epoch 1/1, train loss: 0.2581, train metric: 0.9053, valid metric: 0.8838\n",
            "Epoch 1/1, train loss: 0.2521, train metric: 0.9077, valid metric: 0.8804\n",
            "Epoch 1/1, train loss: 0.2470, train metric: 0.9087, valid metric: 0.8836\n",
            "Epoch 1/1, train loss: 0.2407, train metric: 0.9107, valid metric: 0.8884\n",
            "Epoch 1/1, train loss: 0.2357, train metric: 0.9135, valid metric: 0.8844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:10:49,555] Trial 16 finished with value: 0.8884000182151794 and parameters: {'learning_rate': 0.03266629913173288, 'n_hidden': 142}. Best is trial 16 with value: 0.8884000182151794.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.2294, train metric: 0.9157, valid metric: 0.8854\n",
            "Epoch 1/1, train loss: 0.7843, train metric: 0.7313, valid metric: 0.8150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:11:45,160] Trial 17 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 0.4771, train metric: 0.8312, valid metric: 0.8170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:12:10,628] Trial 18 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 1.7815, train metric: 0.4880, valid metric: 0.6210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-18 11:12:36,561] Trial 19 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, train loss: 1.5623, train metric: 0.4917, valid metric: 0.6522\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "sampler = optuna.samplers.TPESampler(seed=42)\n",
        "pruner = optuna.pruners.MedianPruner()\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler,\n",
        "                            pruner=pruner)\n",
        "study.optimize(objective_with_data, n_trials=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* `partial(objective, train_loader=..., valid_loader=...)` cria uma **versão da função** `objective` já “pré-preenchida” com os *loaders* — assim `study.optimize(...)` pode chamá-la **sem argumentos extras** (só o `trial`).\n",
        "* `torch.manual_seed(42)` e `TPESampler(seed=42)` fixam **seeds** para reprodutibilidade (mesmas amostragens de hiperparâmetros e mesmos pesos iniciais).\n",
        "* `MedianPruner()` ativa **poda de trials**: se o desempenho parcial do trial ficar **abaixo da mediana** dos anteriores no mesmo passo (época), o Optuna **interrompe cedo** esse trial, economizando tempo.\n",
        "* `create_study(direction=\"maximize\", sampler=..., pruner=...)` cria o estudo para **maximizar** a métrica retornada pela `objective` (a sua é acurácia de validação).\n",
        "* `study.optimize(objective_with_data, n_trials=20)` roda **20 trials**, cada um testando combinações diferentes (via **TPE**), com poda automática pelo pruner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "IvmME-y8Mw3I",
        "outputId": "d0896d4c-6987-459c-ad0f-fee2d1c3594d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8884000182151794"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "uSb5AvclMw3J",
        "outputId": "429d548a-87fe-4c82-9c2a-a13902603146"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'learning_rate': 0.03266629913173288, 'n_hidden': 142}"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JJksBDLMw3J"
      },
      "source": [
        "# 9. Saving and Loading a PyTorch Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP3rFs30Wm80"
      },
      "source": [
        "- **Método mais simples:**  \n",
        "  Utilizar `torch.save()` passando o **modelo** e o **caminho do arquivo**.\n",
        "\n",
        "- **Como funciona:**  \n",
        "  1. O objeto do modelo é **serializado** usando o módulo `pickle` do Python, que converte objetos em uma sequência de bytes.  \n",
        "  2. Em seguida, o resultado é **compactado (zip)** e salvo no disco.\n",
        "\n",
        "- **Convenção de arquivos:**  \n",
        "  - Extensões comuns: `.pt` ou `.pth`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "sg7YW9OWMw3J"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"my_fashion_mnist.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j6WcQI9YKMy"
      },
      "source": [
        "Definir weights_only=False garante que todo o objeto do modelo seja carregado, e não apenas os parâmetros do modelo. Assim, você pode usar o modelo carregado para inferência. Não se esqueça de alternar para o modo de avaliação primeiro usando o método eval():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "TqPDfojtMw3J"
      },
      "outputs": [],
      "source": [
        "loaded_model = torch.load(\"my_fashion_mnist.pt\", weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "GoTHOikDMw3J"
      },
      "outputs": [],
      "source": [
        "loaded_model.eval()\n",
        "y_pred_logits = loaded_model(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrjMVY-aYzm9"
      },
      "source": [
        "Embora salvar o modelo completo com `torch.save()` seja fácil e prático, essa abordagem apresenta **problemas sérios**:\n",
        "\n",
        "1. **Segurança:**  \n",
        "   - O formato de serialização do `pickle` é **notoriamente inseguro**.  \n",
        "   - Embora `torch.save()` não salve código personalizado diretamente, o formato pickle suporta isso.  \n",
        "   - Um modelo malicioso pode conter **código que será executado automaticamente** ao carregar o modelo.  \n",
        "   - **Regra importante:** só carregue modelos de fontes confiáveis.\n",
        "\n",
        "2. **Fragilidade e compatibilidade:**  \n",
        "   - Pickle pode ser **suscetível a mudanças entre versões do Python** (ex.: grandes mudanças entre Python 3.7 e 3.8).  \n",
        "   - Ele salva caminhos de arquivos específicos do código, o que pode **quebrar o carregamento** se a estrutura de pastas for diferente.\n",
        "\n",
        "### Recomendação:\n",
        "Para evitar esses problemas, é **melhor salvar e carregar apenas os pesos do modelo**, em vez de salvar o objeto completo. Isso mantém a segurança e aumenta a compatibilidade entre diferentes ambientes e versões do Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "-wo3CdVxMw3J"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"my_fashion_mnist_weights.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "3CFwOqaMMw3J",
        "outputId": "75848dcd-0c02-40ce-df99-4d07e96fd796"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "collections.OrderedDict"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLvhrYywZRLJ"
      },
      "source": [
        "Para aumentar a segurança e compatibilidade ao salvar modelos PyTorch, é recomendado salvar **apenas os pesos** do modelo, usando o **state dictionary**.\n",
        "\n",
        "### O que é `state_dict()`:\n",
        "- Retorna um **OrderedDict do Python** contendo:\n",
        "  - Cada **parâmetro do modelo** (retornado por `named_parameters()`).\n",
        "  - **Buffers**, se houver, que são tensores registrados via `register_buffer()`.  \n",
        "    - Buffers armazenam dados adicionais necessários ao modelo, mas **não são parâmetros treináveis** (ex.: estatísticas de batch-norm).\n",
        "\n",
        "### Como carregar apenas os pesos:\n",
        "1. Crie um modelo com a **mesma estrutura** do modelo original.  \n",
        "2. Carregue os pesos usando `torch.load()` com `weights_only=True`.  \n",
        "3. Aplique os pesos ao modelo com `load_state_dict()`.  \n",
        "4. Coloque o modelo em **modo de avaliação** antes da inferência.\n",
        "\n",
        "```python\n",
        "# Criação do modelo com a mesma arquitetura\n",
        "model = MyModel()\n",
        "\n",
        "# Carrega apenas os pesos\n",
        "weights = torch.load(\"meus_pesos.pth\", weights_only=True)\n",
        "\n",
        "# Aplica os pesos ao modelo\n",
        "model.load_state_dict(weights)\n",
        "\n",
        "# Coloca o modelo em modo de avaliação\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "fYolTJFYMw3J",
        "outputId": "997497af-59c5-4725-daea-8f6fae3319c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassifier(\n",
              "  (mlp): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_model = ImageClassifier(n_inputs=1 * 28 * 28, n_hidden1=300, n_hidden2=100,\n",
        "                            n_classes=10)\n",
        "loaded_weights = torch.load(\"my_fashion_mnist_weights.pt\", weights_only=True)\n",
        "new_model.load_state_dict(loaded_weights)\n",
        "new_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "FF4l8kq4Mw3J"
      },
      "outputs": [],
      "source": [
        "model_data = {\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"model_hyperparameters\": {\n",
        "        \"n_inputs\": 1 * 28 * 28,\n",
        "        \"n_hidden1\": 300,\n",
        "        \"n_hidden2\": 100,\n",
        "        \"n_classes\": 10,\n",
        "    }\n",
        "}\n",
        "torch.save(model_data, \"my_fashion_mnist_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Wu817PkTMw3J",
        "outputId": "c79bf9e9-d757-44d0-fa90-99b3e064f52a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassifier(\n",
              "  (mlp): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=300, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=300, out_features=100, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaded_data = torch.load(\"my_fashion_mnist_model.pt\", weights_only=True)\n",
        "new_model = ImageClassifier(**loaded_data[\"model_hyperparameters\"])\n",
        "new_model.load_state_dict(loaded_data[\"model_state_dict\"])\n",
        "new_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js9FW1KzMw3J"
      },
      "source": [
        "# 10. Compiling and Optimizing a PyTorch Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjwmu2GJPiWh"
      },
      "source": [
        "O PyTorch possui um recurso poderoso chamado **TorchScript**, que permite transformar o código do modelo em uma versão **estaticamente tipada** do Python.  \n",
        "\n",
        "#### Benefícios principais:\n",
        "\n",
        "1. **Otimização e desempenho**  \n",
        "   - TorchScript pode ser **compilado e otimizado**, tornando o modelo mais rápido.  \n",
        "   - Exemplos de otimizações:\n",
        "     - **Fusão de operações**: várias operações podem ser combinadas em uma única operação mais eficiente.  \n",
        "     - **Constant folding**: operações com constantes (ex.: `2 * 3`) são substituídas pelo resultado (`6`).  \n",
        "     - Código não utilizado pode ser removido (pruning).\n",
        "\n",
        "2. **Portabilidade e serialização**  \n",
        "   - Modelos podem ser **salvos em disco** e carregados tanto em **Python** quanto em **C++** usando a biblioteca **LibTorch**.  \n",
        "   - Possibilita a execução em **diversos dispositivos**, incluindo embarcados.\n",
        "\n",
        "#### Formas de conversão:\n",
        "\n",
        "- **Tracing**: O PyTorch executa o modelo com **dados de exemplo**, registra todas as operações e converte esse registro em TorchScript.  \n",
        "  - Implementação: usando a função `torch.jit.trace()`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Fm8rAbQgMw3K"
      },
      "outputs": [],
      "source": [
        "torchscript_model = torch.jit.trace(model, X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "oi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5sj3rJbPr_D"
      },
      "source": [
        "\n",
        "O **tracing** funciona bem para modelos **estáticos**, cujo método `forward()` não utiliza **condicionais** ou **loops**.  \n",
        "\n",
        "- Problemas do tracing em modelos dinâmicos:\n",
        "  - Se o modelo contém **`if` ou `match`**, apenas a **ramificação executada** será capturada.  \n",
        "  - Se o modelo contém **loops**, o TorchScript registrará **uma cópia das operações para cada iteração executada**, o que geralmente não é desejado.\n",
        "\n",
        "#### Alternativa: **Scripting**\n",
        "- O PyTorch **analisa diretamente o código Python** e converte em TorchScript.  \n",
        "- Suporta:\n",
        "  - **`if`** e **`while`** (condições baseadas em tensores).  \n",
        "  - **`for`** ao iterar sobre tensores.\n",
        "\n",
        "- Limitações:\n",
        "  - Apenas um **subconjunto de Python** é suportado:\n",
        "    - Não pode usar **variáveis globais**, **generators (`yield`)**, **list comprehensions complexas**, **argumentos de função variáveis (`*args` ou `**kwargs`)**, ou **`match`**.  \n",
        "    - Tipos devem ser **fixos** (uma função não pode retornar inteiro em alguns casos e float em outros).  \n",
        "    - Chamadas a funções externas só funcionam se também respeitarem essas regras (sem bibliotecas padrão ou de terceiros).  \n",
        "\n",
        "> Apesar das restrições, para a maioria dos modelos reais, é relativamente fácil seguir essas regras.  \n",
        "> Modelos podem ser **salvos e reutilizados** usando scripting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "ED6bPWO3Mw3K"
      },
      "outputs": [],
      "source": [
        "torchscript_model = torch.jit.script(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyra7sk3P5JV"
      },
      "source": [
        "Independentemente de você ter usado **tracing** ou **scripting** para criar o modelo TorchScript, é possível realizar **otimizações adicionais** para melhorar desempenho e eficiência.\n",
        "\n",
        "Essas otimizações podem incluir:\n",
        "\n",
        "- **Fusão de operações** adicionais.  \n",
        "- **Remoção de código não utilizado** (pruning).  \n",
        "- **Constant folding** mais avançado.  \n",
        "\n",
        "> Essas melhorias ajudam a acelerar a execução do modelo e a reduzir o consumo de recursos.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "E842NQxiMw3K"
      },
      "outputs": [],
      "source": [
        "optimized_model = torch.jit.optimize_for_inference(torchscript_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ-Xp3cCP_lk"
      },
      "source": [
        "- **Finalidade:**  \n",
        "  Modelos convertidos para **TorchScript** são destinados **apenas para inferência** (predições).  \n",
        "\n",
        "- **Limitações:**  \n",
        "  - Não suportam **rastreio de gradiente** (`gradient tracking`).  \n",
        "  - Não permitem **atualização de parâmetros** durante treinamento.  \n",
        "\n",
        "- **Salvamento de modelos:**  \n",
        "  Um modelo TorchScript pode ser salvo usando o método `save()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "B45rJUM2Mw3K"
      },
      "outputs": [],
      "source": [
        "optimized_model.save(\"my_fashion_mnist_torchscript.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glJBhdqVVxDv"
      },
      "source": [
        "E então carregue-o usando a função torch.jit.load():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "_IIuVPTeMw3K"
      },
      "outputs": [],
      "source": [
        "loaded_torchscript_model = torch.jit.load(\"my_fashion_mnist_torchscript.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "d-A3AuSqMw3K",
        "outputId": "5aa5f832-2893-465f-b57b-e4afac968527"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ -3.5579,  -0.6257,  -3.8845,  -2.6245,  -3.3111,   2.5662,  -3.4226,\n",
              "           9.2912,  -0.1451,   6.9049],\n",
              "        [ -0.1743,  -1.5920,  10.4675,  -1.6848,  16.2073, -12.6346,   7.4614,\n",
              "         -10.5086,  -0.8643,  -7.8964],\n",
              "        [  0.7588,  -2.5662,   7.3250,  -1.2094,   4.9593,  -5.6175,   5.7191,\n",
              "          -3.9459,  -0.6780,  -4.6496]])"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_logits = loaded_torchscript_model(X_new)\n",
        "y_pred_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQ3Uo5hRfob"
      },
      "source": [
        "\n",
        "- **TorchScript** não está mais em desenvolvimento ativo:\n",
        "  - Correções de bugs ainda ocorrem, mas **nenhuma nova funcionalidade** é adicionada.\n",
        "- Continua sendo útil para **executar modelos PyTorch em C++**.  \n",
        "\n",
        "- Com o **PyTorch 2.0 (março de 2023)**, o foco mudou para **novas ferramentas de compilação** baseadas na função `torch.compile()`, que são:\n",
        "  - Fáceis de usar  \n",
        "  - Mais eficientes  \n",
        "  - Melhor adaptadas para modelos dinâmicos e otimizações automáticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "j2j3PM0_Mw3L"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'cache_on_self_and_args' from 'torch._inductor.utils' (c:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_inductor\\utils.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m compiled_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\__init__.py:2640\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[39m\n\u001b[32m   2637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2638\u001b[39m     backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n\u001b[32m-> \u001b[39m\u001b[32m2640\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dynamo\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnopython\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguard_filter_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguard_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m(model)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1263\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1260\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnopython\u001b[39m\u001b[33m\"\u001b[39m] = ca_kwargs_override[\u001b[33m\"\u001b[39m\u001b[33mfullgraph\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrebuild_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1367\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(rebuild_ctx, backend, nopython, error_on_graph_break, guard_export_fn, guard_fail_fn, guard_filter_fn, disable, dynamic, package)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompilePackage\n\u001b[32m   1352\u001b[39m     package = CompilePackage(fn=\u001b[38;5;28;01mNone\u001b[39;00m, dynamo=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_inlined_sources=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_catch_errors(\n\u001b[32m   1355\u001b[39m     convert_frame.convert_frame(\n\u001b[32m   1356\u001b[39m         backend,\n\u001b[32m   1357\u001b[39m         hooks,\n\u001b[32m   1358\u001b[39m         package=package,\n\u001b[32m   1359\u001b[39m     ),\n\u001b[32m   1360\u001b[39m     hooks,\n\u001b[32m   1361\u001b[39m     backend_ctx_ctor,\n\u001b[32m   1362\u001b[39m     fullgraph=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1363\u001b[39m     error_on_graph_break=error_on_graph_break\n\u001b[32m   1364\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.debug_force_graph_break_on_leaf_return,\n\u001b[32m   1365\u001b[39m     dynamic=dynamic,\n\u001b[32m   1366\u001b[39m     compiler_config=(\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m         \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_compiler_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[33m\"\u001b[39m\u001b[33mget_compiler_config\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1369\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1370\u001b[39m     ),\n\u001b[32m   1371\u001b[39m     rebuild_ctx=rebuild_ctx,\n\u001b[32m   1372\u001b[39m     package=package,\n\u001b[32m   1373\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\__init__.py:2395\u001b[39m, in \u001b[36m_TorchCompileInductorWrapper.get_compiler_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2394\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_compiler_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2395\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_patched_config_dict\n\u001b[32m   2397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_patched_config_dict(config_patches=\u001b[38;5;28mself\u001b[39m.config)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:69\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcudagraph_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     63\u001b[39m     BoxedDeviceIndex,\n\u001b[32m     64\u001b[39m     format_default_skip_message,\n\u001b[32m     65\u001b[39m     log_cudagraph_skip_and_bump_counter,\n\u001b[32m     66\u001b[39m     PlaceholderInfo,\n\u001b[32m     67\u001b[39m )\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcustom_graph_pass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomPartitionerFn\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     70\u001b[39m     create_mapping_pre_post_grad_nodes,\n\u001b[32m     71\u001b[39m     save_args_for_compile_fx_inner,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_code\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     CompiledAOTI,\n\u001b[32m     75\u001b[39m     CompiledFxGraph,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     OutputCode,\n\u001b[32m     80\u001b[39m )\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_dir_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_dir\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_inductor\\debug.py:36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree_map\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, ir  \u001b[38;5;66;03m# noqa: F811, this is needed\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mir\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExternKernel\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     39\u001b[39m     BaseSchedulerNode,\n\u001b[32m     40\u001b[39m     FusedSchedulerNode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     SchedulerNode,\n\u001b[32m     44\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_inductor\\ir.py:97\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbenchmarking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m benchmarker\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhints\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceProperties, ReductionHint\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     98\u001b[39m     argsort,\n\u001b[32m     99\u001b[39m     argsort_sym,\n\u001b[32m    100\u001b[39m     cache_on_self,\n\u001b[32m    101\u001b[39m     cache_on_self_and_args,\n\u001b[32m    102\u001b[39m     ceildiv,\n\u001b[32m    103\u001b[39m     convert_shape_to_inductor,\n\u001b[32m    104\u001b[39m     convert_shape_to_symint,\n\u001b[32m    105\u001b[39m     developer_warning,\n\u001b[32m    106\u001b[39m     do_bench_using_profiling,\n\u001b[32m    107\u001b[39m     dtype_from_size,\n\u001b[32m    108\u001b[39m     get_dtype_size,\n\u001b[32m    109\u001b[39m     get_kernel_metadata,\n\u001b[32m    110\u001b[39m     GPU_ALIGN_BYTES,\n\u001b[32m    111\u001b[39m     ir_dataclass,\n\u001b[32m    112\u001b[39m     is_dynamic,\n\u001b[32m    113\u001b[39m     is_gpu,\n\u001b[32m    114\u001b[39m     sympy_dot,\n\u001b[32m    115\u001b[39m     sympy_index_symbol,\n\u001b[32m    116\u001b[39m     sympy_index_symbol_with_prefix,\n\u001b[32m    117\u001b[39m     sympy_product,\n\u001b[32m    118\u001b[39m     sympy_subs,\n\u001b[32m    119\u001b[39m     tensor_is_aligned,\n\u001b[32m    120\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvirtualized\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops, OpsValue, V\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'cache_on_self_and_args' from 'torch._inductor.utils' (c:\\Users\\estev\\OneDrive\\Área de Trabalho\\NCIA\\.ncia\\Lib\\site-packages\\torch\\_inductor\\utils.py)"
          ]
        }
      ],
      "source": [
        "compiled_model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9ggYSxlMw3L",
        "outputId": "2c9024d4-f272-4d76-e58c-286387c95672"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1007 08:38:10.662000 360 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        }
      ],
      "source": [
        "if device == \"cuda\":\n",
        "    y_pred_logits = compiled_model(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM7wpZ-0QvhV"
      },
      "source": [
        "Após criar um modelo TorchScript, ele pode ser usado **normalmente para inferência** e será **compilado e otimizado automaticamente**.  \n",
        "\n",
        "- Isso é conhecido como **Just-In-Time (JIT) compilation**, em contraste com **Ahead-Of-Time (AOT)**.  \n",
        "- No PyTorch 2.x, `torch.compile()` utiliza internamente:\n",
        "  - **TorchDynamo**: captura o grafo de computação do modelo a partir do bytecode Python, permitindo lidar corretamente com **condicionais** e **loops**.  \n",
        "  - **TorchInductor**: realiza a compilação e otimização real, gerando código altamente eficiente:\n",
        "    - GPU Nvidia: usando a linguagem **Triton**  \n",
        "    - CPU: usando **OpenMP**  \n",
        "  - Outros backends: por exemplo, **XLA** para TPUs (configurável com `device=\"xla\"`).\n",
        "\n",
        "> Com essas ferramentas, você pode construir e treinar **redes neurais complexas e eficientes**.\n",
        "\n",
        "---\n",
        "\n",
        "### Próximos Passos no Aprendizado com PyTorch\n",
        "\n",
        "Nos próximos capítulos, você aprenderá a trabalhar com:\n",
        "\n",
        "- **Redes muito profundas**  \n",
        "- **Arquiteturas populares**:\n",
        "  - CNNs para imagens  \n",
        "  - RNNs para dados sequenciais  \n",
        "  - Transformers para texto  \n",
        "  - Autoencoders para representação  \n",
        "  - GANs e modelos de difusão para geração de dados\n",
        "- **Reinforcement learning** para agentes autônomos  \n",
        "- **Deploy e otimização de modelos PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dImjG69aMw3L"
      },
      "source": [
        "![NCIA2](NCIA_Images\\end.png)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".ncia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
