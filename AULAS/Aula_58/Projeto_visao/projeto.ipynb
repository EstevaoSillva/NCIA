{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto End-to-End: Fine-tuning de YOLO (Ultralytics) para Detec√ß√£o de Capacete (PPE)\n",
    "\n",
    "\n",
    "## 1. Objetivos de Aprendizagem\n",
    "\n",
    "Ao final deste projeto, voc√™ ser√° capaz de:\n",
    "*   **Versionamento**: Gerenciar vers√µes de datasets de forma profissional.\n",
    "*   **Auditoria e QA**: Identificar e corrigir labels ruidosas antes de treinar.\n",
    "*   **Augmentations**: Criar pipelines de aumento de dados que respeitam a distribui√ß√£o f√≠sica do problema.\n",
    "*   **Transfer Learning**: Adaptar pesos pr√©-treinados no COCO para um dom√≠nio espec√≠fico (EPIs).\n",
    "*   **Avalia√ß√£o**: Ir al√©m do mAP e fazer an√°lise de erros com matriz de confus√£o e visualiza√ß√£o de *false positives*.\n",
    "*   **HPO**: Otimizar hiperpar√¢metros (LR, Momentum, Decay) para extrair a √∫ltima gota de performance.\n",
    "\n",
    "## 2. Vis√£o Geral do Pipeline\n",
    "\n",
    "```ascii\n",
    "[Internet/Source] \n",
    "      ‚¨á\n",
    "[Dataset Raw] ‚û° [Auditoria/Cleaning] ‚û° [Dataset Gold v1.0]\n",
    "                                             ‚¨á\n",
    "                                    [Data Augmentation (Albumentations)]\n",
    "                                             ‚¨á\n",
    "                                    [Dataset Ready v2.0]\n",
    "                                             ‚¨á\n",
    "[YOLO11 Pre-trained] ‚û° [Baseline Train] ‚û° [Eval Baseline] ‚û° [Error Analysis]\n",
    "                                                                     ‚¨á\n",
    "                                                         [HPO (Optuna) - Tuning]\n",
    "                                                                     ‚¨á\n",
    "                                                         [Training Final (Best Params)]\n",
    "                                                                     ‚¨á\n",
    "                                                         [Export ONNX/TensorRT] ‚û° [Inference demo]\n",
    "```\n",
    "\n",
    "## 3. Conceitos Chave\n",
    "\n",
    "### O que √© Transfer Learning no contexto YOLO?\n",
    "Em vez de iniciar o treinamento com pesos aleat√≥rios (o que exigiria milh√µes de imagens e dias de treino), usamos **pesos pr√©-treinados** em datasets massivos (como o COCO, que tem 80 classes gerais).\n",
    "No **Fine-tuning**, \"congelamos\" (ou treinamos com LR baixo) as primeiras camadas que detectam caracter√≠sticas b√°sicas (bordas, texturas) e treinamos agressivamente apenas as √∫ltimas camadas (Head) para reconhecer nossas novas classes (Capacete, Colete, Sem Capacete).\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> **Regras de Ouro**\n",
    "> 1.  **Nunca vaze dados**: O conjunto de Teste √© sagrado. Nunca olhe para ele durante o tuning de hiperpar√¢metros. Use Valida√ß√£o para isso.\n",
    "> 2.  **Me√ßa antes de otimizar**: N√£o mude augmentation ou modelo sem ter um baseline s√≥lido.\n",
    "> 3.  **Garbage In, Garbage Out**: 1 hora limpando o dataset vale mais que 10 horas tunando learning rate. A qualidade do label √© o teto da sua performance.\n",
    "> 4.  **Distribui√ß√£o importa**: Augmentations devem simular varia√ß√µes reais. Se sua c√¢mera √© fixa, n√£o faz sentido fazer rota√ß√µes de 180 graus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. Contextualiza√ß√£o: Vis√£o Cl√°ssica vs Deep Learning\n",
    "\n",
    "Antes de mergulhar no treinamento, √© crucial entender *por que* estamos usando Deep Learning (YOLO) e n√£o t√©cnicas cl√°ssicas.\n",
    "\n",
    "| Aspecto | Vis√£o Cl√°ssica (OpenCV/Geometria) | Deep Learning (YOLO/CNNs) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Foco** | Geometria, Medi√ß√£o Exata, Bordas | Sem√¢ntica, Classifica√ß√£o, Robustez |\n",
    "| **Exemplo** | Medir o di√¢metro de um parafuso em micr√¥metros. | Detectar se um oper√°rio est√° usando capacete. |\n",
    "| **Vantagem** | Determin√≠stico, Baixa Lat√™ncia, Explica o \"Como\". | Generaliza bem, ignora ru√≠do, ignora ilumina√ß√£o vari√°vel. |\n",
    "| **Case Real** | **Metrologia Industrial**: Garantir toler√¢ncia de pe√ßas. <br> **Visual SLAM**: Rob√¥s que mapeiam t√∫neis sem GPS. <br> **Leitura de C√≥digo de Barras**: Decodifica√ß√£o de bits 0/1. | **Seguran√ßa**: PPE, Detec√ß√£o de Armas. <br> **Aut√¥nomos**: Detectar pedestres/carros. <br> **M√©dico**: Detectar tumores em Raio-X. |\n",
    "\n",
    "> **Resumo**: Use **Deep Learning** para responder \"O que √© isso?\". Use **Vis√£o Cl√°ssica** para responder \"Qual o tamanho exato disso?\" ou \"Onde isso est√° exatamente?\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup do Ambiente\n",
    "\n",
    "Antes de come√ßar, precisamos garantir que o ambiente est√° pronto. PyTorch e YOLO dependem fortemente da GPU para serem vi√°veis.\n",
    "Checar a VRAM √© vital para decidir o `batch_size`. Na RTX 4060 (8GB), modelos `Nano` e `Small` rodam folgados. `Medium` exige cuidado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:43.934499713Z",
     "start_time": "2026-01-13T15:04:43.651206420Z"
    }
   },
   "source": [
    "# Instala√ß√£o de depend√™ncias\n",
    "# Usamos 'uv pip' se dispon√≠vel para velocidade, ou pip padr√£o.\n",
    "# A flag -q (quiet) reduz o output.\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages(packages):\n",
    "    print(f\"Instalando: {', '.join(packages)}...\")\n",
    "    # Tenta usar uv se estiver no path, senao usa pip module\n",
    "    try:\n",
    "        subprocess.check_call([\"uv\", \"pip\", \"install\"] + packages)\n",
    "    except FileNotFoundError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages)\n",
    "\n",
    "# Lista de libs essenciais para o projeto\n",
    "libs = [\n",
    "    \"ultralytics\",       # YOLO11 framework\n",
    "    \"opencv-python\",     # Vis√£o computacional b√°sica\n",
    "    \"matplotlib\",        # Gr√°ficos\n",
    "    \"numpy\", \n",
    "    \"pandas\",            # An√°lise de logs\n",
    "    \"pyyaml\",            # Configs\n",
    "    \"tqdm\",              # Barras de progresso\n",
    "    \"albumentations\",    # Augmentations avan√ßados\n",
    "    \"optuna\",            # HPO\n",
    "    \"supervision\"        # Visualiza√ß√£o e Utilit√°rios extras\n",
    "    \"mlflow\",            # Experiment Tracking\n",
    "]\n",
    "\n",
    "# Descomente a linha abaixo para instalar (pode demorar um pouco na primeira vez)\n",
    "# install_packages(libs)\n",
    "print(\"Depend√™ncias verificadas.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depend√™ncias verificadas.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:44.316655429Z",
     "start_time": "2026-01-13T15:04:43.980914496Z"
    }
   },
   "source": [
    "import json\n",
    "import torch\n",
    "import ultralytics\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Configura√ß√µes de exibi√ß√£o\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verifica√ß√µes de Vers√£o e Hardware\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Ultralytics Version: {ultralytics.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "\n",
    "# Check CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Dispon√≠vel: Sim\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"CUDA N√ÉO DETECTADO. O treinamento ser√° extremamente lento.\")\n",
    "    device = 'cpu'\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu130\n",
      "Ultralytics Version: 8.3.253\n",
      "OpenCV Version: 4.12.0\n",
      "CUDA Dispon√≠vel: Sim\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "VRAM Total: 6.09 GB\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:44.909745061Z",
     "start_time": "2026-01-13T15:04:44.422410433Z"
    }
   },
   "source": [
    "# Reprodutibilidade (Seeds)\n",
    "# Embora opera√ß√µes em GPU tenham algum n√£o-determinismo inerente, fixar seeds ajuda na consist√™ncia.\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True # Pode deixar lento\n",
    "        # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Seed fixada em: {SEED}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed fixada em: 42\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:45.460333310Z",
     "start_time": "2026-01-13T15:04:45.085676315Z"
    }
   },
   "source": [
    "import pathlib\n",
    "# Configura√ß√£o de Diret√≥rios do Projeto\n",
    "# √â fundamental manter o workspace organizado para n√£o misturar experimentos.\n",
    "\n",
    "BASE_DIR = os.path.abspath(\"./workspace\")\n",
    "DATA_ROOT = os.path.join(BASE_DIR, \"datasets\")\n",
    "RUNS_DIR = os.path.join(BASE_DIR, \"runs\")\n",
    "ARTIFACTS_DIR = os.path.join(BASE_DIR, \"artifacts\") # Pesos finais, onnx, etc\n",
    "REPORTS_DIR = os.path.join(BASE_DIR, \"reports\")     # Gr√°ficos e an√°lises de erro\n",
    "\n",
    "dirs = [DATA_ROOT, RUNS_DIR, ARTIFACTS_DIR, REPORTS_DIR]\n",
    "\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    print(f\"Diret√≥rio garantido: {d}\")\n",
    "\n",
    "# Vari√°veis Globais de Configura√ß√£o\n",
    "PROJECT_NAME = \"yolo_ppe_finetune\"\n",
    "\n",
    "# Configura√ß√£o do MLflow\n",
    "import mlflow\n",
    "\n",
    "# Define onde os logs 'oficiais' do MLflow ficam.\n",
    "# Pode ser local (./mlruns) ou um servidor remoto (http://...)\n",
    "mlflow.set_tracking_uri(pathlib.Path(os.path.join(BASE_DIR, \"mlruns\")).as_uri())\n",
    "mlflow.set_experiment(PROJECT_NAME)\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow Experiment: {PROJECT_NAME}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diret√≥rio garantido: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/datasets\n",
      "Diret√≥rio garantido: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/runs\n",
      "Diret√≥rio garantido: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/artifacts\n",
      "Diret√≥rio garantido: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/reports\n",
      "MLflow Tracking URI: file:///home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/mlruns\n",
      "MLflow Experiment: yolo_ppe_finetune\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepara√ß√£o do Dataset (PPE - Personal Protective Equipment)\n",
    "\n",
    "Para este projeto, usaremos um dataset de trabalhadores ('Hard Hat Workers') contendo 3 classes principais:\n",
    "1.  `helmet` (Capacete)\n",
    "2.  `vest` (Colete reflexivo)\n",
    "3.  `person` (Pessoa)\n",
    "\n",
    "> **Nota sobre Estrutura YOLO**:\n",
    "> O framework Ultralytics espera uma estrutura r√≠gida para evitar configura√ß√µes manuais complexas.\n",
    "> - `dataset/images/train`, `dataset/images/val`\n",
    "> - `dataset/labels/train`, `dataset/labels/val`\n",
    ">\n",
    "> Se seus labels n√£o estiverem na pasta paralela exata, o treino falhar√° silenciosamente ou dar√° erro de \"no labels found\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:45.959787800Z",
     "start_time": "2026-01-13T15:04:45.576967642Z"
    }
   },
   "source": [
    "# Op√ß√£o A: Download Autom√°tico (Exemplo com dataset p√∫blico no formato YOLO)\n",
    "dataset_url = \"https://github.com/gradio-app/gradio/raw/main/demo/yolo_detection/files/ppe_data.zip\" \n",
    "\n",
    "def download_dataset(url, dest_dir):\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    zip_path = os.path.join(dest_dir, \"dataset.zip\")\n",
    "    print(f\"Baixando dataset para {zip_path}...\")\n",
    "    try:\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"Download completo. Extraindo...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "        print(\"Extra√ß√£o conclu√≠da.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no download autom√°tio: {e}\")\n",
    "        print(\"--> USE A OP√á√ÉO B (Manual) <--\")"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:46.469038381Z",
     "start_time": "2026-01-13T15:04:46.009300041Z"
    }
   },
   "source": [
    "# Op√ß√£o B: Upload Manual (Fallback Inteligente)\n",
    "# Procura por zips comuns na raiz ou na pasta de datasets\n",
    "\n",
    "possible_zips = [\n",
    "    os.path.join(DATA_ROOT, \"hardhat_raw.zip\"),\n",
    "    os.path.join(BASE_DIR, \"..\", \"ppe_v1.zip\"),         # Na raiz do projeto\n",
    "    os.path.join(BASE_DIR, \"..\", \"hardhat_raw.zip\"),    # Na raiz do projeto\n",
    "]\n",
    "\n",
    "zip_path = None\n",
    "for p in possible_zips:\n",
    "    if os.path.exists(p):\n",
    "        zip_path = p\n",
    "        break\n",
    "\n",
    "if zip_path:\n",
    "    # Se achou na raiz, copia para o lugar certo\n",
    "    target_zip = os.path.join(DATA_ROOT, \"hardhat_raw.zip\")\n",
    "    if zip_path != target_zip:\n",
    "        print(f\"Encontrado zip em {zip_path}. Copiando para {target_zip}...\")\n",
    "        shutil.copy2(zip_path, target_zip)\n",
    "    else:\n",
    "        print(f\"Zip encontrado em {zip_path}.\")\n",
    "        \n",
    "    # Atualiza a vari√°vel para a c√©lula de extra√ß√£o usar (se necess√°rio)\n",
    "    zip_manual_path = target_zip\n",
    "else:\n",
    "    print(\"Nenhum zip encontrado automaticamente.\")\n",
    "    print(f\"Por favor, coloque 'hardhat_raw.zip' ou 'ppe_v1.zip' em: {DATA_ROOT}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum zip encontrado automaticamente.\n",
      "Por favor, coloque 'hardhat_raw.zip' ou 'ppe_v1.zip' em: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/datasets\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:46.797548524Z",
     "start_time": "2026-01-13T15:04:46.635480510Z"
    }
   },
   "source": [
    "# Estrutura√ß√£o e Limpeza (Standard YOLO)\n",
    "# Objetivo: mover tudo para ./workspace/datasets/ppe_v1/{images,labels}/{train,val,test}\n",
    "\n",
    "RAW_DIR = os.path.join(DATA_ROOT, \"raw_extract\") # Pasta tempor√°ria\n",
    "FINAL_DATASET_DIR = os.path.join(DATA_ROOT, \"ppe_v1\")\n",
    "\n",
    "def organize_yolo_structure(source, dest):\n",
    "    train_dir_check = os.path.join(dest, \"images\", \"train\")\n",
    "    if os.path.exists(train_dir_check) and len(os.listdir(train_dir_check)) > 50:\n",
    "        print(f\"Dataset destino j√° existe e parece v√°lido ({len(os.listdir(train_dir_check))} imagens no treino). Pulando organiza√ß√£o.\")\n",
    "        return\n",
    "    elif os.path.exists(dest):\n",
    "        print(f\"Dataset existe mas parece vazio/incompleto. Re-organizando...\")\n",
    "        # shutil.rmtree(dest) # Opcional: limpar antes\n",
    "\n",
    "\n",
    "    print(f\"Organizando {source} -> {dest} ...\")\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    \n",
    "    # Criar subpastas padr√£o YOLO\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(dest, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(dest, 'labels', split), exist_ok=True)\n",
    "    \n",
    "    images = []\n",
    "    for root, _, files in os.walk(source):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                images.append(os.path.join(root, f))\n",
    "                \n",
    "    random.shuffle(images)\n",
    "    split_idx = int(len(images) * 0.8)\n",
    "    train_imgs = images[:split_idx]\n",
    "    val_imgs = images[split_idx:]\n",
    "    \n",
    "    def move_files(file_list, split_name):\n",
    "        for img_path in file_list:\n",
    "            shutil.copy(img_path, os.path.join(dest, 'images', split_name, os.path.basename(img_path)))\n",
    "            \n",
    "            label_path = os.path.splitext(img_path)[0] + \".txt\"\n",
    "            if os.path.exists(label_path):\n",
    "                shutil.copy(label_path, os.path.join(dest, 'labels', split_name, os.path.basename(label_path)))\n",
    "            else:\n",
    "                with open(os.path.join(dest, 'labels', split_name, os.path.splitext(os.path.basename(img_path))[0] + \".txt\"), 'w') as f:\n",
    "                    pass\n",
    "\n",
    "    move_files(train_imgs, 'train')\n",
    "    move_files(val_imgs, 'val')\n",
    "    print(\"Organiza√ß√£o completa.\")"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:47.104380614Z",
     "start_time": "2026-01-13T15:04:46.896472148Z"
    }
   },
   "source": [
    "# Gerar data.yaml (A Identidade do Dataset)\n",
    "yaml_content = f'''\n",
    "path: {FINAL_DATASET_DIR} # dataset root dir\n",
    "train: images/train  # train images (relative to 'path') \n",
    "val: images/val      # val images (relative to 'path')\n",
    "test:  # test images (optional)\n",
    "\n",
    "names:\n",
    "  0: helmet\n",
    "  1: vest\n",
    "  2: person\n",
    "'''\n",
    "\n",
    "yaml_path = os.path.join(FINAL_DATASET_DIR, \"data.yaml\")\n",
    "\n",
    "# Check diret√≥rio antes de escrever (Bug fix)\n",
    "os.makedirs(FINAL_DATASET_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(yaml_path):\n",
    "    with open(yaml_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(yaml_content)\n",
    "    print(f\"data.yaml criado em: {yaml_path}\")\n",
    "else:\n",
    "    print(f\"data.yaml j√° existe em: {yaml_path}\")\n",
    "    \n",
    "# Mostrar conte√∫do\n",
    "print(\"--- CONTE√öDO DO DATA.YAML ---\")\n",
    "with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "print(\"-----------------------------\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.yaml j√° existe em: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/datasets/ppe_v1/data.yaml\n",
      "--- CONTE√öDO DO DATA.YAML ---\n",
      "\n",
      "path: /home/estevaosilva/PycharmProjects/NCIA/AULAS/Aula_58/Projeto_visao/workspace/datasets/ppe_v1 # dataset root dir\n",
      "train: images/train  # train images (relative to 'path') \n",
      "val: images/val      # val images (relative to 'path')\n",
      "test:  # test images (optional)\n",
      "\n",
      "names:\n",
      "  0: helmet\n",
      "  1: vest\n",
      "  2: person\n",
      "\n",
      "-----------------------------\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:47.301671773Z",
     "start_time": "2026-01-13T15:04:47.117477891Z"
    }
   },
   "source": [
    "# Valida√ß√£o B√°sica\n",
    "def validate_dataset(path):\n",
    "    print(f\"Validando dataset em: {path}\")\n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = os.path.join(path, 'images', split)\n",
    "        lbl_dir = os.path.join(path, 'labels', split)\n",
    "        \n",
    "        if not os.path.exists(img_dir):\n",
    "            print(f\"‚ö†Ô∏è Split {split} n√£o encontrado!\")\n",
    "            continue\n",
    "            \n",
    "        n_imgs = len(os.listdir(img_dir))\n",
    "        n_lbls = len(os.listdir(lbl_dir))\n",
    "        \n",
    "        print(f\"[{split.upper()}] Imagens: {n_imgs} | Labels: {n_lbls}\")\n",
    "        \n",
    "        if n_imgs != n_lbls:\n",
    "            print(f\"   üî¥ ALERTA: N√∫mero de imagens e labels difere!\")\n",
    "        elif n_imgs == 0:\n",
    "            print(f\"   üî¥ ALERTA: Split vazio!\")\n",
    "        else:\n",
    "            print(f\"   üü¢ OK.\")\n",
    "\n",
    "# validate_dataset(FINAL_DATASET_DIR)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Auditoria e Limpeza do Dataset\n",
    "\n",
    "\"Dados Limpos > Modelos Complexos\".\n",
    "Antes de treinar, precisamos garantir que o dataset n√£o tem lixo.\n",
    "Erros comuns em datasets de detec√ß√£o:\n",
    "1.  **Orphans**: Imagem sem txt ou txt sem imagem.\n",
    "2.  **Degenerate BBoxes**: Boxes com largura/altura = 0 ou muito pequenas.\n",
    "3.  **Out of Bounds**: Coordenadas > 1.0 ou < 0.0 (normaliza√ß√£o errada).\n",
    "4.  **Class IDs**: Classes que n√£o existem no `names` (ex: id 5 num dataset de 3 classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:47.373444460Z",
     "start_time": "2026-01-13T15:04:47.309346353Z"
    }
   },
   "source": [
    "# Fun√ß√µes de Valida√ß√£o e M√©tricas\n",
    "import glob\n",
    "\n",
    "def verify_yolo_label(lbl_path, num_classes=3):\n",
    "    issues = []\n",
    "    try:\n",
    "        if os.path.getsize(lbl_path) == 0:\n",
    "            return [\"empty_file\"] # Label vazio √© valido (sem objetos), mas bom saber.\n",
    "            \n",
    "        with open(lbl_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        for i, line in enumerate(lines):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                issues.append(f\"Line {i}: FormatErr (not 5 cols)\")\n",
    "                continue\n",
    "                \n",
    "            cls, x, y, w, h = map(float, parts)\n",
    "            \n",
    "            if not (0 <= int(cls) < num_classes):\n",
    "                issues.append(f\"Line {i}: BadClassId ({int(cls)})\")\n",
    "            \n",
    "            if not (0 <= x <= 1 and 0 <= y <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n",
    "                issues.append(f\"Line {i}: OutOfBounds ({x},{y},{w},{h})\")\n",
    "                \n",
    "            if w <= 0 or h <= 0:\n",
    "                issues.append(f\"Line {i}: Degenerate ({w},{h})\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        issues.append(f\"ReadError: {str(e)}\")\n",
    "        \n",
    "    return issues\n",
    "\n",
    "def find_orphans(img_dir, lbl_dir):\n",
    "    # Procura descasamentos entre imagens e labels\n",
    "    imgs = {os.path.splitext(f)[0] for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))}\n",
    "    lbls = {os.path.splitext(f)[0] for f in os.listdir(lbl_dir) if f.endswith('.txt')}\n",
    "    \n",
    "    img_orphans = imgs - lbls # Imagens sem labels (pode ser intencional bg-only, mas vale avisar)\n",
    "    lbl_orphans = lbls - imgs # Labels sem imagens (erro grave, deletar)\n",
    "    \n",
    "    return img_orphans, lbl_orphans\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:47.454063686Z",
     "start_time": "2026-01-13T15:04:47.377753536Z"
    }
   },
   "source": [
    "# Rotina Clean Dataset (Move para Quarentena)\n",
    "QUARANTINE_DIR = os.path.join(DATA_ROOT, \"quarantine\")\n",
    "os.makedirs(QUARANTINE_DIR, exist_ok=True)\n",
    "\n",
    "def clean_dataset(dataset_path):\n",
    "    report = []\n",
    "    print(f\"Iniciando limpeza silenciosa em: {dataset_path}\")\n",
    "    print(\"Verificando integridade de imagens e labels... (isso pode levar alguns segundos)\")\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        img_dir = os.path.join(dataset_path, 'images', split)\n",
    "        lbl_dir = os.path.join(dataset_path, 'labels', split)\n",
    "        \n",
    "        if not os.path.exists(img_dir): continue\n",
    "\n",
    "        # 1. Checar Orf√£os\n",
    "        img_orphans, lbl_orphans = find_orphans(img_dir, lbl_dir)\n",
    "        \n",
    "        # Labels orf√£s -> Quarentena\n",
    "        for o in lbl_orphans:\n",
    "            src = os.path.join(lbl_dir, o + \".txt\")\n",
    "            dst = os.path.join(QUARANTINE_DIR, \"orphans\", split, o + \".txt\")\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "            report.append({'file': o, 'split': split, 'issue': 'Orphan Label (Moved)'})\n",
    "            \n",
    "        # 2. Validar Conte√∫do\n",
    "        lbl_files = glob.glob(os.path.join(lbl_dir, \"*.txt\"))\n",
    "        for lf in lbl_files:\n",
    "            issues = verify_yolo_label(lf)\n",
    "            if issues:\n",
    "                base = os.path.basename(lf)\n",
    "                serious = [i for i in issues if i != \"empty_file\"]\n",
    "                if serious:\n",
    "                    dst_lbl = os.path.join(QUARANTINE_DIR, \"corrupt\", split, \"labels\", base)\n",
    "                    os.makedirs(os.path.dirname(dst_lbl), exist_ok=True)\n",
    "                    shutil.move(lf, dst_lbl)\n",
    "                    \n",
    "                    base_img = os.path.splitext(base)[0] + \".jpg\" \n",
    "                    src_img = os.path.join(img_dir, base_img)\n",
    "                    if os.path.exists(src_img):\n",
    "                        dst_img = os.path.join(QUARANTINE_DIR, \"corrupt\", split, \"images\", base_img)\n",
    "                        os.makedirs(os.path.dirname(dst_img), exist_ok=True)\n",
    "                        shutil.move(src_img, dst_img)\n",
    "                    \n",
    "                    report.append({'file': base, 'split': split, 'issue': \"; \".join(serious)})\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Limpeza conclu√≠da.\")\n",
    "    print(f\"Total de arquivos problem√°ticos movidos: {len(report)}\")\n",
    "    if len(report) > 0:\n",
    "        rep_path = os.path.join(REPORTS_DIR, 'audit_report.csv')\n",
    "        pd.DataFrame(report).to_csv(rep_path)\n",
    "        print(f\"Relat√≥rio detalhado salvo em: {rep_path}\")\n",
    "        print(\"Arquivos corrompidos foram movidos para a pasta 'quarantine'.\")\n",
    "    else:\n",
    "        print(\"Nenhum problema cr√≠tico encontrado. Dataset limpo!\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Executar Limpeza\n",
    "# clean_dataset(FINAL_DATASET_DIR)\n"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:47.583310333Z",
     "start_time": "2026-01-13T15:04:47.492099318Z"
    }
   },
   "source": [
    "# Visualiza√ß√£o de Amostras\n",
    "# Vamos desenhar as caixas para ver se faz sentido visualmente.\n",
    "\n",
    "def plot_samples(dataset_path, split='train', n=9):\n",
    "    img_dir = os.path.join(dataset_path, 'images', split)\n",
    "    lbl_dir = os.path.join(dataset_path, 'labels', split)\n",
    "    \n",
    "    all_imgs = os.listdir(img_dir)\n",
    "    if len(all_imgs) == 0: return\n",
    "    \n",
    "    samples = random.sample(all_imgs, min(n, len(all_imgs)))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, img_file in enumerate(samples):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        lbl_path = os.path.join(lbl_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h_img, w_img, _ = img.shape\n",
    "        \n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    c, x, y, w, h = map(float, line.split())\n",
    "                    x1 = int((x - w/2) * w_img)\n",
    "                    y1 = int((y - h/2) * h_img)\n",
    "                    x2 = int((x + w/2) * w_img)\n",
    "                    y2 = int((y + h/2) * h_img)\n",
    "                    \n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(img, str(int(c)), (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)\n",
    "        \n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(img_file)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_samples(FINAL_DATASET_DIR)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Como Rotular Corretamente (Guidelines)\n",
    "\n",
    "Se a auditoria mostrar problemas, ou se voc√™ for rotular novos dados, siga este padr√£o:\n",
    "\n",
    "### Defini√ß√µes Operacionais\n",
    "*   **0: helmet** (Capacete de seguran√ßa). Inclua todo o capacete. Se estiver na m√£o, *n√£o* rotular (depende da regra de neg√≥cio, mas geralmente queremos \"na cabe√ßa\").\n",
    "*   **1: vest** (Colete reflexivo).\n",
    "*   **2: person** (Pessoa). Inclua o corpo vis√≠vel.\n",
    "\n",
    "### Regras de Ouro da BBox\n",
    "1.  **Tightness**: A caixa deve \"tocar\" as bordas do objeto. N√£o deixe muito ar, nem corte peda√ßos do objeto.\n",
    "2.  **Oclus√£o**: Se o objeto est√° 50% tapado, rotule o que √© vis√≠vel (YOLO lida bem com isso). Se est√° 95% tapado e irreconhec√≠vel, ignore.\n",
    "3.  **Consist√™ncia**: Se voc√™ rotula \"cabe√ßa sem capacete\" como \"person\" em uma imagem, fa√ßa isso em todas. N√£o crie ambiguidades.\n",
    "\n",
    "### Ferramentas Recomendadas\n",
    "*   **LabelImg** (Local, cl√°ssico).\n",
    "*   **CVAT** (Robustez profissional).\n",
    "*   **Roboflow** (Web, f√°cil collab).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:47.680119319Z",
     "start_time": "2026-01-13T15:04:47.586864744Z"
    }
   },
   "source": [
    "# Review Sampler Autom√°tico (Smart QA)\n",
    "def review_sampler(dataset_path, split='train', top_k=6):\n",
    "    img_dir = os.path.join(dataset_path, 'images', split)\n",
    "    lbl_dir = os.path.join(dataset_path, 'labels', split)\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for lbl_file in glob.glob(os.path.join(lbl_dir, \"*.txt\")):\n",
    "        with open(lbl_file) as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        n_objs = len(lines)\n",
    "        has_micro = False\n",
    "        has_edge = False\n",
    "        \n",
    "        for line in lines:\n",
    "            _, x, y, w, h = map(float, line.split())\n",
    "            if w*h < 0.005: has_micro = True \n",
    "            if x-w/2 < 0.01 or x+w/2 > 0.99: has_edge = True\n",
    "            \n",
    "        score = 0\n",
    "        score += n_objs * 1      \n",
    "        score += 50 if has_micro else 0  \n",
    "        score += 10 if has_edge else 0   \n",
    "        \n",
    "        candidates.append((score, lbl_file))\n",
    "        \n",
    "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "    top_files = [os.path.splitext(os.path.basename(c[1]))[0] + \".jpg\" for c in candidates[:top_k]]\n",
    "    \n",
    "    print(f\"Revisando Top-{top_k} Casos Suspeitos (Crowded/Micro/Edge)...\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, img_file in enumerate(top_files):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        if not os.path.exists(img_path): continue\n",
    "        \n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        h_img, w_img, _ = img.shape\n",
    "        lbl_path = os.path.join(lbl_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "        \n",
    "        with open(lbl_path) as f:\n",
    "            for line in f:\n",
    "                c, x, y, w, h = map(float, line.split())\n",
    "                x1, y1 = int((x - w/2) * w_img), int((y - h/2) * h_img)\n",
    "                x2, y2 = int((x + w/2) * w_img), int((y + h/2) * h_img)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        # FIX: Escape newline for python string serialization\n",
    "        plt.title(f\"{img_file}\\nScore: {candidates[i][0]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# review_sampler(FINAL_DATASET_DIR)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Augmentations: Ensinando Robustez\n",
    "\n",
    "O modelo n√£o \"entende\" objetos; ele v√™ padr√µes de pixels. Se ele s√≥ viu capacetes amarelos √† luz do dia, ele falhar√° em capacetes amarelos √† noite.\n",
    "**Augmentation** expande artificialmente o dataset variando as condi√ß√µes.\n",
    "\n",
    "### Duas Estrat√©gias\n",
    "\n",
    "1.  **Online (On-the-fly via Ultralytics)**: O YOLO11 j√° faz muito augmentation pesado durante o treino (`mosaic`, `mixup`, `hsv`, `crop`, `flip`). Isso √© configur√°vel nos hiperpar√¢metros (arquivo `.yaml` ou args de treino).\n",
    "    *   *Bom para*: Varia√ß√µes geom√©tricas e de cor padr√£o.\n",
    "2.  **Offline (Pr√©-processamento via Albumentations)**: Criar um dataset \"v2\" com efeitos espec√≠ficos que o YOLO n√£o faz nativamente ou controlar melhor a probabilidade.\n",
    "    *   *Bom para*: Efeitos de clima (chuva/neblina), desfoque de movimento, etc.\n",
    "\n",
    "Vamos focar em visualizar o **Pipeline Albumentations** para entender o que est√° acontecendo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:48.833118879Z",
     "start_time": "2026-01-13T15:04:47.757331461Z"
    }
   },
   "source": [
    "import albumentations as A\n",
    "\n",
    "# Pipeline \"Seguro\" para Detec√ß√£o\n",
    "aug_pipeline = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(p=1),  \n",
    "        A.GaussNoise(p=1),  \n",
    "        A.Defocus(p=1),     \n",
    "    ], p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "print(\"Pipeline Albumentations criado.\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Albumentations criado.\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:48.999014259Z",
     "start_time": "2026-01-13T15:04:48.860770420Z"
    }
   },
   "source": [
    "# Visualizando Augmentations\n",
    "def visualize_augmentations(dataset_path, pipeline, samples=6):\n",
    "    img_dir = os.path.join(dataset_path, 'images', 'train')\n",
    "    lbl_dir = os.path.join(dataset_path, 'labels', 'train')\n",
    "    \n",
    "    file_list = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    if not file_list: return\n",
    "    \n",
    "    chosen = random.sample(file_list, min(samples, len(file_list)))\n",
    "    \n",
    "    plt.figure(figsize=(16, 4 * samples))\n",
    "    \n",
    "    for row, img_file in enumerate(chosen):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        lbl_path = os.path.join(lbl_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    c, x, y, w, h = map(float, line.split())\n",
    "                    bboxes.append([x, y, w, h])\n",
    "                    labels.append(int(c))\n",
    "        \n",
    "        plt.subplot(samples, 4, row*4 + 1)\n",
    "        viz_img = image.copy()\n",
    "        h_img, w_img, _ = viz_img.shape\n",
    "        for bbox, cls in zip(bboxes, labels):\n",
    "            x, y, w, h = bbox\n",
    "            x1, y1 = int((x - w/2) * w_img), int((y - h/2) * h_img)\n",
    "            x2, y2 = int((x + w/2) * w_img), int((y + h/2) * h_img)\n",
    "            cv2.rectangle(viz_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        plt.imshow(viz_img)\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        for col in range(3):\n",
    "            try:\n",
    "                augmented = pipeline(image=image, bboxes=bboxes, class_labels=labels)\n",
    "                aug_img = augmented['image']\n",
    "                aug_bboxes = augmented['bboxes']\n",
    "                h_aug, w_aug, _ = aug_img.shape\n",
    "                for bbox in aug_bboxes:\n",
    "                    x, y, w, h = bbox\n",
    "                    x1, y1 = int((x - w/2) * w_aug), int((y - h/2) * h_aug)\n",
    "                    x2, y2 = int((x + w/2) * w_aug), int((y + h/2) * h_aug)\n",
    "                    cv2.rectangle(aug_img, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "                \n",
    "                plt.subplot(samples, 4, row*4 + 2 + col)\n",
    "                plt.imshow(aug_img)\n",
    "                plt.title(f\"Aug {col+1}\")\n",
    "                plt.axis('off')\n",
    "            except Exception as e:\n",
    "                plt.subplot(samples, 4, row*4 + 2 + col)\n",
    "                # FIX: Escape newline for python string serialization\n",
    "                plt.text(0.5, 0.5, f\"Drop/Error\\n{str(e)}\", ha='center')\n",
    "                plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# visualize_augmentations(FINAL_DATASET_DIR, aug_pipeline)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riscos e Contra-indica√ß√µes\n",
    "Nem todo augmentation ajuda. Se voc√™ distorce demais, o modelo aprende lixo.\n",
    "\n",
    "| Objetivo | Augmentation Sugerida | Risco / Cuidado |\n",
    "| :--- | :--- | :--- |\n",
    "| **Robustez a Luz** | Brightness, Contrast, Gamma | Se exagerar, apaga detalhes em √°reas escuras. |\n",
    "| **Robustez a C√¢mera** | Blur, Noise, Compression | Se o objeto for muito pequeno (ex: < 10px), o blur pode faz√™-lo desaparecer. |\n",
    "| **Robustez a Posi√ß√£o** | Shift, Crop, Scale | **Flip Vertical** quase nunca faz sentido para pessoas/capacetes (gravidade existe!). |\n",
    "| **Generaliza√ß√£o** | MixUp, Mosaic (Ultralytics) | √ìtimo para treino, mas confuso visualmente para humanos. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Treino Baseline (Transfer Learning)\n",
    "\n",
    "Agora que conhecemos os dados, vamos rodar um modelo simples para ter uma **Linha de Base**.\n",
    "N√£o tente tunar nada ainda. Use \"defaults razo√°veis\".\n",
    "\n",
    "### O que acontece no Fine-tuning?\n",
    "Carregamos `yolov8n.pt` (treinado em COCO).\n",
    "1.  **Backbone (Corpo)**: Detector de caracter√≠sticas universais (bordas, texturas). Mantemos (ou treinamos pouco).\n",
    "2.  **Head (Cabe√ßa)**: A √∫ltima camada. Ela \"sabia\" detectar *Cachorro, Gato, Carro...*. N√≥s substitu√≠mos ela por uma nova que aprender√° *Capacete, Colete, Pessoa*.\n",
    "\n",
    "### Monitorando Overfitting\n",
    "*   **Good**: Loss de Treino cai, Loss de Valida√ß√£o cai.\n",
    "*   **Overfit**: Loss de Treino cai MUITO, Loss de Valida√ß√£o come√ßa a **SUBIR**. (O modelo decorou o treino).\n",
    "*   **Underfit**: Loss de Treino n√£o cai (ou cai muito devagar). (Modelo burro ou dados ruins).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T15:04:50.684262593Z",
     "start_time": "2026-01-13T15:04:49.026660529Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Carregar Modelo Pr√©-treinado\n",
    "# 'n' = Nano (mais r√°pido, menor acur√°cia). Bom para baseline.\n",
    "model_baseline = YOLO('yolo11n.pt') \n",
    "\n",
    "print(\"Modelo carregado. Classes originais:\", len(model_baseline.names))\n",
    "# Nota: Ao iniciar o treino com um data.yaml diferente, o YOLO substitui automaticamente o Head."
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "operator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[51]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01multralytics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m YOLO\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# 1. Carregar Modelo Pr√©-treinado\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# 'n' = Nano (mais r√°pido, menor acur√°cia). Bom para baseline.\u001B[39;00m\n\u001B[32m      5\u001B[39m model_baseline = YOLO(\u001B[33m'\u001B[39m\u001B[33myolo11n.pt\u001B[39m\u001B[33m'\u001B[39m) \n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1406\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/ultralytics/__init__.py:38\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m     36\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Lazy-import model classes on first access.\"\"\"\u001B[39;00m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m MODELS:\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[43mimportlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43multralytics.models\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m, name)\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmodule \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/3.12.0/lib/python3.12/importlib/__init__.py:90\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m     88\u001B[39m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     89\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m90\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/ultralytics/models/__init__.py:6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NAS\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrtdetr\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RTDETR\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01msam\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SAM\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01myolo\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m YOLO, YOLOE, YOLOWorld\n\u001B[32m      9\u001B[39m __all__ = \u001B[33m\"\u001B[39m\u001B[33mNAS\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mRTDETR\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mSAM\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mYOLO\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mYOLOE\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mFastSAM\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mYOLOWorld\u001B[39m\u001B[33m\"\u001B[39m  \u001B[38;5;66;03m# allow simpler import\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/ultralytics/models/sam/__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SAM\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpredict\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      5\u001B[39m     Predictor,\n\u001B[32m      6\u001B[39m     SAM2DynamicInteractivePredictor,\n\u001B[32m   (...)\u001B[39m\u001B[32m     12\u001B[39m     SAM3VideoSemanticPredictor,\n\u001B[32m     13\u001B[39m )\n\u001B[32m     15\u001B[39m __all__ = (\n\u001B[32m     16\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mSAM\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     17\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mPredictor\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     24\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mSAM3VideoSemanticPredictor\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     25\u001B[39m )  \u001B[38;5;66;03m# tuple or list of exportable items\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/ultralytics/models/sam/model.py:24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01multralytics\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mengine\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01multralytics\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtorch_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m model_info\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpredict\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Predictor, SAM2Predictor, SAM3Predictor\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mSAM\u001B[39;00m(Model):\n\u001B[32m     28\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"SAM (Segment Anything Model) interface class for real-time image segmentation tasks.\u001B[39;00m\n\u001B[32m     29\u001B[39m \n\u001B[32m     30\u001B[39m \u001B[33;03m    This class provides an interface to the Segment Anything Model (SAM) from Ultralytics, designed for promptable\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     47\u001B[39m \u001B[33;03m        ...     print(f\"Detected {len(r.masks)} masks\")\u001B[39;00m\n\u001B[32m     48\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/ultralytics/models/sam/predict.py:40\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01multralytics\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtorch_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m select_device, smart_inference_mode\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mamg\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     30\u001B[39m     batch_iterator,\n\u001B[32m     31\u001B[39m     batched_mask_to_box,\n\u001B[32m   (...)\u001B[39m\u001B[32m     38\u001B[39m     uncrop_masks,\n\u001B[32m     39\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01msam3\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgeometry_encoders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Prompt\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mPredictor\u001B[39;00m(BasePredictor):\n\u001B[32m     44\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Predictor class for SAM, enabling real-time image segmentation with promptable capabilities.\u001B[39;00m\n\u001B[32m     45\u001B[39m \n\u001B[32m     46\u001B[39m \u001B[33;03m    This class extends BasePredictor and implements the Segment Anything Model (SAM) for advanced image segmentation\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     82\u001B[39m \u001B[33;03m        >>> results = predictor(bboxes=bboxes)\u001B[39;00m\n\u001B[32m     83\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/ultralytics/models/sam/sam3/geometry_encoders.py:7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnn\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01multralytics\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodules\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _get_clones\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01multralytics\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m xywh2xyxy\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/torchvision/__init__.py:10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# .extensions) before entering _meta_registrations.\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mextension\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _HAS_OPS  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorchvision\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001B[38;5;66;03m# usort:skip\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mversion\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001B[39m\n\u001B[32m    153\u001B[39m     torch._check(\n\u001B[32m    154\u001B[39m         grad.dtype == rois.dtype,\n\u001B[32m    155\u001B[39m         \u001B[38;5;28;01mlambda\u001B[39;00m: (\n\u001B[32m   (...)\u001B[39m\u001B[32m    158\u001B[39m         ),\n\u001B[32m    159\u001B[39m     )\n\u001B[32m    160\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m grad.new_empty((batch_size, channels, height, width))\n\u001B[32m--> \u001B[39m\u001B[32m163\u001B[39m \u001B[38;5;129;43m@torch\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlibrary\u001B[49m\u001B[43m.\u001B[49m\u001B[43mregister_fake\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtorchvision::nms\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;43;01mdef\u001B[39;49;00m\u001B[38;5;250;43m \u001B[39;49m\u001B[34;43mmeta_nms\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou_threshold\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdets\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mboxes should be a 2d tensor, got \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdets\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43mD\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdets\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mboxes should have 4 elements in dimension 1, got \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdets\u001B[49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/torch/library.py:1063\u001B[39m, in \u001B[36mregister_fake.<locals>.register\u001B[39m\u001B[34m(func)\u001B[39m\n\u001B[32m   1061\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1062\u001B[39m     use_lib = lib\n\u001B[32m-> \u001B[39m\u001B[32m1063\u001B[39m \u001B[43muse_lib\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_register_fake\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1064\u001B[39m \u001B[43m    \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacklevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstacklevel\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_override\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_override\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/torch/library.py:211\u001B[39m, in \u001B[36mLibrary._register_fake\u001B[39m\u001B[34m(self, op_name, fn, _stacklevel, allow_override)\u001B[39m\n\u001B[32m    208\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    209\u001B[39m     func_to_register = fn\n\u001B[32m--> \u001B[39m\u001B[32m211\u001B[39m handle = \u001B[43mentry\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfake_impl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mregister\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    212\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfunc_to_register\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlib\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_override\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_override\u001B[49m\n\u001B[32m    213\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[38;5;28mself\u001B[39m._registration_handles.append(handle)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.pyenv/versions/ncia/lib/python3.12/site-packages/torch/_library/fake_impl.py:50\u001B[39m, in \u001B[36mFakeImplHolder.register\u001B[39m\u001B[34m(self, func, source, lib, allow_override)\u001B[39m\n\u001B[32m     44\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.kernel \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     45\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m     46\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mregister_fake(...): the operator \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.qualname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     47\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33malready has an fake impl registered at \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     48\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.kernel.source\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     49\u001B[39m     )\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_dispatch_has_kernel_for_dispatch_key\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mqualname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mMeta\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m     52\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mregister_fake(...): the operator \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.qualname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     53\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33malready has an DispatchKey::Meta implementation via a \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     56\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mregister_fake.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     57\u001B[39m     )\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch._C._dispatch_has_kernel_for_dispatch_key(\n\u001B[32m     60\u001B[39m     \u001B[38;5;28mself\u001B[39m.qualname, \u001B[33m\"\u001B[39m\u001B[33mCompositeImplicitAutograd\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     61\u001B[39m ):\n",
      "\u001B[31mRuntimeError\u001B[39m: operator torchvision::nms does not exist"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. Executar Treino Baseline\n",
    "# Configura√ß√£o Conservadora para RTX 4060 (8GB)\n",
    "\n",
    "train_args = {\n",
    "    'data': os.path.join(FINAL_DATASET_DIR, \"data.yaml\"),\n",
    "    'epochs': 30,           # Baseline curto. Na pr√°tica use 50-100.\n",
    "    'imgsz': 640,           # Tamanho padr√£o.\n",
    "    'batch': 16,            # 16 costuma ser seguro para 8GB. Se der OOM, baixe para 8.\n",
    "    'project': RUNS_DIR,    # Salvar em ./workspace/runs\n",
    "    'name': 'baseline_v1',  # Nome da pasta do experimento\n",
    "    'exist_ok': True,       # Sobrescrever se existir (cuidado!)\n",
    "    'amp': True,            # Mixed Precision (r√°pido e menos mem√≥ria)\n",
    "    'cache': True,          # Cache RAM (se couber, acelera muito)\n",
    "    'patience': 10,         # Early Stopping (para se estagnar por 10 epochs)\n",
    "    'device': '0' if torch.cuda.is_available() else 'cpu',\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "print(f\"Iniciando Treino Baseline em {train_args['device']}...\")\n",
    "results = model_baseline.train(**train_args)\n",
    "\n",
    "print(f\"Treino conclu√≠do. Resultados em: {results.save_dir}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. Registrar Experimento\n",
    "# √â √∫til manter um log JSON pr√≥prio al√©m do MLFlow/WandB para an√°lises r√°pidas via Pandas.\n",
    "\n",
    "import datetime\n",
    "\n",
    "def log_experiment(name, results_obj, params):\n",
    "    log_path = os.path.join(REPORTS_DIR, \"experiments_log.json\")\n",
    "    \n",
    "    # Extrair m√©tricas chave\n",
    "    metrics = {\n",
    "        \"map50\": results_obj.box.map50,\n",
    "        \"map50-95\": results_obj.box.map,\n",
    "        \"precision\": results_obj.box.mp,\n",
    "        \"recall\": results_obj.box.mr\n",
    "    }\n",
    "    \n",
    "    entry = {\n",
    "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "        \"name\": name,\n",
    "        \"metrics\": metrics,\n",
    "        \"params\": {k:str(v) for k,v in params.items()}, # Serialize\n",
    "        \"save_dir\": str(results_obj.save_dir)\n",
    "    }\n",
    "    \n",
    "    logs = []\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, 'r', encoding='utf-8') as f:\n",
    "            try: logs = json.load(f)\n",
    "            except: pass\n",
    "            \n",
    "    logs.append(entry)\n",
    "    \n",
    "    with open(log_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "        \n",
    "    print(f\"Experimento '{name}' registrado em {log_path}\")\n",
    "    return pd.DataFrame([entry])\n",
    "\n",
    "# Logar o baseline\n",
    "log_experiment(\"baseline_nano_30ep\", results, train_args)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicas Avan√ßadas: Resume e Freeze\n",
    "\n",
    "1.  **Resume**: Se a luz cair na epoch 29/30:\n",
    "    ```python\n",
    "    # model = YOLO(\"workspace/runs/baseline_v1/weights/last.pt\")\n",
    "    # model.train(resume=True)\n",
    "    ```\n",
    "\n",
    "2.  **Freeze**: Se voc√™ tiver pouqu√≠ssimos dados (< 100 img), pode ajudar congelar o backbone para n√£o \"quebrar\" os pesos pr√©-treinados.\n",
    "    ```python\n",
    "    # model.train(data=..., freeze=10) # Congela as 10 primeiras camadas\n",
    "    ```\n",
    "    No YOLO11, `freeze` aceita um int (n√∫mero de camadas) ou lista de √≠ndices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Avalia√ß√£o e Error Analysis\n",
    "\n",
    "O `mAP` √© apenas um n√∫mero. Ele n√£o te diz **onde** seu modelo est√° errando.\n",
    "Vamos fazer o \"Raio-X\" do modelo baseline.\n",
    "\n",
    "### Interpretando M√©tricas\n",
    "*   **Precision (Precis√£o)**: De todas as detec√ß√µes do modelo, quantas est√£o corretas? (Evita e-mails falsos para o chefe).\n",
    "*   **Recall (Revoca√ß√£o)**: De todos os EPIs reais na imagem, quantos o modelo achou? (Evita acidentes n√£o detectados).\n",
    "*   **mAP50**: A \"nota geral\" da prova. Considera Precision e Recall combinados, aceitando caixas com >50% de sobreposi√ß√£o (IoU).\n",
    "\n",
    "    > **Trade-off**: Geralmente, aumentar Precision diminui Recall e vice-versa. O mAP √© o equil√≠brio.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 1. Rodar Valida√ß√£o Oficial\n",
    "# Isso gera as matrizes de confus√£o e curvas PR oficiais na pasta do experimento.\n",
    "val_results = model_baseline.val(split='val')\n",
    "\n",
    "print(f\"Mean Average Precision (mAP50): {val_results.box.map50:.3f}\")\n",
    "print(f\"Precision: {val_results.box.mp:.3f}\")\n",
    "print(f\"Recall: {val_results.box.mr:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 2. Plotar Curvas e Matrizes Geradas\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# O YOLO salva plots automaticamente na pasta do 'run'. Vamos ach√°-la.\n",
    "run_dir = str(val_results.save_dir) \n",
    "\n",
    "print(f\"Exibindo gr√°ficos salvos em: {run_dir}\")\n",
    "\n",
    "plots = [\n",
    "    \"confusion_matrix.png\",\n",
    "    \"F1_curve.png\",\n",
    "    \"PR_curve.png\",\n",
    "    \"labels.jpg\" # Mostra distribui√ß√£o do GT\n",
    "]\n",
    "\n",
    "for p in plots:\n",
    "    path = os.path.join(run_dir, p)\n",
    "    if os.path.exists(path):\n",
    "        display(Image(filename=path, width=600))\n",
    "    else:\n",
    "        print(f\"Plot {p} n√£o encontrado (pode exigir scikit-learn instalado).\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise de Erros Visual (Dashboard)\n",
    "N√∫meros agregados escondem a verdade. Vamos ver **lado a lado**:\n",
    "*   Esquerda: **Ground Truth** (O que o humano marcou).\n",
    "*   Direita: **Prediction** (O que o modelo viu).\n",
    "\n",
    "Se o modelo vir algo na direita que n√£o est√° na esquerda:\n",
    "1.  O modelo est√° alucinando (Erro dele)? ou...\n",
    "2.  O humano esqueceu de rotular (Erro nosso)? **(Muito comum!)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualiza√ß√£o Lado a Lado: GT vs Pred\n",
    "def plot_errors(model, dataset_path, split='val', n=6, conf_threshold=0.25):\n",
    "    img_dir = os.path.join(dataset_path, 'images', split)\n",
    "    lbl_dir = os.path.join(dataset_path, 'labels', split)\n",
    "    \n",
    "    files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    if not files: return\n",
    "    \n",
    "    samples = random.sample(files, min(n, len(files)))\n",
    "    \n",
    "    plt.figure(figsize=(15, 5*len(samples)))\n",
    "    \n",
    "    for i, img_file in enumerate(samples):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        lbl_path = os.path.join(lbl_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "        \n",
    "        # Leitura Imagem\n",
    "        img_raw = cv2.imread(img_path)\n",
    "        img_gt = img_raw.copy()\n",
    "        img_pred = img_raw.copy()\n",
    "        h, w, _ = img_raw.shape\n",
    "        \n",
    "        # --- ESQUERDA: GROUND TRUTH ---\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    c, cx, cy, bw, bh = map(float, line.split())\n",
    "                    x1 = int((cx - bw/2) * w)\n",
    "                    y1 = int((cy - bh/2) * h)\n",
    "                    x2 = int((cx + bw/2) * w)\n",
    "                    y2 = int((cy + bh/2) * h)\n",
    "                    color = (0, 255, 0) # Verde = Verdade\n",
    "                    cv2.rectangle(img_gt, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(img_gt, f\"GT {int(c)}\", (x1, y1-5), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        \n",
    "        # --- DIREITA: PREDICTIONS ---\n",
    "        results = model.predict(img_path, conf=conf_threshold, verbose=False)\n",
    "        for box in results[0].boxes:\n",
    "            coords = box.xyxy[0].cpu().numpy() # x1, y1, x2, y2\n",
    "            conf = float(box.conf)\n",
    "            cls_id = int(box.cls)\n",
    "            \n",
    "            x1, y1, x2, y2 = map(int, coords)\n",
    "            color = (0, 0, 255) # Vermelho = Prediction\n",
    "            cv2.rectangle(img_pred, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(img_pred, f\"{model.names[cls_id]} {conf:.2f}\", (x1, y1-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            \n",
    "        # Plot\n",
    "        plt.subplot(len(samples), 2, i*2 + 1)\n",
    "        plt.imshow(cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Ground Truth: {img_file}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(len(samples), 2, i*2 + 2)\n",
    "        plt.imshow(cv2.cvtColor(img_pred, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Prediction (Conf > {conf_threshold})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot_errors(model_baseline, FINAL_DATASET_DIR, split='val', n=6)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guia de A√ß√£o\n",
    "*   **Se FN domina (Muitos objetos perdidos):**\n",
    "    *   Falta de treino (mais epochs).\n",
    "    *   Objetos muito pequenos (aumentar `imgsz` de 640 para 1280).\n",
    "    *   Labels faltando no GT (o modelo aprendeu a ignorar).\n",
    "*   **Se FP domina (Muitas alucina√ß√µes):**\n",
    "    *   Background ruidoso? (Adicionar imagens vazias `background` no treino ajuda).\n",
    "    *   Aumentar threshold de confian√ßa na infer√™ncia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. B√¥nus: Rastreamento com MLflow\n",
    "\n",
    "O YOLOv8 integra-se nativamente com MLflow.\n",
    "N√£o √© necess√°rio escrever callbacks complexos. Basta configurar o ambiente.\n",
    "\n",
    "### Como usar o MLflow Dashboard\n",
    "1.  Abra um terminal no VSCode (`Ctrl + '`).\n",
    "2.  Ative seu ambiente (`myenv`).\n",
    "3.  Rode: `mlflow ui --port 5000 --backend-store-uri ./workspace/mlruns`\n",
    "4.  Acesse `http://localhost:5000` no navegador.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import mlflow\n",
    "\n",
    "# Configurar MLflow para salvar localmente na pasta do projeto\n",
    "# Isso evita poluir seu diret√≥rio de usu√°rio (e funciona no Colab/Kaggle igual).\n",
    "mlflow_tracking_uri = os.path.join(BASE_DIR, \"mlruns\")\n",
    "os.makedirs(mlflow_tracking_uri, exist_ok=True)\n",
    "\n",
    "mlflow.set_tracking_uri(f\"file:///{mlflow_tracking_uri}\")\n",
    "mlflow.set_experiment(PROJECT_NAME)\n",
    "\n",
    "# O YOLOv8 detecta automaticamente o MLflow se ele estiver instalado e ativo.\n",
    "# Mas podemos for√ßar o nome do run via vari√°vel de ambiente se quisermos.\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = f\"file:///{mlflow_tracking_uri}\"\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = PROJECT_NAME\n",
    "\n",
    "print(f\"MLflow configurado! Logs ir√£o para: {mlflow_tracking_uri}\")\n",
    "print(\"Para ver o dashboard, rode no terminal:\")\n",
    "print(f\"mlflow ui --port 5000 --backend-store-uri {mlflow_tracking_uri}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 8.5 ‚Äî Migra√ß√£o para Dataset Real (PPE) + Valida√ß√£o\n",
    "\n",
    "**OBJETIVO CR√çTICO**: Esta se√ß√£o serve como um \"Safety Check\" antes de gastar horas treinando ou rodando HPO.\n",
    "At√© agora, √© poss√≠vel que tenhamos usado um **Dummy Dataset** (ret√¢ngulos coloridos) apenas para testar o c√≥digo.\n",
    "Para ter resultados reais, precisamos garantir que o dataset `ppe_v1` cont√©m fotos reais e labels corretos.\n",
    "\n",
    "**O que vamos verificar:**\n",
    "1.  **Configura√ß√£o Atual**: Onde o `data.yaml` est√° apontando.\n",
    "2.  **Volume de Dados**: Se tivermos < 50 imagens, algo est√° errado (dataset dummy ou incompleto).\n",
    "3.  **Consist√™ncia Profunda**:\n",
    "    *   Orphans (Imagem sem label ou Label sem imagem).\n",
    "    *   Formato YOLO (5 colunas, classes v√°lidas, coords normalizadas).\n",
    "4.  **Prova Visual**: Vamos plotar imagens reais com suas bounding boxes para confirma√ß√£o humana.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "import yaml\n",
    "\n",
    "print(\"--- [CHECK 1] Current data.yaml Configuration ---\")\n",
    "yaml_path = os.path.join(FINAL_DATASET_DIR, \"data.yaml\")\n",
    "\n",
    "if os.path.exists(yaml_path):\n",
    "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "        \n",
    "    print(f\"Path: {data_config.get('path')}\")\n",
    "    print(f\"Train: {data_config.get('train')}\")\n",
    "    print(f\"Val: {data_config.get('val')}\")\n",
    "    print(f\"Names: {data_config.get('names')}\")\n",
    "else:\n",
    "    print(f\"ERROR: {yaml_path} not found!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "print(\"\\n--- [CHECK 2] Dataset Volume (Real vs Dummy) ---\")\n",
    "\n",
    "train_img_dir = os.path.join(FINAL_DATASET_DIR, \"images\", \"train\")\n",
    "val_img_dir = os.path.join(FINAL_DATASET_DIR, \"images\", \"val\")\n",
    "\n",
    "n_train = len(glob.glob(os.path.join(train_img_dir, \"*.jpg\"))) + len(glob.glob(os.path.join(train_img_dir, \"*.png\")))\n",
    "n_val = len(glob.glob(os.path.join(val_img_dir, \"*.jpg\"))) + len(glob.glob(os.path.join(val_img_dir, \"*.png\")))\n",
    "\n",
    "print(f\"Training Images: {n_train}\")\n",
    "print(f\"Validation Images: {n_val}\")\n",
    "\n",
    "if n_train < 100 or n_val < 20:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARNING: DATASET MULTI-SMALL DETECTED!\")\n",
    "    print(\"Isso se parece com um Dummy Dataset ou um teste muito pequeno.\")\n",
    "    print(\"Se voc√™ espera resultados reais, por favor extraia o dataset completo.\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Volume parece razo√°vel para um dataset real (ou small subset).\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "print(\"\\n--- [CHECK 3] Consistency & Sanitization ---\")\n",
    "\n",
    "def sanitize_split(split_name):\n",
    "    img_path = os.path.join(FINAL_DATASET_DIR, \"images\", split_name)\n",
    "    lbl_path = os.path.join(FINAL_DATASET_DIR, \"labels\", split_name)\n",
    "    quarantine_path = os.path.join(FINAL_DATASET_DIR, \"_quarantine\", split_name)\n",
    "    \n",
    "    os.makedirs(quarantine_path, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(img_path) or not os.path.exists(lbl_path):\n",
    "        print(f\"[{split_name}] Directory not found. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Get basenames\n",
    "    imgs = {os.path.splitext(f)[0] for f in os.listdir(img_path) if f.endswith(('.jpg', '.png'))}\n",
    "    lbls = {os.path.splitext(f)[0] for f in os.listdir(lbl_path) if f.endswith('.txt')}\n",
    "    \n",
    "    # 1. Orphans\n",
    "    img_orphans = imgs - lbls\n",
    "    lbl_orphans = lbls - imgs\n",
    "    \n",
    "    if img_orphans:\n",
    "        print(f\"[{split_name}] Found {len(img_orphans)} images without labels. Moving to quarantine...\")\n",
    "        for name in img_orphans:\n",
    "            # find extension\n",
    "            full_name = [f for f in os.listdir(img_path) if f.startswith(name)][0]\n",
    "            shutil.move(os.path.join(img_path, full_name), os.path.join(quarantine_path, full_name))\n",
    "            \n",
    "    if lbl_orphans:\n",
    "        print(f\"[{split_name}] Found {len(lbl_orphans)} labels without images. Moving to quarantine...\")\n",
    "        for name in lbl_orphans:\n",
    "            shutil.move(os.path.join(lbl_path, name + \".txt\"), os.path.join(quarantine_path, name + \".txt\"))\n",
    "\n",
    "    # 2. Format Val\n",
    "    valid_count, bad_count = 0, 0\n",
    "    \n",
    "    # Refresh lists after orphan removal\n",
    "    lbls = [f for f in os.listdir(lbl_path) if f.endswith('.txt')]\n",
    "    \n",
    "    for lbl_file in lbls:\n",
    "        is_bad = False\n",
    "        fpath = os.path.join(lbl_path, lbl_file)\n",
    "        with open(fpath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    is_bad = True; break\n",
    "                \n",
    "                try:\n",
    "                    cls = int(parts[0])\n",
    "                    coords = [float(x) for x in parts[1:]]\n",
    "                    if not (0 <= cls <= 2): # Hardcoded helper for PPE (3 classes)\n",
    "                        is_bad = True; break\n",
    "                    if any(c < 0 or c > 1 for c in coords):\n",
    "                        is_bad = True; break\n",
    "                except ValueError:\n",
    "                    is_bad = True; break\n",
    "        \n",
    "        if is_bad:\n",
    "            print(f\"Corrupt label found: {lbl_file}. Moving to quarantine.\")\n",
    "            shutil.move(fpath, os.path.join(quarantine_path, lbl_file))\n",
    "            # Also move image\n",
    "            img_cand = [f for f in os.listdir(img_path) if f.startswith(os.path.splitext(lbl_file)[0])]\n",
    "            if img_cand:\n",
    "                shutil.move(os.path.join(img_path, img_cand[0]), os.path.join(quarantine_path, img_cand[0]))\n",
    "            bad_count += 1\n",
    "        else:\n",
    "            valid_count += 1\n",
    "            \n",
    "    print(f\"[{split_name}] Valid Pairs: {valid_count} | Moved to Quarantine: {len(img_orphans) + len(lbl_orphans) + bad_count}\")\n",
    "\n",
    "sanitize_split('train')\n",
    "sanitize_split('val')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "print(\"\\n--- [CHECK 4] Visual Sanity Check (Is this Real Data?) ---\")\n",
    "\n",
    "def plot_random_grid(split='train', num=9):\n",
    "    img_dir = os.path.join(FINAL_DATASET_DIR, 'images', split)\n",
    "    lbl_dir = os.path.join(FINAL_DATASET_DIR, 'labels', split)\n",
    "    \n",
    "    if not os.path.exists(img_dir):\n",
    "        print(f\"Directory {img_dir} does not exist.\")\n",
    "        return\n",
    "\n",
    "    all_imgs = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
    "    if not all_imgs:\n",
    "        print(f\"No images in {split} to plot.\")\n",
    "        return\n",
    "        \n",
    "    selected = random.sample(all_imgs, min(num, len(all_imgs)))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, img_file in enumerate(selected):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        lbl_path = os.path.join(lbl_dir, os.path.splitext(img_file)[0] + \".txt\")\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None: continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = img.shape\n",
    "        \n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = list(map(float, line.split()))\n",
    "                    if len(parts) == 5:\n",
    "                        c, cx, cy, bw, bh = parts\n",
    "                        x1 = int((cx - bw/2) * w)\n",
    "                        y1 = int((cy - bh/2) * h)\n",
    "                        x2 = int((cx + bw/2) * w)\n",
    "                        y2 = int((cy + bh/2) * h)\n",
    "                        \n",
    "                        color = (0, 255, 0) # Green for annotations\n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "                        cv2.putText(img, str(int(c)), (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        \n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{split}/{img_file}\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Plotting Training Samples...\")\n",
    "plot_random_grid('train')\n",
    "\n",
    "print(\"Plotting Validation Samples...\")\n",
    "plot_random_grid('val')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "print(\"\\n--- [CHECK 5] Class Statistics ---\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def get_class_stats(split):\n",
    "    lbl_dir = os.path.join(FINAL_DATASET_DIR, 'labels', split)\n",
    "    if not os.path.exists(lbl_dir): return {}\n",
    "    \n",
    "    counts = {0:0, 1:0, 2:0}\n",
    "    \n",
    "    for lfile in os.listdir(lbl_dir):\n",
    "        if not lfile.endswith('.txt'): continue\n",
    "        with open(os.path.join(lbl_dir, lfile)) as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if not parts: continue\n",
    "                c = int(parts[0])\n",
    "                if c in counts: counts[c] += 1\n",
    "    return counts\n",
    "\n",
    "stats_train = get_class_stats('train')\n",
    "stats_val = get_class_stats('val')\n",
    "\n",
    "if stats_train and stats_val:\n",
    "    df_stats = pd.DataFrame([stats_train, stats_val], index=['Train', 'Val']).T\n",
    "    print(df_stats)\n",
    "\n",
    "    # Warning for zero classes\n",
    "    if (df_stats == 0).any().any():\n",
    "        print(\"\\nCRITICAL WARNING: Uma ou mais classes t√™m ZERO exemplos. Isso quebrar√° o treino.\")\n",
    "else:\n",
    "    print(\"Could not generate statistics (missing directories).\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "print(\"\\n--- [CHECK 6] Finalizing configuration ---\")\n",
    "\n",
    "# Re-write data.yaml to be absolutely sure\n",
    "yaml_content = f'''\n",
    "path: {FINAL_DATASET_DIR}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: \n",
    "\n",
    "names:\n",
    "  0: helmet\n",
    "  1: vest\n",
    "  2: person\n",
    "'''\n",
    "with open(os.path.join(FINAL_DATASET_DIR, \"data.yaml\"), 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"data.yaml atualizado e verificado em: {FINAL_DATASET_DIR}/data.yaml\")\n",
    "print(\"Checklist:\")\n",
    "print(\"[ ] Estrutura OK\")\n",
    "print(f\"[ ] Val Count: {n_val}\")\n",
    "print(\"[ ] Visual Check (acima)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
